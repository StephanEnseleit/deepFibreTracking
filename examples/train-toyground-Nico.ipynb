{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import gym\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "\n",
    "import os, sys\n",
    "sys.path.insert(0,'..')\n",
    "\n",
    "from collections import deque \n",
    "\n",
    "from dfibert.tracker.nn.rl import Agent, Action_Scheduler, DQN\n",
    "import dfibert.envs.RLtractEnvironment as RLTe\n",
    "from dfibert.cache import save_vtk_streamlines\n",
    "\n",
    "from dfibert.envs._state import TractographyState\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib notebook\n",
    "\n",
    "from train import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'dfibert.envs.RLtractEnvironment' from '../dfibert/envs/RLtractEnvironment.py'>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import importlib\n",
    "importlib.reload(RLTe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading precomputed streamlines (dti_ijk_0.8_maxDirecGetter.vtk) for ID 100307\n",
      "Repulsion100!\n",
      "Computing ODF\n"
     ]
    }
   ],
   "source": [
    "#env = RLTe.RLtractEnvironment(stepWidth=0.8, action_space=100, device = 'cpu', pReferenceStreamlines='data/HCP307200_DTI_min40.vtk')\n",
    "env = RLTe.RLtractEnvironment(stepWidth=0.8, action_space=100, device = 'cpu', \n",
    "                              pReferenceStreamlines='dti_ijk_0.8_maxDirecGetter.vtk', tracking_in_RAS = False)\n",
    "n_actions = env.action_space.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.interpolate import RegularGridInterpolator\n",
    "import dipy.reconst.dti as dti\n",
    "\n",
    "from dipy.direction import peaks_from_model\n",
    "from dipy.data import get_sphere\n",
    "\n",
    "\n",
    "# fit DTI model to data\n",
    "dti_model = dti.TensorModel(env.dataset.data.gtab, fit_method='LS')\n",
    "dti_fit = dti_model.fit(env.dataset.data.dwi, mask=env.dataset.data.binarymask)\n",
    "\n",
    "#TODO: Issue => are we using the correct data for tractography actually??? The data got 288 gradient directions\n",
    "# seems like its using the data of all bvals!!!\n",
    "mysphere = get_sphere('repulsion100')\n",
    "odf = dti_fit.odf(mysphere)\n",
    "\n",
    "## set up interpolator for directions\n",
    "x_range = np.arange(odf.shape[0])\n",
    "y_range = np.arange(odf.shape[1])\n",
    "z_range = np.arange(odf.shape[2])\n",
    "\n",
    "\n",
    "\n",
    "#affine = env.dataset.data.aff # tracking in RAS\n",
    "affine = np.eye(4) # tracking in IJK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dir_interpolator = RegularGridInterpolator((x_range,y_range,z_range), dir)\n",
    "odf_interpolator = RegularGridInterpolator((x_range,y_range,z_range), odf)\n",
    "fa_interpolator = RegularGridInterpolator((x_range,y_range,z_range), dti_fit.fa)\n",
    "#pd_interpolator = RegularGridInterpolator((x_range,y_range,z_range), peak_indices.peak_dirs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.dataset.get_fa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "peak_indices = peaks_from_model(\n",
    "    model=dti_model, data=env.dataset.data.dwi, sphere=mysphere, relative_peak_threshold=.2,\n",
    "    min_separation_angle=25, mask=env.dataset.data.binarymask, npeaks=2) # Peaks and Metrics object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dipy\n",
    "dg = dipy.direction.DeterministicMaximumDirectionGetter.from_shcoeff(peak_indices.shm_coeff, 80, peak_indices.sphere)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dipy.tracking.stopping_criterion import ThresholdStoppingCriterion\n",
    "fa_img = dti_fit.fa\n",
    "fa_img[np.isnan(fa_img)] = 0\n",
    "stopping_criterion = ThresholdStoppingCriterion(fa_img, .1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dipy.tracking import utils\n",
    "\n",
    "seed_mask = fa_img.copy()\n",
    "seed_mask[seed_mask >= 0.2] = 1\n",
    "seed_mask[seed_mask < 0.2] = 0\n",
    "\n",
    "seeds = utils.seeds_from_mask(seed_mask, affine=np.eye(4), density=1) # tracking in IJK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from dipy.tracking.local_tracking import LocalTracking\n",
    "from dipy.tracking.streamline import Streamlines\n",
    "\n",
    "# Initialize local tracking - computation happens in the next step. \n",
    "# EuDX, https://dipy.org/documentation/1.0.0./examples_built/tracking_introduction_eudx/#garyfallidis12\n",
    "# EuDx => change dg into pam\n",
    "streamlines_generator = LocalTracking(\n",
    "    dg, stopping_criterion, seeds, affine=np.eye(4), step_size=.8) # tracking in IJK\n",
    "\n",
    "# Generate streamlines object\n",
    "streamlines = Streamlines(streamlines_generator)\n",
    "streamlines[0]\n",
    "# tracked_streamlines = filter(lambda sl: len(sl) >= 10, tracked_streamlines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "streamlines_cropped = list(filter(lambda sl: len(sl) >= 10, streamlines))\n",
    "len(streamlines_cropped) / len(streamlines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_vtk_streamlines(streamlines=streamlines_cropped, filename=\"dti_ijk_0.8_maxDirecGetter.vtk\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ground-truth direction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cool_sl = 2\n",
    "idx = 4\n",
    "ref_sl = env.referenceStreamline_ijk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diff_vector = (ref_sl[idx+1] - ref_sl[idx])\n",
    "diff_vector_norm = diff_vector / torch.sqrt(torch.sum(diff_vector**2))\n",
    "\"gt\", diff_vector_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pv_norm = pd_interpolator(ref_sl[idx+1]) / np.sqrt(np.sum((pd_interpolator(ref_sl[idx+1])**2)))\n",
    "\n",
    "odf_max = np.argmax(odf_interpolator(ref_sl[idx]))\n",
    "odf_max_norm = mysphere.vertices[odf_max]\n",
    "\n",
    "#\"pv\", pv_norm, \n",
    "\"odf\", odf_max_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "odf_x = odf_interpolator(ref_sl[idx]).squeeze()\n",
    "plt.plot(odf_x,'.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start RL training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_steps = 30000000\n",
    "replay_memory_size = 100000\n",
    "agent_history_length = 1\n",
    "evaluate_every = 200000\n",
    "eval_runs = 5#20\n",
    "network_update_every = 10000\n",
    "start_learning = 10000\n",
    "eps_annealing_steps = 400000\n",
    "\n",
    "max_episode_length = 2000\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "batch_size = 512\n",
    "learning_rate = 0.000001 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = env.reset().getValue()\n",
    "print(state.shape)\n",
    "agent = Agent(n_actions=n_actions, inp_size=state.shape, device=device, hidden=10, gamma=0.99, agent_history_length=agent_history_length, memory_size=replay_memory_size, batch_size=batch_size, learning_rate=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#### Fill replay memory with perfect actions for supervised approach\n",
    "\n",
    "from tqdm import trange\n",
    "state = env.reset().getValue()\n",
    "agent = Agent(n_actions=n_actions, inp_size=state.shape, device=device, hidden=10, gamma=0.99, agent_history_length=agent_history_length, memory_size=replay_memory_size, batch_size=512, learning_rate=learning_rate)\n",
    "\n",
    "overall_runs = 0\n",
    "overall_reward = []\n",
    "for overall_runs in trange(60):\n",
    "    state = env.reset(streamline_index=overall_runs)\n",
    "    #episode_step_counter = 0\n",
    "    episode_reward = 0\n",
    "    terminal = False\n",
    "    #print(\"New run\")\n",
    "    #print(env.stepCounter, state.getCoordinate().numpy())\n",
    "    while not terminal:\n",
    "        #print(env.stepCounter)\n",
    "        #if np.random.rand(1) < 0.1: \n",
    "        #    action = np.random.randint(0, n_actions)\n",
    "        #else:\n",
    "        action = env._get_best_action()\n",
    "        next_state, reward, terminal, _ = env.step(action)\n",
    "        \n",
    "            \n",
    "        agent.replay_memory.add_experience(action=action,\n",
    "                                state = state.getValue(),\n",
    "                                reward = reward,\n",
    "                                new_state = next_state.getValue(),\n",
    "                                terminal=terminal)\n",
    "        \n",
    "        episode_reward += reward\n",
    "        \n",
    "        state = next_state\n",
    "        \n",
    "        if terminal == True:\n",
    "            break\n",
    "            \n",
    "    overall_runs += 1\n",
    "    overall_reward.append(episode_reward)\n",
    "    print(overall_runs, np.mean(overall_reward[-100:]))\n",
    "print(\"Replay memory ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.optim.Adam(agent.target_dqn.parameters(), 0.0001)\n",
    "losses = []\n",
    "for i in trange(70000):\n",
    "    states, actions, _, _, _ = agent.replay_memory.get_minibatch()\n",
    "\n",
    "    states = torch.FloatTensor(states).to(agent.device)\n",
    "    actions = torch.LongTensor(actions).to(agent.device)\n",
    "    predicted_q = agent.main_dqn(states)\n",
    "    loss = torch.nn.functional.cross_entropy(predicted_q, actions)\n",
    "    #print(loss.item())\n",
    "    agent.optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    agent.optimizer.step()\n",
    "    losses.append(loss.item())\n",
    "    \n",
    "mean_losses = []\n",
    "for i in range(len(losses)):\n",
    "    mean_losses.append(np.mean(losses[i:i+99]))\n",
    "#print(mean_losses[-20:])\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(range(len(losses[:])), losses[:])\n",
    "ax.plot(range(len(losses[:])), mean_losses[:])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Debug data  generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "\n",
    "import gym\n",
    "from gym.spaces import Discrete, Box\n",
    "import numpy as np\n",
    "\n",
    "from dipy.data import get_sphere\n",
    "from dipy.data import HemiSphere, Sphere\n",
    "from dipy.core.sphere import disperse_charges\n",
    "import torch\n",
    "\n",
    "\n",
    "from dfibert.data.postprocessing import res100, resample\n",
    "from dfibert.data import HCPDataContainer, ISMRMDataContainer, PointOutsideOfDWIError\n",
    "from dfibert.tracker import StreamlinesFromFileTracker\n",
    "from dfibert.util import get_grid\n",
    "\n",
    "import shapely.geometry as geom\n",
    "from shapely.ops import nearest_points\n",
    "from shapely.strtree import STRtree\n",
    "\n",
    "\n",
    "from collections import deque\n",
    "\n",
    "dataset = HCPDataContainer('100307')\n",
    "dataset.normalize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coord, data = next_state.getCoordinate(), next_state.getValue()\n",
    "grid = get_grid(np.array([3,3,3]))\n",
    "ras_points = env.dataset.to_ras(coord)\n",
    "ras_points = grid + ras_points\n",
    "\n",
    "interpolated_dwi = env.dataset.get_interpolated_dwi(ras_points, postprocessing=None)\n",
    "#interpolated_dwi = np.rollaxis(interpolated_dwi,3)\n",
    "\n",
    "dti_fit = dti_model.fit(interpolated_dwi)\n",
    "mysphere = get_sphere('repulsion100')\n",
    "odf = dti_fit.odf(mysphere)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(range(100), np.mean(odf.reshape(-1,100), axis=0))\n",
    "plt.plot(range(100), odf[1,1,1,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interpolated_dwi.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "states, actions, _, _, _ = agent.replay_memory.get_minibatch()\n",
    "states = torch.FloatTensor(states).to(agent.device)\n",
    "predicted_q = torch.argmax(agent.main_dqn(states), dim=1)\n",
    "\n",
    "false = 0\n",
    "for i in range(len(actions)):\n",
    "    if predicted_q[i] != actions[i]:\n",
    "        false += 1 \n",
    "    \n",
    "print(\"Accuracy =\", 1 - false / len(actions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "step_counter = 0\n",
    "eps_rewards = []\n",
    "episode_lengths = []\n",
    "\n",
    "eps = 1.0\n",
    "\n",
    "print(\"Start training...\")\n",
    "while step_counter < max_steps:\n",
    "    epoch_step = 0\n",
    "    while (epoch_step < evaluate_every) or (step_counter < start_learning):\n",
    "        state = env.reset()\n",
    "        episode_reward_sum = 0\n",
    "        terminal = False\n",
    "        episode_step_counter = 0\n",
    "        positive_run = 0\n",
    "        points_visited = 0\n",
    "        \n",
    "        negative_rewards = 0\n",
    "        \n",
    "        \n",
    "        # reduce epsilon\n",
    "        if step_counter > start_learning:\n",
    "            eps = max(eps * 0.999, 0.01)\n",
    "        \n",
    "        # play an episode\n",
    "        while episode_step_counter <= 1000.:\n",
    "            \n",
    "            # get an action with epsilon-greedy strategy\n",
    "            if random.random() < eps:                                 \n",
    "                action = np.random.randint(env.action_space.n)           # either random action\n",
    "                #action = env._get_best_action()\n",
    "            else:                                                        # or action from agent\n",
    "                agent.main_dqn.eval()\n",
    "                with torch.no_grad():\n",
    "                    state_v = torch.from_numpy(state.getValue()).unsqueeze(0).float().to(device)\n",
    "                    action = torch.argmax(agent.main_dqn(state_v)).item()\n",
    "                agent.main_dqn.train()\n",
    "            \n",
    "            # perform step on environment\n",
    "            next_state, reward, terminal, _ = env.step(action)\n",
    "\n",
    "            \n",
    "            episode_step_counter += 1\n",
    "            step_counter += 1\n",
    "            epoch_step += 1\n",
    "            \n",
    "            episode_reward_sum += reward\n",
    "            \n",
    "            # store experience in replay buffer\n",
    "            agent.replay_memory.add_experience(action=action, state = state.getValue(), reward=reward, new_state = next_state.getValue(), terminal=terminal)\n",
    "            \n",
    "            state = next_state\n",
    "            \n",
    "            # optimize agent after certain amount of steps\n",
    "            if step_counter > start_learning and step_counter % 4 == 0:\n",
    "                \n",
    "                # original optimization function\n",
    "                #agent.optimize()\n",
    "                \n",
    "                ### debugging optimization function\n",
    "                \n",
    "                states, actions, rewards, new_states, terminal_flags = agent.replay_memory.get_minibatch()\n",
    "                \n",
    "                #states = torch.tensor(states)#.view(replay_memory.batch_size, -1) # 1, -1\n",
    "                #next_states = torch.tensor(new_states)#.view(replay_memory.batch_size, -1)\n",
    "                #actions = torch.LongTensor(actions)\n",
    "                #rewards = torch.tensor(rewards)\n",
    "                #terminal_flags = torch.BoolTensor(terminal_flags)\n",
    "\n",
    "                states = torch.from_numpy(states).to(device)\n",
    "                next_states = torch.from_numpy(new_states).to(device)\n",
    "                actions = torch.from_numpy(actions).unsqueeze(1).long().to(device)\n",
    "                rewards = torch.from_numpy(rewards).to(device)\n",
    "                terminal_flags = torch.from_numpy(terminal_flags).to(device)\n",
    "                \n",
    "                \n",
    "                state_action_values = agent.main_dqn(states).gather(1, actions).squeeze(-1)\n",
    "                next_state_actions = torch.argmax(agent.main_dqn(next_states), dim=1)\n",
    "                next_state_values = agent.target_dqn(next_states).gather(1, next_state_actions.unsqueeze(-1)).squeeze(-1)\n",
    "                #\n",
    "                next_state_values[terminal_flags] = 0.0\n",
    "                #\n",
    "                expected_state_action_values = next_state_values.detach() * 0.9995 + rewards\n",
    "                #\n",
    "                loss = F.smooth_l1_loss(state_action_values, expected_state_action_values)\n",
    "                agent.optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                agent.optimizer.step()\n",
    "                \n",
    "            # update target network after certain amount of steps    \n",
    "            if step_counter > start_learning and step_counter % network_update_every == 0:\n",
    "                agent.target_dqn.load_state_dict(agent.main_dqn.state_dict())\n",
    "            \n",
    "            # if epsiode has ended, step out of the episode while loop\n",
    "            if terminal:\n",
    "                break\n",
    "                \n",
    "        # keep track of past episode rewards\n",
    "        eps_rewards.append(episode_reward_sum)\n",
    "        if len(eps_rewards) % 20 == 0:\n",
    "            print(\"{}, done {} episodes, {}, current eps {}\".format(step_counter, len(eps_rewards), np.mean(eps_rewards[-100:]), eps))#action_scheduler.eps_current))\n",
    "            \n",
    "    ## evaluation        \n",
    "    eval_rewards = []\n",
    "    episode_final = 0\n",
    "    agent.main_dqn.eval()\n",
    "    for _ in range(eval_runs):\n",
    "        eval_steps = 0\n",
    "        state = env.reset()\n",
    "        \n",
    "        eval_episode_reward = 0\n",
    "        negative_rewards = 0\n",
    "        \n",
    "        # play an episode\n",
    "        while eval_steps < 1000:\n",
    "            # get the action from the agent\n",
    "            with torch.no_grad():\n",
    "                    state_v = torch.from_numpy(state.getValue()).unsqueeze(0).float().to(device)\n",
    "                    action = torch.argmax(agent.main_dqn(state_v)).item()\n",
    "                  \n",
    "            # perform a step on the environment\n",
    "            next_state, reward, terminal, _ = env.step(action)\n",
    "            \n",
    "            eval_steps += 1\n",
    "            \n",
    "            eval_episode_reward += reward\n",
    "            state = next_state\n",
    "            \n",
    "            # step out of the episode while loop if \n",
    "            if terminal:\n",
    "                terminal = False\n",
    "                if reward == 1.:\n",
    "                    episode_final += 1\n",
    "                break\n",
    "\n",
    "        eval_rewards.append(eval_episode_reward)\n",
    "\n",
    "    print(\"Evaluation score:\", np.mean(eval_rewards))\n",
    "    print(\"{} of {} episodes ended close to / at the final state.\".format(episode_final, eval_runs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation of trained agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "paths = glob.glob('test_newReward/checkpoints/*.pt') # new reward with penalty when leaving brain mask\n",
    "\n",
    "p_cp = max(paths, key=os.path.getctime)\n",
    "model, step_counter, mean_reward, epsilon = load_model(p_cp)\n",
    "\n",
    "agent.main_dqn.load_state_dict(model)\n",
    "agent.target_dqn.load_state_dict(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.dataset.get_fa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sphere_verts_torch = torch.from_numpy(mysphere.vertices)\n",
    "\n",
    "def get_multi_best_action(current_direction, odf_interpolator, my_position, sphere, sphere_verts_torch, K = 3):\n",
    "    \n",
    "    # main peak from ODF\n",
    "    reward = get_multi_best_action_ODF(odf_interpolator, my_position, sphere, K)\n",
    "            \n",
    "    if(current_direction is not None):\n",
    "        reward = reward * (torch.nn.functional.cosine_similarity(sphere_verts_torch, current_direction)).view(1,-1)\n",
    "    \n",
    "    reward = torch.max(reward, axis = 0).values\n",
    "    best_action = torch.argmax(reward)\n",
    "    print(\"Max reward: %.2f\" % (torch.max(reward).cpu().detach().numpy()))\n",
    "    return best_action\n",
    "\n",
    "def get_best_action(current_direction, odf_interpolator, my_position, sphere, sphere_verts_torch, K = 2):\n",
    "    #odf = torch.from_numpy(odf_interpolator(my_position).squeeze())\n",
    "    \n",
    "    # main peak from ODF\n",
    "    peak_dir = get_best_action_ODF(odf_interpolator, my_position, sphere)\n",
    "    #peak_dir = get_best_action_pd(odf_interpolator, my_position, sphere)\n",
    "    \n",
    "    # cosine similarity wrt. all directions\n",
    "    reward = abs(torch.nn.functional.cosine_similarity(torch.from_numpy(peak_dir).view(1,-1), sphere_verts_torch))\n",
    "        \n",
    "    if(current_direction is not None):\n",
    "        reward = reward * (torch.nn.functional.cosine_similarity(sphere_verts_torch, current_direction))\n",
    "    \n",
    "    best_action = torch.argmax(reward)\n",
    "    print(\"Max reward: %.2f\" % (torch.max(reward).cpu().detach().numpy()))\n",
    "    return best_action\n",
    "\n",
    "def get_best_action_ODF(odf_interpolator, my_position, sphere):\n",
    "    '''\n",
    "    ODF computation at 3x3x3 grid\n",
    "    #coolsl0_odf = odf_interpolator(my_position).squeeze()\n",
    "    #coord, data = next_state.getCoordinate(), next_state.getValue()\n",
    "    #grid = get_grid(np.array([3,3,3]))\n",
    "    ras_points = env.dataset.to_ras(my_position)\n",
    "    ras_points = grid + ras_points\n",
    "\n",
    "    interpolated_dwi = env.dataset.get_interpolated_dwi(ras_points, postprocessing=None)\n",
    "\n",
    "    dti_fit = dti_model.fit(interpolated_dwi)\n",
    "    coolsl0_odf = dti_fit.odf(sphere)\n",
    "    coolsl0_odf = np.mean(coolsl0_odf.reshape(-1,100), axis=0)\n",
    "    '''\n",
    "    \n",
    "    # ODF interpolation\n",
    "    coolsl0_odf = odf_interpolator(my_position).squeeze()\n",
    "    \n",
    "    best_action = np.argmax(coolsl0_odf)\n",
    "    peak_dir = sphere.vertices[best_action]\n",
    "    return peak_dir\n",
    "\n",
    "def get_multi_best_action_ODF(odf_interpolator, my_position, sphere, K = 3):\n",
    "    my_odf = odf_interpolator(my_position).squeeze()\n",
    "\n",
    "    k_largest = np.argpartition(my_odf.squeeze(),-K)[-K:]\n",
    "    peak_dirs_torch = torch.from_numpy(sphere.vertices[k_largest]).view(K,3)\n",
    "    rewards = torch.stack([abs(torch.nn.functional.cosine_similarity(peak_dirs_torch[k:k+1,:], sphere_verts_torch.view(-1, 3))) for k in range(K)])\n",
    "    \n",
    "    '''rewards = torch.stack([torch.nn.functional.cosine_similarity(peak_dirs_torch[0:1,:], sphere_verts_torch.view(-1, 3)),\n",
    "             torch.nn.functional.cosine_similarity(peak_dirs_torch[1:2,:], sphere_verts_torch.view(-1, 3)),\n",
    "             torch.nn.functional.cosine_similarity(peak_dirs_torch[2:3,:], sphere_verts_torch.view(-1, 3))\n",
    "             ])\n",
    "    '''\n",
    "    return rewards\n",
    "\n",
    "def get_best_action_pd(pd_interpolator, my_position, sphere):\n",
    "    peak_dir = pd_interpolator(my_position)[0,0]\n",
    "    return peak_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_rewards = []\n",
    "all_distances = []\n",
    "all_states = []\n",
    "l2s = []\n",
    "max_episode_length = 10\n",
    "streamline_index = 11\n",
    "fa_threshold = 0.1\n",
    "K = 3\n",
    "\n",
    "#agent.main_dqn.eval()\n",
    "for _ in range(1):\n",
    "    eval_steps = 0\n",
    "    state = env.reset(streamline_index=streamline_index)\n",
    "    next_state = state\n",
    "    #state = env.reset()\n",
    "    #print(state.getCoordinate())\n",
    "    all_states.append(state.getCoordinate())\n",
    "    #transition = init_transition()\n",
    "    #all_states.append(torch.tensor(list(transition)[:3]))\n",
    "    eval_episode_reward = 0\n",
    "    episode_final = 0\n",
    "    #print(env.referenceStreamline_ijk[:6])\n",
    "    while eval_steps < max_episode_length:\n",
    "        '''\n",
    "        #action = torch.argmax(main_dqn(torch.FloatTensor(state.getValue()).unsqueeze(0).to(device)))\n",
    "        #action = env._get_best_action()\n",
    "        with torch.no_grad():\n",
    "            state_v = torch.from_numpy(state.getValue()).unsqueeze(0).float().to(device)\n",
    "            action = torch.argmax(agent.main_dqn(state_v)).item()\n",
    "        '''\n",
    "        #action = get_best_action(next_state, env, dir_interpolator)\n",
    "        \n",
    "        my_position = all_states[-1]\n",
    "        current_direction = None\n",
    "        \n",
    "        if(eval_steps > 0):\n",
    "            # compute tangent of previous step\n",
    "            current_direction = all_states[-1] - all_states[-2]\n",
    "            current_direction = current_direction / torch.sqrt(torch.sum(current_direction**2))\n",
    "            current_direction = current_direction.view(1,3)\n",
    "        \n",
    "        #action = get_multi_best_action(current_direction, odf_interpolator, my_position, mysphere, sphere_verts_torch, K = K)\n",
    "        action = env._get_best_action(current_direction, my_position)\n",
    "\n",
    "        \n",
    "        next_state, reward, terminal, _ = env.step(action)\n",
    "        \n",
    "        eval_episode_reward += reward\n",
    "        print(eval_steps, action, next_state.getCoordinate().numpy(), reward)       \n",
    "        eval_steps += 1\n",
    "        \n",
    "        #if(env.line.distance(geom.Point(next_state.getCoordinate())) > 0.1):\n",
    "        #    print(\"We left our streamline. Switching to closest one\")\n",
    "        #    break\n",
    "        l2s.append(env.l2_distance.detach().cpu().numpy())\n",
    "        \n",
    "        if eval_steps == 1000:\n",
    "            terminal = True\n",
    "        all_distances.append(reward)\n",
    "        all_states.append(next_state.getCoordinate())\n",
    "        \n",
    "        \n",
    "        fa_x = fa_interpolator(next_state.getCoordinate())\n",
    "        if(fa_x < fa_threshold):\n",
    "            print(\"fa_threshold reached.. terminated. %.2f\" % (fa_x))\n",
    "            terminal = True\n",
    "        \n",
    "        state = next_state\n",
    "        if terminal:\n",
    "            terminal = False\n",
    "            break\n",
    "\n",
    "    eval_rewards.append(eval_episode_reward)\n",
    "\n",
    "print(\"Evaluation score:\", np.min(eval_rewards))\n",
    "\n",
    "########################\n",
    "### visualise streamline\n",
    "########################\n",
    "%matplotlib notebook \n",
    "state = env.reset(streamline_index=streamline_index) \n",
    "\n",
    "states = torch.stack(all_states)\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = plt.axes(projection='3d')\n",
    "ax.plot3D(env.referenceStreamline_ijk.T[0][0:max_episode_length+1], env.referenceStreamline_ijk.T[1][0:max_episode_length+1], env.referenceStreamline_ijk.T[2][0:max_episode_length+1], '-*')\n",
    "ax.plot3D(states.T[0], states.T[1], states.T[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_largest.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.argmax(my_odf.squeeze())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "all_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### visualise streamline\n",
    "%matplotlib notebook \n",
    "state = env.reset(streamline_index=50) \n",
    "\n",
    "states = torch.stack(all_states)\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = plt.axes(projection='3d')\n",
    "ax.plot3D(env.referenceStreamline_ijk.T[0][0:10], env.referenceStreamline_ijk.T[1][0:10], env.referenceStreamline_ijk.T[2][0:10])\n",
    "ax.plot3D(states.T[0][0:10], states.T[1][0:10], states.T[2][0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "767d51c1340bd893661ea55ea3124f6de3c7a262a8b4abca0554b478b1e2ff90"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
