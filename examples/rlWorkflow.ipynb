{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import gym\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "\n",
    "import os, sys\n",
    "sys.path.insert(0,'..')\n",
    "\n",
    "from collections import deque \n",
    "\n",
    "from dfibert.tracker.nn.rl import Agent, DQN\n",
    "import dfibert.envs.RLtractEnvironment as RLTe\n",
    "from dfibert.cache import save_vtk_streamlines\n",
    "from dfibert.envs._state import TractographyState\n",
    "from tqdm import trange\n",
    "from dfibert.data import ISMRMDataContainer \n",
    "from dipy.tracking import utils\n",
    "import dipy.reconst.dti as dti\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib notebook\n",
    "\n",
    "#from train import load_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset #  100307\n",
      "sphere_odf = sphere_action = repulsion100\n",
      "Computing ODF\n"
     ]
    }
   ],
   "source": [
    "env = RLTe.RLtractEnvironment(stepWidth=0.8, action_space=100, dataset = '100307', device = 'cpu', seeds = None, tracking_in_RAS = False, odf_state = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env._init_shmcoeff()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tracking\n",
    "## Analysis of tracking on subset of streamlines\n",
    "The next cell carries out a ground-truth tracking workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "streamlines = []\n",
    "for i in trange(100):\n",
    "    terminal = False\n",
    "    all_states = []\n",
    "    state = env.reset(seed_index=i)\n",
    "    current_direction = None\n",
    "    all_states.append(state.getCoordinate().numpy())\n",
    "    terminal = False\n",
    "    while not terminal:\n",
    "        my_position = state.getCoordinate().double().squeeze(0)\n",
    "        action = env._get_best_action(current_direction, my_position)\n",
    "        current_direction = env.directions[action].numpy()\n",
    "        state, reward, terminal, _  = env.step(action.reshape(-1,1))\n",
    "        all_states.append(state.getCoordinate().squeeze(0).numpy())\n",
    "\n",
    "    state = env.reset(seed_index=i, terminal_F=True)\n",
    "    current_direction = None\n",
    "    terminal = False\n",
    "    all_states = all_states[::-1]\n",
    "    while not terminal:\n",
    "        my_position = state.getCoordinate().double().squeeze(0)\n",
    "        action = env._get_best_action(current_direction, my_position)\n",
    "        current_direction = env.directions[action].numpy()\n",
    "        #action = gt_actions[i]\n",
    "        state, reward, terminal, _  = env.step(action.reshape(-1,1), direction=\"backward\")\n",
    "        if False in torch.eq(state.getCoordinate().squeeze(0), my_position):\n",
    "            all_states.append(state.getCoordinate().squeeze(0).numpy())\n",
    "            \n",
    "    streamlines.append(np.asarray(all_states))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize our streamlines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "streamline_index = 9\n",
    "streamline_np = np.stack(streamlines[streamline_index])\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = plt.axes(projection='3d')\n",
    "#ax.plot3D(env.referenceStreamline_ijk.T[0], env.referenceStreamline_ijk.T[1], env.referenceStreamline_ijk.T[2], '-*')\n",
    "ax.plot3D(streamline_np[:,0], streamline_np[:,1], streamline_np[:,2])\n",
    "#plt.legend(['gt', 'agent'])\n",
    "plt.legend('agent')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation on ISMRM data\n",
    "The next cell conducts tracking on ISMRM evaluation data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = ISMRMDataContainer()\n",
    "\n",
    "dti_model = dti.TensorModel(dataset.data.gtab, fit_method='LS')\n",
    "dti_fit = dti_model.fit(dataset.data.dwi, mask=dataset.data.binarymask)\n",
    "fa_img = dti_fit.fa\n",
    "\n",
    "seed_mask = fa_img.copy()\n",
    "seed_mask[seed_mask >= 0.2] = 1\n",
    "seed_mask[seed_mask < 0.2] = 0\n",
    "\n",
    "seeds = utils.seeds_from_mask(seed_mask, affine=np.eye(4), density=1) # tracking in IJK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset #  100307\n",
      "sphere_odf = sphere_action = repulsion100\n",
      "Computing ODF\n",
      "Computing ODF\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<dfibert.envs._state.TractographyState at 0x2aaab7e891d0>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env = RLTe.RLtractEnvironment(stepWidth=0.8, action_space=100, device = 'cpu', seeds = torch.FloatTensor(seeds), tracking_in_RAS = False, odf_state = False)\n",
    "env.dataset = dataset\n",
    "env.dataset.generate_fa()\n",
    "env._init_odf()\n",
    "env.reset(seed_index=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 56234/56234 [09:42<00:00, 96.47it/s] \n"
     ]
    }
   ],
   "source": [
    "streamlines = []\n",
    "for i in trange(len(seeds)):\n",
    "    terminal = False\n",
    "    all_states = []\n",
    "    state = env.reset(seed_index=i)\n",
    "    current_direction = None\n",
    "    all_states.append(state.getCoordinate().numpy())\n",
    "    terminal = False\n",
    "    eval_steps = 0\n",
    "    while not terminal:        \n",
    "        # current position\n",
    "        my_position = state.getCoordinate().double().squeeze(0)\n",
    "        # get best choice from environment\n",
    "        action = env._get_best_action(current_direction, my_position)\n",
    "        # store tangent for next time step\n",
    "        current_direction = env.directions[action].numpy()\n",
    "        # take a step\n",
    "        state, reward, terminal, _  = env.step(action)\n",
    "        all_states.append(state.getCoordinate().squeeze(0).numpy())\n",
    "        eval_steps = eval_steps + 1\n",
    "\n",
    "    state = env.reset(seed_index=i, terminal_F=True)\n",
    "    current_direction = None\n",
    "    terminal = False\n",
    "    all_states = all_states[::-1]\n",
    "    while not terminal:\n",
    "        # current position\n",
    "        my_position = state.getCoordinate().double().squeeze(0)\n",
    "        # get best choice from environment\n",
    "        action = env._get_best_action(current_direction, my_position)\n",
    "        # store tangent for next time step\n",
    "        current_direction = env.directions[action].numpy()\n",
    "        # take a step\n",
    "        state, reward, terminal, _  = env.step(action, direction=\"backward\")\n",
    "        if False in torch.eq(state.getCoordinate().squeeze(0), my_position):\n",
    "            all_states.append(state.getCoordinate().squeeze(0).numpy())\n",
    "            \n",
    "    streamlines.append(np.asarray(all_states))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "streamline_index = 0\n",
    "streamline_np = np.stack(streamlines[streamline_index])\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = plt.axes(projection='3d')\n",
    "ax.plot3D(streamline_np[:,0], streamline_np[:,1], streamline_np[:,2])\n",
    "plt.legend('agent')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforcement Learning\n",
    "## DQN\n",
    "\n",
    "WIP code !!! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "streamline_index = 0\n",
    "device = \"cpu\"\n",
    "max_steps = 30000000\n",
    "replay_memory_size = 100000\n",
    "agent_history_length = 1\n",
    "evaluate_every = 200000\n",
    "eval_runs = 5#20\n",
    "network_update_every = 10000\n",
    "start_learning = 10000\n",
    "eps_annealing_steps = 400000\n",
    "\n",
    "max_episode_length = 2000\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "batch_size = 512\n",
    "learning_rate = 0.000001 \n",
    "\n",
    "\n",
    "state = env.reset(seed_index=streamline_index)\n",
    "env.referenceStreamline_ijk, state.getCoordinate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = Agent(n_actions=100, inp_size=state.getValue().shape, device=device, hidden=10, gamma=0.99, \n",
    "              agent_history_length=agent_history_length, \n",
    "              memory_size=replay_memory_size, batch_size=batch_size, learning_rate=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fill replay memory \n",
    "The replay memory consists of perfect actions for supervised pre-training of our agent. This allows us to take leverage on knowledge and significantly speedup convergence of the training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from tqdm import trange\n",
    "state = env.reset().getValue()\n",
    "\n",
    "overall_runs = 0\n",
    "overall_reward = []\n",
    "for overall_runs in trange(600):\n",
    "    state = env.reset(seed_index=overall_runs)\n",
    "    #episode_step_counter = 0\n",
    "    episode_reward = 0\n",
    "    terminal = False\n",
    "    current_direction = None\n",
    "    #print(\"New run\")\n",
    "    #print(env.stepCounter, state.getCoordinate().numpy())\n",
    "    while not terminal:\n",
    "        my_position = state.getCoordinate().double().squeeze(0)\n",
    "        #print(env.stepCounter)\n",
    "        #if np.random.rand(1) < 0.1: \n",
    "        #    action = np.random.randint(0, n_actions)\n",
    "        #else:\n",
    "        \n",
    "        # current position\n",
    "        my_position = state.getCoordinate().double().squeeze(0)\n",
    "        \n",
    "        action = env._get_best_action(current_direction, my_position)\n",
    "\n",
    "        current_direction = env.directions[action].numpy()\n",
    "        \n",
    "        next_state, reward, terminal, _ = env.step(action)\n",
    "        print(\"Reward: \", reward)\n",
    "        agent.replay_memory.add_experience(action=action,\n",
    "                                state = state.getValue(),\n",
    "                                reward=reward,\n",
    "                                new_state = next_state.getValue(),\n",
    "                                terminal=terminal)\n",
    "        \n",
    "        episode_reward += reward\n",
    "        \n",
    "        state = next_state\n",
    "        \n",
    "        if terminal == True:\n",
    "            break\n",
    "            \n",
    "    overall_runs += 1\n",
    "    overall_reward.append(episode_reward)\n",
    "    print(overall_runs, np.mean(overall_reward[-100:]))\n",
    "print(\"Replay memory ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following cell, we are testing the accuracy of our agent right after initialisation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "states, actions, _, _, _ = agent.replay_memory.get_minibatch()\n",
    "states = torch.FloatTensor(states).to(agent.device)\n",
    "predicted_q = torch.argmax(agent.main_dqn(states), dim=1)\n",
    "\n",
    "false = 0\n",
    "for i in range(len(actions)):\n",
    "    if predicted_q[i] != actions[i]:\n",
    "        false += 1 \n",
    "    \n",
    "print(\"Accuracy =\", 1 - false / len(actions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "step_counter = 0\n",
    "eps_rewards = []\n",
    "episode_lengths = []\n",
    "\n",
    "eps = 1.0\n",
    "\n",
    "print(\"Start training...\")\n",
    "while step_counter < max_steps:\n",
    "    epoch_step = 0\n",
    "    while (epoch_step < evaluate_every) or (step_counter < start_learning):\n",
    "        state = env.reset()\n",
    "        episode_reward_sum = 0\n",
    "        terminal = False\n",
    "        episode_step_counter = 0\n",
    "        positive_run = 0\n",
    "        points_visited = 0\n",
    "        \n",
    "        negative_rewards = 0\n",
    "        \n",
    "        \n",
    "        # reduce epsilon\n",
    "        if step_counter > start_learning:\n",
    "            eps = max(eps * 0.999, 0.01)\n",
    "        \n",
    "        # play an episode\n",
    "        while episode_step_counter <= 1000.:\n",
    "            \n",
    "            # get an action with epsilon-greedy strategy\n",
    "            if random.random() < eps:                                 \n",
    "                action = np.random.randint(env.action_space.n)           # either random action\n",
    "                #action = env._get_best_action(current_direction, my_position)\n",
    "            else:                                                        # or action from agent\n",
    "                agent.main_dqn.eval()\n",
    "                with torch.no_grad():\n",
    "                    state_v = torch.from_numpy(state.getValue()).unsqueeze(0).float().to(device)\n",
    "                    action = torch.argmax(agent.main_dqn(state_v)).item()\n",
    "                agent.main_dqn.train()\n",
    "            \n",
    "            # perform step on environment\n",
    "            next_state, reward, terminal, _ = env.step(action)\n",
    "\n",
    "            \n",
    "            episode_step_counter += 1\n",
    "            step_counter += 1\n",
    "            epoch_step += 1\n",
    "            \n",
    "            episode_reward_sum += reward\n",
    "            \n",
    "            # store experience in replay buffer\n",
    "            agent.replay_memory.add_experience(action=action, state = state.getValue(), reward=reward, new_state = next_state.getValue(), terminal=terminal)\n",
    "            \n",
    "            state = next_state\n",
    "            \n",
    "            # optimize agent after certain amount of steps\n",
    "            if step_counter > start_learning and step_counter % 4 == 0:\n",
    "                \n",
    "                # original optimization function\n",
    "                #agent.optimize()\n",
    "                \n",
    "                ### debugging optimization function\n",
    "                \n",
    "                states, actions, rewards, new_states, terminal_flags = agent.replay_memory.get_minibatch()\n",
    "                \n",
    "                #states = torch.tensor(states)#.view(replay_memory.batch_size, -1) # 1, -1\n",
    "                #next_states = torch.tensor(new_states)#.view(replay_memory.batch_size, -1)\n",
    "                #actions = torch.LongTensor(actions)\n",
    "                #rewards = torch.tensor(rewards)\n",
    "                #terminal_flags = torch.BoolTensor(terminal_flags)\n",
    "\n",
    "                states = torch.from_numpy(states).to(device)\n",
    "                next_states = torch.from_numpy(new_states).to(device)\n",
    "                actions = torch.from_numpy(actions).unsqueeze(1).long().to(device)\n",
    "                rewards = torch.from_numpy(rewards).to(device)\n",
    "                terminal_flags = torch.from_numpy(terminal_flags).to(device)\n",
    "                \n",
    "                \n",
    "                state_action_values = agent.main_dqn(states).gather(1, actions).squeeze(-1)\n",
    "                next_state_actions = torch.argmax(agent.main_dqn(next_states), dim=1)\n",
    "                next_state_values = agent.target_dqn(next_states).gather(1, next_state_actions.unsqueeze(-1)).squeeze(-1)\n",
    "                #\n",
    "                next_state_values[terminal_flags] = 0.0\n",
    "                #\n",
    "                expected_state_action_values = next_state_values.detach() * 0.9995 + rewards\n",
    "                #\n",
    "                loss = F.smooth_l1_loss(state_action_values, expected_state_action_values)\n",
    "                agent.optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                agent.optimizer.step()\n",
    "                \n",
    "            # update target network after certain amount of steps    \n",
    "            if step_counter > start_learning and step_counter % network_update_every == 0:\n",
    "                agent.target_dqn.load_state_dict(agent.main_dqn.state_dict())\n",
    "            \n",
    "            # if epsiode has ended, step out of the episode while loop\n",
    "            if terminal:\n",
    "                break\n",
    "                \n",
    "        # keep track of past episode rewards\n",
    "        eps_rewards.append(episode_reward_sum)\n",
    "        if len(eps_rewards) % 20 == 0:\n",
    "            print(\"{}, done {} episodes, {}, current eps {}\".format(step_counter, len(eps_rewards), np.mean(eps_rewards[-100:]), eps))#action_scheduler.eps_current))\n",
    "\n",
    "            \n",
    "    ##########################\n",
    "    ##########################\n",
    "    ## evaluation#############\n",
    "    ##########################\n",
    "    ##########################\n",
    "    eval_rewards = []\n",
    "    episode_final = 0\n",
    "    agent.main_dqn.eval()\n",
    "    for _ in range(eval_runs):\n",
    "        eval_steps = 0\n",
    "        state = env.reset()\n",
    "        \n",
    "        eval_episode_reward = 0\n",
    "        negative_rewards = 0\n",
    "        \n",
    "        # play an episode\n",
    "        while eval_steps < 1000:\n",
    "            # get the action from the agent\n",
    "            with torch.no_grad():\n",
    "                    state_v = torch.from_numpy(state.getValue()).unsqueeze(0).float().to(device)\n",
    "                    action = torch.argmax(agent.main_dqn(state_v)).item()\n",
    "                  \n",
    "            # perform a step on the environment\n",
    "            next_state, reward, terminal, _ = env.step(action)\n",
    "            \n",
    "            eval_steps += 1\n",
    "            \n",
    "            eval_episode_reward += reward\n",
    "            state = next_state\n",
    "            \n",
    "            # step out of the episode while loop if \n",
    "            if terminal:\n",
    "                terminal = False\n",
    "                if reward == 1.:\n",
    "                    episode_final += 1\n",
    "                break\n",
    "\n",
    "        eval_rewards.append(eval_episode_reward)\n",
    "\n",
    "    print(\"Evaluation score:\", np.mean(eval_rewards))\n",
    "    print(\"{} of {} episodes ended close to / at the final state.\".format(episode_final, eval_runs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
