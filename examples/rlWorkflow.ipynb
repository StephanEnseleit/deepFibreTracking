{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import random\n",
    "import sys\n",
    "sys.path.insert(0,'..')\n",
    "\n",
    "\n",
    "from dfibert.tracker.nn.rl import Agent\n",
    "import dfibert.envs.RLTractEnvironment as RLTe\n",
    "from dfibert.tracker import save_streamlines\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib notebook\n",
    "\n",
    "#from train import load_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# I. HCP Tracking\n",
    "The environment is able to run tracking on a fixed set of datasets. At the moment, it is able to load HCP data as well as ISMRM data. The following cells shows the initalisation of our environment on HCP dataset `100307` while seed points are automatically determined at voxels with fa-value >= 0.2 via `seeds = None`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = RLTe.RLTractEnvironment(step_width=0.8, dataset = '100307',\n",
    "                              device = 'cpu', seeds = None, tracking_in_RAS = False,\n",
    "                              odf_state = False, odf_mode = \"DTI\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "streamlines = env.track()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also directly visualize our streamlines in this notebook by `ax.plot3d`. However, a single streamline is typically very hard to comprehend so this is merely one tool to qualitatively reason about major bugs in our tracking code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "streamline_index = 9\n",
    "streamline_np = np.stack(streamlines[streamline_index])\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = plt.axes(projection='3d')\n",
    "#ax.plot3D(env.referenceStreamline_ijk.T[0], env.referenceStreamline_ijk.T[1], env.referenceStreamline_ijk.T[2], '-*')\n",
    "ax.plot3D(streamline_np[:,0], streamline_np[:,1], streamline_np[:,2])\n",
    "#plt.legend(['gt', 'agent'])\n",
    "plt.legend('agent')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# II. Evaluation of Cortico Spinal Tract @ ISMRM benchmark data\n",
    "We will now be using our environment along with our reward function to track streamlines on the ISMRM dataset. For this purpose, we first initialise our environment and set seed points to the cortico spinal tract. We precomputed seed points in IJK for our ISMRM dataset. These seeds will now be loaded into our environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "seeds_CST = np.load('data/ismrm_seeds_CST.npy')\n",
    "seeds_CST = torch.from_numpy(seeds_CST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset #  ISMRM\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hoffma83/.local/lib/python3.8/site-packages/dipy/core/sphere.py:187: UserWarning: Vertices are not on the unit sphere.\n",
      "  warnings.warn(\"Vertices are not on the unit sphere.\")\n"
     ]
    }
   ],
   "source": [
    "env = RLTe.RLTractEnvironment(dataset = 'ISMRM', max_angle = 80., step_width=0.8,\n",
    "                            device = 'cpu', seeds = seeds_CST,\n",
    "                              tracking_in_RAS = False, odf_state = False, odf_mode = \"DTI\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tracking itself can now be done by basically calling the `.track()` function that tracks our streamlines from each of the provided seed points in a forward and backward direciton."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/5701 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialising ODF\n",
      "DTI-based ODF computation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 13/5701 [00:05<16:43,  5.67it/s] /home/hoffma83/Code/RLtract/deepFibreTracking/examples/../dfibert/envs/RLTractEnvironment.py:285: RuntimeWarning: invalid value encountered in true_divide\n",
      "  reward = pmf_cur / np.max(pmf_cur)\n",
      "100%|██████████| 5701/5701 [07:33<00:00, 12.57it/s]\n"
     ]
    }
   ],
   "source": [
    "streamlines = env.track()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The streamlines are now stored as VTK file. The nice thing about this format is that we can directly import the streamlines into 3dSlicer via the slicer-dMRI extension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_streamlines(streamlines=streamlines, path=\"ismrm_cst_ijk_naRew.vtk\")\n",
    "streamlines_ras = [env.dataset.to_ras(sl) for sl in streamlines]\n",
    "save_streamlines(streamlines=streamlines_ras, path=\"ismrm_cst_ras_naRew.vtk\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforcement Learning\n",
    "## DQN\n",
    "\n",
    "WIP code !!! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "streamline_index = 0\n",
    "device = \"cpu\"\n",
    "max_steps = 30000000\n",
    "replay_memory_size = 100000\n",
    "agent_history_length = 1\n",
    "evaluate_every = 200000\n",
    "eval_runs = 5#20\n",
    "network_update_every = 10000\n",
    "start_learning = 10000\n",
    "eps_annealing_steps = 400000\n",
    "\n",
    "max_episode_length = 2000\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "batch_size = 512\n",
    "learning_rate = 0.000001 \n",
    "\n",
    "\n",
    "state = env.reset(seed_index=streamline_index)\n",
    "env.referenceStreamline_ijk, state.getCoordinate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = Agent(n_actions=100, inp_size=state.getValue().shape, device=device, hidden=10, gamma=0.99, \n",
    "              agent_history_length=agent_history_length, \n",
    "              memory_size=replay_memory_size, batch_size=batch_size, learning_rate=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fill replay memory \n",
    "The replay memory consists of perfect actions for supervised pre-training of our agent. This allows us to take leverage on knowledge and significantly speedup convergence of the training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from tqdm import trange\n",
    "state = env.reset().getValue()\n",
    "\n",
    "overall_runs = 0\n",
    "overall_reward = []\n",
    "for overall_runs in trange(600):\n",
    "    state = env.reset(seed_index=overall_runs)\n",
    "    #episode_step_counter = 0\n",
    "    episode_reward = 0\n",
    "    terminal = False\n",
    "    current_direction = None\n",
    "    #print(\"New run\")\n",
    "    #print(env.stepCounter, state.getCoordinate().numpy())\n",
    "    while not terminal:\n",
    "        my_position = state.getCoordinate().double().squeeze(0)\n",
    "        #print(env.stepCounter)\n",
    "        #if np.random.rand(1) < 0.1: \n",
    "        #    action = np.random.randint(0, n_actions)\n",
    "        #else:\n",
    "        \n",
    "        # current position\n",
    "        my_position = state.getCoordinate().double().squeeze(0)\n",
    "        \n",
    "        action = env._get_best_action(current_direction, my_position)\n",
    "\n",
    "        current_direction = env.directions[action].numpy()\n",
    "        \n",
    "        next_state, reward, terminal, _ = env.step(action)\n",
    "        print(\"Reward: \", reward)\n",
    "        agent.replay_memory.add_experience(action=action,\n",
    "                                state = state.getValue(),\n",
    "                                reward=reward,\n",
    "                                new_state = next_state.getValue(),\n",
    "                                terminal=terminal)\n",
    "        \n",
    "        episode_reward += reward\n",
    "        \n",
    "        state = next_state\n",
    "        \n",
    "        if terminal == True:\n",
    "            break\n",
    "            \n",
    "    overall_runs += 1\n",
    "    overall_reward.append(episode_reward)\n",
    "    print(overall_runs, np.mean(overall_reward[-100:]))\n",
    "print(\"Replay memory ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following cell, we are testing the accuracy of our agent right after initialisation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "states, actions, _, _, _ = agent.replay_memory.get_minibatch()\n",
    "states = torch.FloatTensor(states).to(agent.device)\n",
    "predicted_q = torch.argmax(agent.main_dqn(states), dim=1)\n",
    "\n",
    "false = 0\n",
    "for i in range(len(actions)):\n",
    "    if predicted_q[i] != actions[i]:\n",
    "        false += 1 \n",
    "    \n",
    "print(\"Accuracy =\", 1 - false / len(actions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "step_counter = 0\n",
    "eps_rewards = []\n",
    "episode_lengths = []\n",
    "\n",
    "eps = 1.0\n",
    "\n",
    "print(\"Start training...\")\n",
    "while step_counter < max_steps:\n",
    "    epoch_step = 0\n",
    "    while (epoch_step < evaluate_every) or (step_counter < start_learning):\n",
    "        state = env.reset()\n",
    "        episode_reward_sum = 0\n",
    "        terminal = False\n",
    "        episode_step_counter = 0\n",
    "        positive_run = 0\n",
    "        points_visited = 0\n",
    "        \n",
    "        negative_rewards = 0\n",
    "        \n",
    "        \n",
    "        # reduce epsilon\n",
    "        if step_counter > start_learning:\n",
    "            eps = max(eps * 0.999, 0.01)\n",
    "        \n",
    "        # play an episode\n",
    "        while episode_step_counter <= 1000.:\n",
    "            \n",
    "            # get an action with epsilon-greedy strategy\n",
    "            if random.random() < eps:                                 \n",
    "                action = np.random.randint(env.action_space.n)           # either random action\n",
    "                #action = env._get_best_action(current_direction, my_position)\n",
    "            else:                                                        # or action from agent\n",
    "                agent.main_dqn.eval()\n",
    "                with torch.no_grad():\n",
    "                    state_v = torch.from_numpy(state.getValue()).unsqueeze(0).float().to(device)\n",
    "                    action = torch.argmax(agent.main_dqn(state_v)).item()\n",
    "                agent.main_dqn.train()\n",
    "            \n",
    "            # perform step on environment\n",
    "            next_state, reward, terminal, _ = env.step(action)\n",
    "\n",
    "            \n",
    "            episode_step_counter += 1\n",
    "            step_counter += 1\n",
    "            epoch_step += 1\n",
    "            \n",
    "            episode_reward_sum += reward\n",
    "            \n",
    "            # store experience in replay buffer\n",
    "            agent.replay_memory.add_experience(action=action, state = state.getValue(), reward=reward, new_state = next_state.getValue(), terminal=terminal)\n",
    "            \n",
    "            state = next_state\n",
    "            \n",
    "            # optimize agent after certain amount of steps\n",
    "            if step_counter > start_learning and step_counter % 4 == 0:\n",
    "                \n",
    "                # original optimization function\n",
    "                #agent.optimize()\n",
    "                \n",
    "                ### debugging optimization function\n",
    "                \n",
    "                states, actions, rewards, new_states, terminal_flags = agent.replay_memory.get_minibatch()\n",
    "                \n",
    "                #states = torch.tensor(states)#.view(replay_memory.batch_size, -1) # 1, -1\n",
    "                #next_states = torch.tensor(new_states)#.view(replay_memory.batch_size, -1)\n",
    "                #actions = torch.LongTensor(actions)\n",
    "                #rewards = torch.tensor(rewards)\n",
    "                #terminal_flags = torch.BoolTensor(terminal_flags)\n",
    "\n",
    "                states = torch.from_numpy(states).to(device)\n",
    "                next_states = torch.from_numpy(new_states).to(device)\n",
    "                actions = torch.from_numpy(actions).unsqueeze(1).long().to(device)\n",
    "                rewards = torch.from_numpy(rewards).to(device)\n",
    "                terminal_flags = torch.from_numpy(terminal_flags).to(device)\n",
    "                \n",
    "                \n",
    "                state_action_values = agent.main_dqn(states).gather(1, actions).squeeze(-1)\n",
    "                next_state_actions = torch.argmax(agent.main_dqn(next_states), dim=1)\n",
    "                next_state_values = agent.target_dqn(next_states).gather(1, next_state_actions.unsqueeze(-1)).squeeze(-1)\n",
    "                #\n",
    "                next_state_values[terminal_flags] = 0.0\n",
    "                #\n",
    "                expected_state_action_values = next_state_values.detach() * 0.9995 + rewards\n",
    "                #\n",
    "                loss = F.smooth_l1_loss(state_action_values, expected_state_action_values)\n",
    "                agent.optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                agent.optimizer.step()\n",
    "                \n",
    "            # update target network after certain amount of steps    \n",
    "            if step_counter > start_learning and step_counter % network_update_every == 0:\n",
    "                agent.target_dqn.load_state_dict(agent.main_dqn.state_dict())\n",
    "            \n",
    "            # if epsiode has ended, step out of the episode while loop\n",
    "            if terminal:\n",
    "                break\n",
    "                \n",
    "        # keep track of past episode rewards\n",
    "        eps_rewards.append(episode_reward_sum)\n",
    "        if len(eps_rewards) % 20 == 0:\n",
    "            print(\"{}, done {} episodes, {}, current eps {}\".format(step_counter, len(eps_rewards), np.mean(eps_rewards[-100:]), eps))#action_scheduler.eps_current))\n",
    "\n",
    "            \n",
    "    ##########################\n",
    "    ##########################\n",
    "    ## evaluation#############\n",
    "    ##########################\n",
    "    ##########################\n",
    "    eval_rewards = []\n",
    "    episode_final = 0\n",
    "    agent.main_dqn.eval()\n",
    "    for _ in range(eval_runs):\n",
    "        eval_steps = 0\n",
    "        state = env.reset()\n",
    "        \n",
    "        eval_episode_reward = 0\n",
    "        negative_rewards = 0\n",
    "        \n",
    "        # play an episode\n",
    "        while eval_steps < 1000:\n",
    "            # get the action from the agent\n",
    "            with torch.no_grad():\n",
    "                    state_v = torch.from_numpy(state.getValue()).unsqueeze(0).float().to(device)\n",
    "                    action = torch.argmax(agent.main_dqn(state_v)).item()\n",
    "                  \n",
    "            # perform a step on the environment\n",
    "            next_state, reward, terminal, _ = env.step(action)\n",
    "            \n",
    "            eval_steps += 1\n",
    "            \n",
    "            eval_episode_reward += reward\n",
    "            state = next_state\n",
    "            \n",
    "            # step out of the episode while loop if \n",
    "            if terminal:\n",
    "                terminal = False\n",
    "                if reward == 1.:\n",
    "                    episode_final += 1\n",
    "                break\n",
    "\n",
    "        eval_rewards.append(eval_episode_reward)\n",
    "\n",
    "    print(\"Evaluation score:\", np.mean(eval_rewards))\n",
    "    print(\"{} of {} episodes ended close to / at the final state.\".format(episode_final, eval_runs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py_ml",
   "language": "python",
   "name": "py_ml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
