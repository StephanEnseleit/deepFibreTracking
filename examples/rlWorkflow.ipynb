{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import gym\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "\n",
    "import os, sys\n",
    "sys.path.insert(0,'..')\n",
    "\n",
    "from collections import deque \n",
    "\n",
    "from dfibert.tracker.nn.rl import Agent, DQN\n",
    "import dfibert.envs.RLtractEnvironment as RLTe\n",
    "from dfibert.cache import save_vtk_streamlines\n",
    "from dfibert.envs._state import TractographyState\n",
    "from tqdm import trange\n",
    "from dfibert.data import ISMRMDataContainer \n",
    "from dipy.tracking import utils\n",
    "import dipy.reconst.dti as dti\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib notebook\n",
    "\n",
    "#from train import load_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset #  100307\n",
      "sphere_odf = sphere_action = repulsion100\n",
      "Computing ODF\n"
     ]
    }
   ],
   "source": [
    "env = RLTe.RLtractEnvironment(stepWidth=0.8, action_space=100, dataset = '100307', device = 'cpu', seeds = None, tracking_in_RAS = False, odf_state = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "env._init_shmcoeff()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tracking\n",
    "## Analysis of tracking on single streamline\n",
    "The next cell carries out a ground-truth tracking workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "streamlines = []\n",
    "for i in trange(100):\n",
    "    terminal = False\n",
    "    all_states = []\n",
    "    state = env.reset(seed_index=i)\n",
    "    current_direction = None\n",
    "    all_states.append(state.getCoordinate().numpy())\n",
    "    terminal = False\n",
    "    while not terminal:\n",
    "        my_position = state.getCoordinate().double().squeeze(0)\n",
    "        action = env._get_best_action(current_direction, my_position.numpy())\n",
    "        current_direction = env.directions[action].numpy()\n",
    "        state, reward, terminal, _  = env.step(action.reshape(-1,1))\n",
    "        all_states.append(state.getCoordinate().squeeze(0).numpy())\n",
    "\n",
    "    state = env.reset(seed_index=i, terminal_F=True)\n",
    "    current_direction = None\n",
    "    terminal = False\n",
    "    all_states = all_states[::-1]\n",
    "    while not terminal:\n",
    "        my_position = state.getCoordinate().double().squeeze(0)\n",
    "        action = env._get_best_action(current_direction, my_position.numpy())\n",
    "        current_direction = env.directions[action].numpy()\n",
    "        #action = gt_actions[i]\n",
    "        state, reward, terminal, _  = env.step(action.reshape(-1,1), direction=\"backward\")\n",
    "        if False in torch.eq(state.getCoordinate().squeeze(0), my_position):\n",
    "            all_states.append(state.getCoordinate().squeeze(0).numpy())\n",
    "            \n",
    "    streamlines.append(np.asarray(all_states))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize our streamlines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "streamline_index = 3\n",
    "streamline_np = np.stack(streamlines[streamline_index])\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = plt.axes(projection='3d')\n",
    "#ax.plot3D(env.referenceStreamline_ijk.T[0], env.referenceStreamline_ijk.T[1], env.referenceStreamline_ijk.T[2], '-*')\n",
    "ax.plot3D(streamline_np[:,0], streamline_np[:,1], streamline_np[:,2])\n",
    "#plt.legend(['gt', 'agent'])\n",
    "plt.legend('agent')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation on ISMRM data\n",
    "The next cell conducts tracking on ISMRM evaluation data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = ISMRMDataContainer()\n",
    "\n",
    "dti_model = dti.TensorModel(dataset.data.gtab, fit_method='LS')\n",
    "dti_fit = dti_model.fit(dataset.data.dwi, mask=dataset.data.binarymask)\n",
    "fa_img = dti_fit.fa\n",
    "\n",
    "seed_mask = fa_img.copy()\n",
    "seed_mask[seed_mask >= 0.2] = 1\n",
    "seed_mask[seed_mask < 0.2] = 0\n",
    "\n",
    "seeds = utils.seeds_from_mask(seed_mask, affine=np.eye(4), density=1) # tracking in IJK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = RLTe.RLtractEnvironment(stepWidth=0.8, action_space=100, device = 'cpu', seeds = torch.FloatTensor(seeds), tracking_in_RAS = False, odf_state = False)\n",
    "env.dataset = dataset\n",
    "env.dataset.generate_fa()\n",
    "env._init_odf()\n",
    "env.reset(seed_index=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "streamlines = []\n",
    "for i in trange(len(seeds)):\n",
    "    terminal = False\n",
    "    all_states = []\n",
    "    state = env.reset(seed_index=i)\n",
    "    current_direction = None\n",
    "    all_states.append(state.getCoordinate().numpy())\n",
    "    terminal = False\n",
    "    while not terminal:\n",
    "        my_position = state.getCoordinate().double().squeeze(0)\n",
    "        #print(action)\n",
    "        action = env._get_best_action(current_direction, my_position)\n",
    "        current_direction = env.directions[action].numpy()\n",
    "        #action = gt_actions[i]\n",
    "        state, reward, terminal, _  = env.step(action)\n",
    "        all_states.append(state.getCoordinate().squeeze(0).numpy())\n",
    "\n",
    "    state = env.reset(seed_index=i, terminal_F=True)\n",
    "    #print(env.seed_index)\n",
    "    current_direction = None\n",
    "    terminal = False\n",
    "    all_states = all_states[::-1]\n",
    "    while not terminal:\n",
    "        my_position = state.getCoordinate().double().squeeze(0)\n",
    "        action = env._get_best_action(current_direction, my_position)\n",
    "        current_direction = env.directions[action].numpy()\n",
    "        #action = gt_actions[i]\n",
    "        state, reward, terminal, _  = env.step(action, direction=\"backward\")\n",
    "        if False in torch.eq(state.getCoordinate().squeeze(0), my_position):\n",
    "            all_states.append(state.getCoordinate().squeeze(0).numpy())\n",
    "            \n",
    "    streamlines.append(np.asarray(all_states))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "streamline_index = 0\n",
    "streamline_np = np.stack(streamlines[streamline_index])\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = plt.axes(projection='3d')\n",
    "#ax.plot3D(env.referenceStreamline_ijk.T[0], env.referenceStreamline_ijk.T[1], env.referenceStreamline_ijk.T[2], '-*')\n",
    "ax.plot3D(streamline_np[:,0], streamline_np[:,1], streamline_np[:,2])\n",
    "#plt.legend(['gt', 'agent'])\n",
    "plt.legend('agent')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforcement Learning\n",
    "## DQN\n",
    "\n",
    "WIP code !!! Here by dragons :) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "streamline_index = 0\n",
    "device = \"cpu\"\n",
    "max_steps = 30000000\n",
    "replay_memory_size = 100000\n",
    "agent_history_length = 1\n",
    "evaluate_every = 200000\n",
    "eval_runs = 5#20\n",
    "network_update_every = 10000\n",
    "start_learning = 10000\n",
    "eps_annealing_steps = 400000\n",
    "\n",
    "max_episode_length = 2000\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "batch_size = 512\n",
    "learning_rate = 0.000001 \n",
    "\n",
    "\n",
    "state = env.reset(seed_index=streamline_index)\n",
    "env.referenceStreamline_ijk, state.getCoordinate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = Agent(n_actions=20, inp_size=state.getValue().shape, device=device, hidden=10, gamma=0.99, \n",
    "              agent_history_length=agent_history_length, \n",
    "              memory_size=replay_memory_size, batch_size=batch_size, learning_rate=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training loop is run in the next cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_rewards = []\n",
    "all_distances = []\n",
    "all_states = []\n",
    "l2s = []\n",
    "max_episode_length = 15\n",
    "fa_threshold = 0.1\n",
    "K = 3\n",
    "\n",
    "#agent.main_dqn.eval()\n",
    "for _ in range(1):\n",
    "    eval_steps = 0\n",
    "    state = env.reset(seed_index=streamline_index)\n",
    "    next_state = state\n",
    "    all_states.append(state.getCoordinate())\n",
    "    eval_episode_reward = 0\n",
    "    episode_final = 0\n",
    "    while eval_steps < max_episode_length:\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            state_v = torch.from_numpy(state.getValue()).unsqueeze(0).float().to(device)\n",
    "            action = torch.argmax(agent.main_dqn(state_v)).item()\n",
    "        \n",
    "        my_position = all_states[-1]\n",
    "        current_direction = None\n",
    "        \n",
    "        if(eval_steps > 0):\n",
    "            # compute tangent of previous step\n",
    "            current_direction = all_states[-1] - all_states[-2]\n",
    "            current_direction = current_direction / torch.sqrt(torch.sum(current_direction**2))\n",
    "            current_direction = current_direction.view(1,3)\n",
    "        \n",
    "        #action = get_multi_best_action(current_direction, odf_interpolator, my_position, mysphere, sphere_verts_torch, K = K)\n",
    "        action = env._get_best_action(current_direction, my_position)\n",
    "\n",
    "        \n",
    "        next_state, reward, terminal, _ = env.step(action)\n",
    "        \n",
    "        #reward = reward\n",
    "        \n",
    "        print(eval_steps, my_position, \"=>\", next_state.getCoordinate().numpy(), action, reward)\n",
    "\n",
    "        \n",
    "        all_distances.append(reward)\n",
    "        all_states.append(next_state.getCoordinate().squeeze())\n",
    "                \n",
    "        state = next_state\n",
    "        print(\"---\")\n",
    "        if terminal:\n",
    "            terminal = False\n",
    "            break\n",
    "            \n",
    "        eval_episode_reward += reward.squeeze()\n",
    "        eval_steps += 1\n",
    "\n",
    "    eval_rewards.append(eval_episode_reward)\n",
    "\n",
    "print(\"Evaluation score:\", np.min(eval_rewards))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
