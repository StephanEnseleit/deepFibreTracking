{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import gym\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "import os, sys\n",
    "sys.path.insert(0,'..')\n",
    "\n",
    "from collections import deque \n",
    "\n",
    "from dfibert.tracker.nn.rl import Agent, Action_Scheduler\n",
    "import dfibert.envs.RLtractEnvironment as RLTe\n",
    "from dfibert.envs._state import TractographyState"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test on Lunar Lander to check functionality of agent\n",
    "#env = gym.make('LunarLander-v2')\n",
    "#n_actions= env.action_space.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_steps = 3000000\n",
    "replay_memory_size = 30000\n",
    "agent_history_length = 1\n",
    "evaluate_every = 20000\n",
    "eval_runs = 20\n",
    "network_update_every = 1500\n",
    "start_learning = 700\n",
    "\n",
    "max_episode_length = 200\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "learning_rate = 0.0001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading precomputed streamlines (data/HCP307200_DTI_smallSet.vtk) for ID 100307\n"
     ]
    }
   ],
   "source": [
    "env = RLTe.RLtractEnvironment(device = 'cpu')\n",
    "n_actions = env.action_space.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_transition():\n",
    "    state = env.reset()\n",
    "    transition = deque(maxlen=12)\n",
    "    while len(transition) < 12:\n",
    "        for i in range(len(state.getCoordinate())):\n",
    "            transition.append(state.getCoordinate()[i].item())\n",
    "    return transition\n",
    "\n",
    "def add_to_transition(state, transition):\n",
    "    for i in range(len(state.getCoordinate())):\n",
    "            transition.append(state.getCoordinate()[i].item())\n",
    "    return transition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([43.4998, 94.5377, 24.2661])\n"
     ]
    }
   ],
   "source": [
    "state = env.reset()\n",
    "print(state.getCoordinate())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "deque([73.6513442993164, 107.88105773925781, 93.29415130615234, 73.6513442993164, 107.88105773925781, 93.29415130615234, 73.6513442993164, 107.88105773925781, 93.29415130615234, 73.6513442993164, 107.88105773925781, 93.29415130615234], maxlen=12)\n",
      "[ 73.6513443  107.88105774  93.29415131  73.6513443  107.88105774\n",
      "  93.29415131  73.6513443  107.88105774  93.29415131  74.58926433\n",
      " 108.1431821   93.52130066]\n"
     ]
    }
   ],
   "source": [
    "transition = init_transition()\n",
    "print(transition)\n",
    "next_state, _, _ = env.step(42)\n",
    "next_transition = add_to_transition(next_state, transition)\n",
    "print(np.array(next_transition))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-13.6672,  -7.8499,  -5.0823,  14.6562,  -2.7858,  -0.6165,  10.3545,\n",
      "          -2.1052,  -6.4893,  -6.9334,  -1.0502,  -5.7020, -10.8006,  -2.3180,\n",
      "         -11.9924,  -0.8486,   3.1842,  -3.2134,   2.1775,  -0.9403,   3.2331,\n",
      "           1.7278,  -3.3707, -10.6123,   8.4084, -17.0882,  -3.0332,  -3.0189,\n",
      "          13.1050,  -5.0179,   7.5423,   4.0707,  -8.9227,  -2.1233,  10.4102,\n",
      "         -17.7410,  12.1755,  -0.2614,   4.6796,   9.3646,   5.0021,  10.0463,\n",
      "          -2.7939,  -9.3681,  -5.5738,  -3.8558, -13.2895,   8.2217,   1.9178,\n",
      "           9.1176,  -4.2744,   8.5829,   9.8390,   4.5286,  -8.5778,   4.4940,\n",
      "          -2.8302,  18.6438,  -1.0051,   1.0747,  -9.0311,   2.8591,   9.1929,\n",
      "          -3.2381,  -1.0803,  -8.8236,  -7.3553, -10.1507, -11.5579,   7.1048,\n",
      "          -4.4199,   5.1859,  13.8964,   1.0838,  -7.0013,  -1.5684,   3.1538,\n",
      "          -3.0316,   8.9216,   7.6663,   3.3042,  -7.4507,  14.3557,  -6.6734,\n",
      "         -13.6536,  -1.9597,  -6.0354,  -0.6945,  -5.4558,  -7.2604,  -2.2367,\n",
      "           5.1702,   1.4439,  -1.3349, -13.6596,   1.7791,   7.1995,  -4.0517,\n",
      "          16.7861, -15.0391]], device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor([[-13.6672,  -7.8499,  -5.0823,  14.6562,  -2.7858,  -0.6165,  10.3545,\n",
      "          -2.1052,  -6.4893,  -6.9334,  -1.0502,  -5.7020, -10.8006,  -2.3180,\n",
      "         -11.9924,  -0.8486,   3.1842,  -3.2134,   2.1775,  -0.9403,   3.2331,\n",
      "           1.7278,  -3.3707, -10.6123,   8.4084, -17.0882,  -3.0332,  -3.0189,\n",
      "          13.1050,  -5.0179,   7.5423,   4.0707,  -8.9227,  -2.1233,  10.4102,\n",
      "         -17.7410,  12.1755,  -0.2614,   4.6796,   9.3646,   5.0021,  10.0463,\n",
      "          -2.7939,  -9.3681,  -5.5738,  -3.8558, -13.2895,   8.2217,   1.9178,\n",
      "           9.1176,  -4.2744,   8.5829,   9.8390,   4.5286,  -8.5778,   4.4940,\n",
      "          -2.8302,  18.6438,  -1.0051,   1.0747,  -9.0311,   2.8591,   9.1929,\n",
      "          -3.2381,  -1.0803,  -8.8236,  -7.3553, -10.1507, -11.5579,   7.1048,\n",
      "          -4.4199,   5.1859,  13.8964,   1.0838,  -7.0013,  -1.5684,   3.1538,\n",
      "          -3.0316,   8.9216,   7.6663,   3.3042,  -7.4507,  14.3557,  -6.6734,\n",
      "         -13.6536,  -1.9597,  -6.0354,  -0.6945,  -5.4558,  -7.2604,  -2.2367,\n",
      "           5.1702,   1.4439,  -1.3349, -13.6596,   1.7791,   7.1995,  -4.0517,\n",
      "          16.7861, -15.0391]], device='cuda:0', grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "agent = Agent(n_actions=n_actions, inp_size=np.array(transition).shape, device=device, hidden=10, agent_history_length=agent_history_length, memory_size=replay_memory_size, batch_size=32, learning_rate=learning_rate)\n",
    "#print(agent.main_dqn(torch.FloatTensor([np.array(transition)]).to(device)))\n",
    "#print(agent.target_dqn(torch.FloatTensor([np.array(transition)]).to(device)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 8])\n",
      "tensor([[0.2314, 0.2480, 0.3690, 0.1016]], device='cuda:0',\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([[0.2314, 0.2480, 0.3690, 0.1016]], device='cuda:0',\n",
      "       grad_fn=<AddBackward0>)\n",
      "[ 0.01214266  1.4017366   0.60866606 -0.2167907  -0.01216728 -0.1019486\n",
      "  0.          0.        ] 0.0867171307642434 False\n",
      "tensor([[0.2314, 0.2480, 0.3690, 0.1016]], device='cuda:0',\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0.2480], device='cuda:0', grad_fn=<IndexBackward>)\n",
      "tensor([2], device='cuda:0')\n",
      "tensor([0.3673], device='cuda:0', grad_fn=<IndexBackward>)\n",
      "tensor([0.3673], device='cuda:0', grad_fn=<IndexPutBackward>)\n",
      "tensor([0.4503], device='cuda:0')\n",
      "tensor(0.0205, device='cuda:0', grad_fn=<SmoothL1LossBackward>)\n",
      "tensor([[0.2505, 0.2896, 0.3830, 0.1175]], device='cuda:0',\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([[0.2314, 0.2480, 0.3690, 0.1016]], device='cuda:0',\n",
      "       grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Debugging the optimization of the agent\n",
    "\n",
    "state = env.reset()\n",
    "state = torch.tensor([state]).to(device).float()\n",
    "print(state.shape)\n",
    "print(agent.main_dqn(state))\n",
    "print(agent.target_dqn(state))\n",
    "\n",
    "action = 1\n",
    "\n",
    "next_state, reward, done, _ = env.step(action)\n",
    "print(next_state, reward, done)\n",
    "\n",
    "action = torch.tensor([action]).to(device)\n",
    "next_state = torch.tensor([next_state]).float().to(device)\n",
    "reward = torch.tensor([reward]).float().to(device)\n",
    "done = torch.BoolTensor([done]).to(device)\n",
    "state_action_values = agent.main_dqn(state)\n",
    "print(state_action_values)\n",
    "state_action_values = state_action_values[0][action]\n",
    "print(state_action_values)\n",
    "\n",
    "next_state_actions = agent.main_dqn(next_state).max(1)[1]\n",
    "print(next_state_actions)\n",
    "next_state_values = agent.target_dqn(next_state)[0][next_state_actions]\n",
    "print(next_state_values)\n",
    "next_state_values[done] = 0.0\n",
    "print(next_state_values)\n",
    "expected_state_action_values = next_state_values.detach() * 0.99 + reward\n",
    "print(expected_state_action_values)\n",
    "\n",
    "agent.optimizer.zero_grad()\n",
    "loss = torch.nn.SmoothL1Loss()(state_action_values, expected_state_action_values)\n",
    "print(loss)\n",
    "loss.backward()\n",
    "agent.optimizer.step()\n",
    "\n",
    "print(agent.main_dqn(state))\n",
    "print(agent.target_dqn(state))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ToyEnv(gym.Env):\n",
    "     def __init__(self):\n",
    "            self.possible_states = []\n",
    "            self.state, self."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training...\n",
      "255, done 100 episodes, -91.52257676215422, current eps 1 0\n",
      "487, done 200 episodes, -93.74538980099233, current eps 1 0\n",
      "722, done 300 episodes, -92.84386526448344, current eps 0.9993139999999999 0\n",
      "963, done 400 episodes, -94.27903659944688, current eps 0.9914413333333333 0\n",
      "1205, done 500 episodes, -92.19572983533861, current eps 0.983536 0\n",
      "1446, done 600 episodes, -92.13998960303218, current eps 0.9756633333333332 0\n",
      "1694, done 700 episodes, -91.80794335959835, current eps 0.9675619999999999 0\n",
      "1954, done 800 episodes, -91.53070089451913, current eps 0.9590686666666666 0\n",
      "2200, done 900 episodes, -92.94762733928825, current eps 0.9510326666666666 0\n",
      "2443, done 1000 episodes, -93.08632689534697, current eps 0.9430946666666666 0\n",
      "2679, done 1100 episodes, -93.7654386284055, current eps 0.9353853333333333 0\n",
      "2924, done 1200 episodes, -92.28232436813649, current eps 0.9273819999999999 0\n",
      "3179, done 1300 episodes, -92.75655589469227, current eps 0.919052 0\n",
      "3444, done 1400 episodes, -92.10768250613586, current eps 0.9103953333333332 0\n",
      "3707, done 1500 episodes, -90.8824868715941, current eps 0.9018039999999999 0\n",
      "3949, done 1600 episodes, -93.49526776346453, current eps 0.8938986666666666 0\n",
      "4196, done 1700 episodes, -94.82505786451235, current eps 0.8858299999999999 0\n",
      "4438, done 1800 episodes, -93.85914281263895, current eps 0.8779246666666666 0\n",
      "4681, done 1900 episodes, -92.51985422460285, current eps 0.8699866666666666 0\n",
      "4930, done 2000 episodes, -91.3419568089439, current eps 0.8618526666666666 0\n",
      "5203, done 2100 episodes, -89.1840102042933, current eps 0.8529346666666666 0\n",
      "5451, done 2200 episodes, -92.14700547468762, current eps 0.8448333333333333 0\n",
      "5711, done 2300 episodes, -92.13067604615064, current eps 0.83634 0\n",
      "5981, done 2400 episodes, -88.211303362628, current eps 0.8275199999999999 0\n",
      "6254, done 2500 episodes, -90.74610517717902, current eps 0.8186019999999999 0\n",
      "6515, done 2600 episodes, -91.77839403510708, current eps 0.8100759999999999 0\n",
      "6782, done 2700 episodes, -88.60672436175828, current eps 0.8013539999999999 0\n",
      "7051, done 2800 episodes, -91.32813431118973, current eps 0.7925666666666666 0\n",
      "7313, done 2900 episodes, -92.45805273405507, current eps 0.7840079999999999 0\n",
      "7577, done 3000 episodes, -92.02232318101049, current eps 0.775384 0\n",
      "7865, done 3100 episodes, -88.26346174890436, current eps 0.765976 0\n",
      "8139, done 3200 episodes, -88.67896867710344, current eps 0.7570253333333332 0\n",
      "8430, done 3300 episodes, -87.8687236000665, current eps 0.7475193333333332 0\n",
      "8704, done 3400 episodes, -91.54142985706429, current eps 0.7385686666666667 0\n",
      "8976, done 3500 episodes, -90.77205524812527, current eps 0.7296833333333332 0\n",
      "9258, done 3600 episodes, -89.63030310225902, current eps 0.7204713333333332 0\n",
      "9567, done 3700 episodes, -88.00505232803596, current eps 0.7103773333333333 0\n",
      "9856, done 3800 episodes, -91.4393701233139, current eps 0.7009366666666665 0\n",
      "10156, done 3900 episodes, -91.08522275794597, current eps 0.6911366666666666 0\n",
      "10432, done 4000 episodes, -89.7516111270197, current eps 0.6821206666666666 0\n",
      "10731, done 4100 episodes, -88.08114129449665, current eps 0.6723533333333334 0\n",
      "11031, done 4200 episodes, -91.022487271486, current eps 0.6625533333333333 0\n",
      "11313, done 4300 episodes, -88.52514792213087, current eps 0.6533413333333333 0\n",
      "11631, done 4400 episodes, -86.30131910420967, current eps 0.6429533333333333 0\n",
      "11884, done 4500 episodes, -92.24315380815273, current eps 0.6346886666666667 0\n",
      "12175, done 4600 episodes, -89.8997584445512, current eps 0.6251826666666667 0\n",
      "12497, done 4700 episodes, -92.18049067495949, current eps 0.614664 0\n",
      "12784, done 4800 episodes, -89.227035401912, current eps 0.6052886666666666 0\n",
      "13132, done 4900 episodes, -82.36222889481381, current eps 0.5939206666666667 0\n",
      "13439, done 5000 episodes, -89.16421599647312, current eps 0.583892 0\n",
      "13757, done 5100 episodes, -86.44066559507155, current eps 0.573504 0\n",
      "14086, done 5200 episodes, -87.39832014908137, current eps 0.5627566666666666 0\n",
      "14411, done 5300 episodes, -90.19800452873694, current eps 0.55214 0\n",
      "14712, done 5400 episodes, -91.4464908811434, current eps 0.5423073333333333 0\n",
      "14996, done 5500 episodes, -91.33045433492192, current eps 0.5330299999999999 0\n",
      "15300, done 5600 episodes, -86.78462679160539, current eps 0.5230993333333334 0\n",
      "15668, done 5700 episodes, -82.05007353360115, current eps 0.5110779999999999 0\n",
      "16026, done 5800 episodes, -87.23866854513248, current eps 0.4993833333333333 0\n",
      "16392, done 5900 episodes, -82.78991101803676, current eps 0.4874273333333333 0\n",
      "16735, done 6000 episodes, -86.14513086940278, current eps 0.4762226666666667 0\n",
      "17054, done 6100 episodes, -86.7884586827958, current eps 0.46580199999999994 0\n",
      "17423, done 6200 episodes, -81.22208368179, current eps 0.45374799999999993 0\n",
      "17762, done 6300 episodes, -85.06570846743962, current eps 0.442674 0\n",
      "18134, done 6400 episodes, -83.50811095424011, current eps 0.43052199999999996 0\n",
      "18469, done 6500 episodes, -88.20440734721818, current eps 0.41957866666666666 0\n",
      "18814, done 6600 episodes, -84.36246250469044, current eps 0.40830866666666665 0\n",
      "19162, done 6700 episodes, -82.33032280717764, current eps 0.3969406666666666 0\n",
      "19557, done 6800 episodes, -76.92810439024015, current eps 0.38403733333333334 0\n",
      "19889, done 6900 episodes, -86.26903329673996, current eps 0.37319199999999997 0\n",
      "Evaluation score: -54.021393969848724\n",
      "0 of 20 episodes ended close to / at the final state.\n",
      "20268, done 7000 episodes, -81.7234383910689, current eps 0.3608113333333334 0\n",
      "20647, done 7100 episodes, -82.73879986506877, current eps 0.34843066666666667 0\n",
      "21032, done 7200 episodes, -83.35316189472427, current eps 0.335854 0\n",
      "21377, done 7300 episodes, -83.90592467083479, current eps 0.324584 0\n",
      "21787, done 7400 episodes, -75.9328676154078, current eps 0.3111906666666666 0\n",
      "22186, done 7500 episodes, -79.21560227572964, current eps 0.2981566666666666 0\n",
      "22573, done 7600 episodes, -80.62030587167833, current eps 0.2855146666666667 0\n",
      "22955, done 7700 episodes, -82.14322953949832, current eps 0.27303599999999995 0\n",
      "23354, done 7800 episodes, -80.06947740684613, current eps 0.26000199999999996 0\n",
      "23774, done 7900 episodes, -80.05176237902386, current eps 0.246282 0\n",
      "24163, done 8000 episodes, -80.08583358259216, current eps 0.2335746666666667 0\n",
      "24598, done 8100 episodes, -77.3355967004231, current eps 0.21936466666666665 0\n",
      "24997, done 8200 episodes, -82.5507490379747, current eps 0.20633066666666666 0\n",
      "25410, done 8300 episodes, -79.86789750933863, current eps 0.19283933333333336 0\n",
      "25846, done 8400 episodes, -77.28445572852706, current eps 0.17859666666666663 0\n",
      "26220, done 8500 episodes, -83.07591229086972, current eps 0.16637933333333332 0\n",
      "26653, done 8600 episodes, -79.10412120548399, current eps 0.15223466666666663 0\n",
      "27133, done 8700 episodes, -73.61973928849659, current eps 0.1365546666666667 0\n",
      "27562, done 8800 episodes, -75.45972670552196, current eps 0.12254066666666663 0\n",
      "28095, done 8900 episodes, -72.25987220386646, current eps 0.1051293333333333 0\n",
      "28579, done 9000 episodes, -75.74706780939634, current eps 0.08931866666666666 0\n",
      "29050, done 9100 episodes, -76.41657667666877, current eps 0.07393266666666665 0\n",
      "29618, done 9200 episodes, -66.8578734851889, current eps 0.05537800000000004 0\n",
      "30180, done 9300 episodes, -64.07732319009052, current eps 0.037019333333333404 0\n",
      "30741, done 9400 episodes, -64.16935545214619, current eps 0.019999865288115044 0\n",
      "31265, done 9500 episodes, -70.61997065175883, current eps 0.01999810056242212 0\n",
      "31796, done 9600 episodes, -73.53496244367685, current eps 0.01999631226214933 0\n",
      "32295, done 9700 episodes, -71.2446259304758, current eps 0.019994631731384504 0\n",
      "32871, done 9800 episodes, -64.07593728112342, current eps 0.019992691880241137 0\n",
      "33449, done 9900 episodes, -64.96561608547249, current eps 0.01999074529350352 0\n",
      "34024, done 10000 episodes, -64.91935918043495, current eps 0.01998880881015728 0\n",
      "34558, done 10100 episodes, -65.99704623870926, current eps 0.019987010406493113 0\n",
      "35186, done 10200 episodes, -62.43654995743813, current eps 0.019984895429899306 0\n",
      "35790, done 10300 episodes, -72.23097268251868, current eps 0.019982861280436467 0\n",
      "36336, done 10400 episodes, -68.63385964525301, current eps 0.019981022463206818 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36952, done 10500 episodes, -59.86310136694818, current eps 0.019978947900178495 0\n",
      "37522, done 10600 episodes, -70.64578301335708, current eps 0.01997702825581787 0\n",
      "38152, done 10700 episodes, -67.54090552518903, current eps 0.019974906543629812 0\n",
      "38706, done 10800 episodes, -69.29872323930005, current eps 0.019973040784023174 0\n",
      "39323, done 10900 episodes, -62.76175148346458, current eps 0.019970962853197726 0\n",
      "Evaluation score: -44.577124970683585\n",
      "0 of 20 episodes ended close to / at the final state.\n",
      "40016, done 11000 episodes, -65.40294873750577, current eps 0.01996862896979086 0\n",
      "40639, done 11100 episodes, -54.41928360389338, current eps 0.01996653083218267 0\n",
      "41321, done 11200 episodes, -55.30817961649798, current eps 0.01996423399454417 0\n",
      "42028, done 11300 episodes, -58.34112740332852, current eps 0.019961852961977573 0\n",
      "42664, done 11400 episodes, -61.847108364945605, current eps 0.01995971104300677 0\n",
      "43405, done 11500 episodes, -56.872060224369115, current eps 0.01995721550533796 0\n",
      "44019, done 11600 episodes, -61.100111445154106, current eps 0.019955147677903885 0\n",
      "44642, done 11700 episodes, -64.53884662990497, current eps 0.019953049540295694 0\n",
      "45261, done 11800 episodes, -64.73831054857534, current eps 0.019950964873876 0\n",
      "45814, done 11900 episodes, -68.85634135996169, current eps 0.01994910248206648 0\n",
      "46363, done 12000 episodes, -67.31457726576555, current eps 0.01994725356144546 0\n",
      "47055, done 12100 episodes, -56.74800205844298, current eps 0.01994492304583572 0\n",
      "47740, done 12200 episodes, -61.75075648259312, current eps 0.019942616104805848 0\n",
      "48334, done 12300 episodes, -64.94382604296524, current eps 0.01994061563331425 0\n",
      "48983, done 12400 episodes, -62.73917587344487, current eps 0.01993842993298084 0\n",
      "49585, done 12500 episodes, -69.5104592970658, current eps 0.01993640251911225 0\n",
      "50214, done 12600 episodes, -60.73681865799468, current eps 0.019934284174721317 0\n",
      "50903, done 12700 episodes, -56.742129657915626, current eps 0.01993196376250295 0\n",
      "51535, done 12800 episodes, -65.39544692049652, current eps 0.019929835314720642 0\n",
      "52187, done 12900 episodes, -62.29469521822457, current eps 0.01992763951099586 0\n",
      "52849, done 13000 episodes, -65.81633958742998, current eps 0.019925410029299838 0\n",
      "53597, done 13100 episodes, -56.037478613165746, current eps 0.01992289091705116 0\n",
      "54251, done 13200 episodes, -62.513880736689906, current eps 0.019920688377732126 0\n",
      "54936, done 13300 episodes, -65.82530867490797, current eps 0.019918381436702253 0\n",
      "55606, done 13400 episodes, -63.97800393557997, current eps 0.01991612501262924 0\n",
      "56415, done 13500 episodes, -53.31880657176598, current eps 0.019913400464756006 0\n",
      "57156, done 13600 episodes, -53.31283410857142, current eps 0.019910904927087195 0\n",
      "57748, done 13700 episodes, -66.70131900927502, current eps 0.019908911191189843 0\n",
      "58482, done 13800 episodes, -64.10956366707882, current eps 0.0199064392281009 0\n",
      "59109, done 13900 episodes, -66.6553643387968, current eps 0.019904327619304215 0\n",
      "59728, done 14000 episodes, -68.92366535131248, current eps 0.01990224295288452 0\n",
      "Evaluation score: -30.036369371735645\n",
      "0 of 20 episodes ended close to / at the final state.\n",
      "60429, done 14100 episodes, -59.76344019246487, current eps 0.019899882127100664 0\n",
      "61004, done 14200 episodes, -67.44317644497214, current eps 0.01989794564375442 0\n",
      "61596, done 14300 episodes, -64.40968634670739, current eps 0.019895951907857073 0\n",
      "62296, done 14400 episodes, -60.68963129849113, current eps 0.019893594449870343 0\n",
      "63034, done 14500 episodes, -57.68429784039921, current eps 0.019891109015592903 0\n",
      "63724, done 14600 episodes, -62.32509970832327, current eps 0.01988878523557741 0\n",
      "64410, done 14700 episodes, -62.52251249235853, current eps 0.019886474926750414 0\n",
      "65152, done 14800 episodes, -60.035510456578805, current eps 0.01988397602128448 0\n",
      "65913, done 14900 episodes, -52.58094248407354, current eps 0.019881413127673192 0\n",
      "66514, done 15000 episodes, -63.070383310887856, current eps 0.019879389081601726 0\n",
      "67139, done 15100 episodes, -63.632805229835974, current eps 0.019877284208399286 0\n",
      "67590, done 15200 episodes, -80.7885132556006, current eps 0.01987576533189641 0\n",
      "68195, done 15300 episodes, -57.35571475231986, current eps 0.01987372781463645 0\n",
      "68726, done 15400 episodes, -69.73581558575894, current eps 0.019871939514363657 0\n",
      "69289, done 15500 episodes, -69.57609706122643, current eps 0.0198700434445829 0\n",
      "69909, done 15600 episodes, -65.70574303081145, current eps 0.01986795541036608 0\n",
      "70588, done 15700 episodes, -56.72179942487026, current eps 0.019865668676118952 0\n",
      "71245, done 15800 episodes, -62.31580310296032, current eps 0.01986345603340855 0\n",
      "71838, done 15900 episodes, -69.32080553102831, current eps 0.019861458929714075 0\n",
      "72451, done 16000 episodes, -66.19799148947071, current eps 0.019859394470077125 0\n",
      "72952, done 16100 episodes, -72.93453662340201, current eps 0.019857707203718048 0\n",
      "73669, done 16200 episodes, -53.71348910861167, current eps 0.019855292493180212 0\n",
      "74384, done 16300 episodes, -54.32697824977867, current eps 0.01985288451823662 0\n",
      "75175, done 16400 episodes, -60.53089116916593, current eps 0.019850220590711617 0\n",
      "75812, done 16500 episodes, -58.730832186107925, current eps 0.019848075303943693 0\n",
      "76519, done 16600 episodes, -54.42158628758362, current eps 0.019845694271377095 0\n",
      "77134, done 16700 episodes, -65.15820112539635, current eps 0.019843623076145893 0\n",
      "77820, done 16800 episodes, -63.633251911445605, current eps 0.0198413127673189 0\n",
      "78474, done 16900 episodes, -55.9744434747232, current eps 0.019839110227999866 0\n",
      "79176, done 17000 episodes, -60.70923038062222, current eps 0.019836746034418887 0\n",
      "79849, done 17100 episodes, -68.36633052368121, current eps 0.0198344795069545 0\n",
      "Evaluation score: -28.623613048006725\n",
      "0 of 20 episodes ended close to / at the final state.\n",
      "80552, done 17200 episodes, -58.96063357603679, current eps 0.0198321119455764 0\n",
      "81319, done 17300 episodes, -55.89000008098235, current eps 0.019829528845182368 0\n",
      "82162, done 17400 episodes, -51.600575072376586, current eps 0.01982668979220692 0\n",
      "82845, done 17500 episodes, -60.10449800965749, current eps 0.019824389586771296 0\n",
      "83547, done 17600 episodes, -62.71232151470813, current eps 0.019822025393190317 0\n",
      "84210, done 17700 episodes, -61.902185692536804, current eps 0.01981979254369717 0\n",
      "84969, done 17800 episodes, -59.666335362395294, current eps 0.01981723638568013 0\n",
      "85628, done 17900 episodes, -59.59815360226414, current eps 0.019815017007375476 0\n",
      "86325, done 18000 episodes, -65.99439229439248, current eps 0.01981266965278012 0\n",
      "86913, done 18100 episodes, -67.62360284718305, current eps 0.019810689388071265 0\n",
      "87556, done 18200 episodes, -66.49632950789527, current eps 0.019808523894520595 0\n",
      "88212, done 18300 episodes, -58.79520149775276, current eps 0.019806314619607315 0\n",
      "88929, done 18400 episodes, -60.98812765749647, current eps 0.01980389990906948 0\n",
      "89612, done 18500 episodes, -58.0111739306066, current eps 0.019801599703633856 0\n",
      "90325, done 18600 episodes, -63.351297219411464, current eps 0.019799198464284514 0\n",
      "91120, done 18700 episodes, -59.34036971028176, current eps 0.01979652106557101 0\n",
      "91821, done 18800 episodes, -66.1762467433393, current eps 0.019794160239787157 0\n",
      "92379, done 18900 episodes, -71.64764959887897, current eps 0.01979228100899202 0\n",
      "93144, done 19000 episodes, -56.695620040615175, current eps 0.019789704644192236 0\n",
      "93932, done 19100 episodes, -62.42401768058124, current eps 0.0197870508200586 0\n",
      "94591, done 19200 episodes, -62.0323425211763, current eps 0.01978483144175395 0\n",
      "95303, done 19300 episodes, -59.36371829125609, current eps 0.01978243357020173 0\n",
      "96009, done 19400 episodes, -62.108000825178436, current eps 0.019780055905432258 0\n",
      "96664, done 19500 episodes, -64.85065173483468, current eps 0.019777849998316104 0\n",
      "97389, done 19600 episodes, -63.49788442828931, current eps 0.019775408345401275 0\n",
      "97972, done 19700 episodes, -70.26597762738831, current eps 0.01977344491967804 0\n",
      "98569, done 19800 episodes, -67.78016347340032, current eps 0.019771434344795072 0\n",
      "99208, done 19900 episodes, -73.62136776456444, current eps 0.0197692823224329 0\n",
      "99877, done 20000 episodes, -70.75583559127932, current eps 0.019767029266157008 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation score: -31.88673207929539\n",
      "0 of 20 episodes ended close to / at the final state.\n",
      "100459, done 20100 episodes, -67.17986074445741, current eps 0.019765069208230897 0\n",
      "101072, done 20200 episodes, -66.44629800229009, current eps 0.019763004748593948 0\n",
      "101686, done 20300 episodes, -64.93762563090752, current eps 0.01976093692115987 0\n",
      "102241, done 20400 episodes, -69.02836960634086, current eps 0.019759067793756104 0\n",
      "102878, done 20500 episodes, -68.49473466740872, current eps 0.01975692250698818 0\n",
      "103465, done 20600 episodes, -66.2737664499189, current eps 0.01975494561007645 0\n",
      "104099, done 20700 episodes, -68.92194031755565, current eps 0.019752810426699897 0\n",
      "104759, done 20800 episodes, -68.58073812646994, current eps 0.019750587680598123 0\n",
      "105404, done 20900 episodes, -66.45081604633225, current eps 0.019748415451453206 0\n",
      "106150, done 21000 episodes, -60.55705376645089, current eps 0.019745903074798777 0\n",
      "106885, done 21100 episodes, -55.72332244057929, current eps 0.019743427743912707 0\n",
      "107548, done 21200 episodes, -59.35342113850558, current eps 0.019741194894419563 0\n",
      "108112, done 21300 episodes, -65.66626431680533, current eps 0.019739295456841683 0\n",
      "108624, done 21400 episodes, -62.907462367529796, current eps 0.019737571144714243 0\n",
      "109250, done 21500 episodes, -63.66076120651451, current eps 0.01973546290371468 0\n",
      "109786, done 21600 episodes, -68.31545123345278, current eps 0.01973365776445627 0\n",
      "110374, done 21700 episodes, -62.879125426606116, current eps 0.019731677499747417 0\n",
      "111014, done 21800 episodes, -66.76016931586743, current eps 0.01972952210958812 0\n",
      "111596, done 21900 episodes, -69.15179249692957, current eps 0.01972756205166201 0\n",
      "112135, done 22000 episodes, -75.56798968202135, current eps 0.019725746809012228 0\n",
      "112748, done 22100 episodes, -66.87252503443595, current eps 0.019723682349375275 0\n",
      "113375, done 22200 episodes, -72.43491324567898, current eps 0.01972157074057859 0\n",
      "114085, done 22300 episodes, -68.49518902145043, current eps 0.01971917960462062 0\n",
      "114765, done 22400 episodes, -58.761273646459685, current eps 0.019716889502576367 0\n",
      "115526, done 22500 episodes, -61.714082493261365, current eps 0.019714326608965077 0\n",
      "116126, done 22600 episodes, -74.02910725038628, current eps 0.019712305930690736 0\n",
      "116730, done 22700 episodes, -72.51791781774924, current eps 0.0197102717812279 0\n",
      "117276, done 22800 episodes, -73.02558029584912, current eps 0.01970843296399825 0\n",
      "117798, done 22900 episodes, -74.65359078059092, current eps 0.019706674973899573 0\n",
      "118446, done 23000 episodes, -63.75279396339672, current eps 0.019704492641363286 0\n",
      "119149, done 23100 episodes, -62.28083699012583, current eps 0.019702125079985183 0\n",
      "119963, done 23200 episodes, -61.79337739703522, current eps 0.019699383693126328 0\n",
      "Evaluation score: -41.821560591225804\n",
      "0 of 20 episodes ended close to / at the final state.\n",
      "120598, done 23300 episodes, -63.166222602477575, current eps 0.01969724514195265 0\n",
      "121267, done 23400 episodes, -57.368554501087054, current eps 0.01969499208567676 0\n",
      "122053, done 23500 episodes, -58.125940363915646, current eps 0.019692344997137373 0\n",
      "122856, done 23600 episodes, -55.887510209202446, current eps 0.01968964065604688 0\n",
      "123521, done 23700 episodes, -57.48604279581498, current eps 0.01968740107095949 0\n",
      "124221, done 23800 episodes, -55.9395268223964, current eps 0.019685043612972755 0\n",
      "124901, done 23900 episodes, -66.39248997409672, current eps 0.0196827535109285 0\n",
      "125566, done 24000 episodes, -65.48870163029953, current eps 0.01968051392584111 0\n",
      "126313, done 24100 episodes, -61.3348066782168, current eps 0.019677998181389555 0\n",
      "126944, done 24200 episodes, -67.05454493645838, current eps 0.019675873101404374 0\n",
      "127605, done 24300 episodes, -64.3911165178828, current eps 0.019673646987505473 0\n",
      "128287, done 24400 episodes, -66.31914131996511, current eps 0.019671350149866974 0\n",
      "128984, done 24500 episodes, -64.12256713300543, current eps 0.019669002795271614 0\n",
      "129671, done 24600 episodes, -61.2912445457425, current eps 0.019666689118647496 0\n",
      "130275, done 24700 episodes, -68.40118910029197, current eps 0.019664654969184656 0\n",
      "130885, done 24800 episodes, -65.94665766263961, current eps 0.019662600612939077 0\n",
      "131415, done 24900 episodes, -66.06612598432868, current eps 0.01966081568046341 0\n",
      "132100, done 25000 episodes, -59.193503914873, current eps 0.019658508739433537 0\n",
      "132668, done 25100 episodes, -66.15089347808578, current eps 0.019656595830667162 0\n",
      "133140, done 25200 episodes, -78.04366284233592, current eps 0.019655006230424682 0\n",
      "133673, done 25300 episodes, -73.59156832057437, current eps 0.01965321119455764 0\n",
      "134324, done 25400 episodes, -62.895722980924255, current eps 0.01965101875862998 0\n",
      "134890, done 25500 episodes, -71.98255513322188, current eps 0.019649112585457852 0\n",
      "135578, done 25600 episodes, -57.733270899998885, current eps 0.01964679554103661 0\n",
      "136336, done 25700 episodes, -52.203856054715416, current eps 0.019644242750816693 0\n",
      "136956, done 25800 episodes, -64.29227909811496, current eps 0.019642154716599875 0\n",
      "137719, done 25900 episodes, -55.530728207275835, current eps 0.019639585087394336 0\n",
      "138442, done 26000 episodes, -49.754957017194755, current eps 0.019637150170073756 0\n",
      "139255, done 26100 episodes, -58.22454958220266, current eps 0.019634412151012026 0\n",
      "140000, done 26200 episodes, -54.05954305762348, current eps 0.019631903142154718 0\n",
      "Evaluation score: -34.94624179167809\n",
      "0 of 20 episodes ended close to / at the final state.\n",
      "140727, done 26300 episodes, -59.52267752812033, current eps 0.019629454753645643 0\n",
      "141474, done 26400 episodes, -56.113713410537486, current eps 0.01962693900919409 0\n",
      "142187, done 26500 episodes, -60.47144845635602, current eps 0.019624537769844748 0\n",
      "143003, done 26600 episodes, -49.88817689941822, current eps 0.019621789647391644 0\n",
      "143710, done 26700 episodes, -63.07745269873876, current eps 0.019619408614825043 0\n",
      "144411, done 26800 episodes, -60.68673166965902, current eps 0.01961704778904119 0\n",
      "145207, done 26900 episodes, -52.03572089952275, current eps 0.019614367022530565 0\n",
      "145974, done 27000 episodes, -44.594948663831765, current eps 0.01961178392213653 0\n",
      "146720, done 27100 episodes, -56.29501413765862, current eps 0.0196092715454821 0\n",
      "147372, done 27200 episodes, -68.10909495849045, current eps 0.019607075741757317 0\n",
      "148258, done 27300 episodes, -52.951443404961736, current eps 0.019604091873505543 0\n",
      "149079, done 27400 episodes, -56.39794898945908, current eps 0.01960132691206682 0\n",
      "149888, done 27500 episodes, -48.41894638259504, current eps 0.01959860236419358 0\n",
      "150632, done 27600 episodes, -61.56572867279519, current eps 0.0195960967231334 0\n",
      "151302, done 27700 episodes, -63.708756803495824, current eps 0.019593840299060385 0\n",
      "152039, done 27800 episodes, -54.80705231223813, current eps 0.01959135823258007 0\n",
      "152896, done 27900 episodes, -59.57050922966292, current eps 0.01958847203044489 0\n",
      "153771, done 28000 episodes, -44.93303008690064, current eps 0.019585525207961474 0\n",
      "154507, done 28100 episodes, -59.33106755456398, current eps 0.019583046509278283 0\n",
      "155057, done 28200 episodes, -71.30198861876808, current eps 0.019581194220860135 0\n",
      "155686, done 28300 episodes, -72.80627730387968, current eps 0.019579075876469204 0\n",
      "156312, done 28400 episodes, -66.86409940690946, current eps 0.01957696763546964 0\n",
      "157002, done 28500 episodes, -59.19349253194318, current eps 0.019574643855454148 0\n",
      "157652, done 28600 episodes, -64.21795987438803, current eps 0.019572454787323612 0\n",
      "158387, done 28700 episodes, -49.875358578345555, current eps 0.019569979456437545 0\n",
      "159158, done 28800 episodes, -50.181257186519495, current eps 0.019567382884855017 0\n",
      "159844, done 28900 episodes, -50.69325031246861, current eps 0.019565072576028023 0\n",
      "Evaluation score: -36.77308324070559\n",
      "0 of 20 episodes ended close to / at the final state.\n",
      "160426, done 29000 episodes, -66.22845576706197, current eps 0.019563112518101913 0\n",
      "161088, done 29100 episodes, -64.18907227531108, current eps 0.01956088303640589 0\n",
      "161803, done 29200 episodes, -56.15805570412164, current eps 0.0195584750614623 0\n",
      "162666, done 29300 episodes, -31.530824921023736, current eps 0.01955556865254437 0\n",
      "163705, done 29400 episodes, -25.419962170758176, current eps 0.019552069511332638 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "164590, done 29500 episodes, -37.558011556558334, current eps 0.019549089010877985 0\n",
      "165461, done 29600 episodes, -33.16594495894446, current eps 0.019546155659583068 0\n",
      "166272, done 29700 episodes, -39.57158123773962, current eps 0.019543424376115583 0\n",
      "167007, done 29800 episodes, -55.01057585503375, current eps 0.019540949045229516 0\n",
      "167761, done 29900 episodes, -47.8076902595503, current eps 0.019538409726198094 0\n",
      "168764, done 30000 episodes, -14.43886377719857, current eps 0.019535031825682822 0\n",
      "169643, done 30100 episodes, -43.32483678726392, current eps 0.019532071532010913 0\n",
      "170569, done 30200 episodes, -29.9809871583543, current eps 0.019528952951874182 0\n",
      "171381, done 30300 episodes, -45.30626659700855, current eps 0.019526218300609573 0\n",
      "172248, done 30400 episodes, -38.686818758982696, current eps 0.01952329842050315 0\n",
      "173052, done 30500 episodes, -56.30101047403763, current eps 0.019520590711615535 0\n",
      "174027, done 30600 episodes, -40.96627715224999, current eps 0.01951730710941973 0\n",
      "174856, done 30700 episodes, -42.54715755770213, current eps 0.019514515205604016 0\n",
      "175783, done 30800 episodes, -38.907616714949704, current eps 0.019511393257670158 0\n",
      "176629, done 30900 episodes, -44.00853873698975, current eps 0.01950854410130334 0\n",
      "177460, done 31000 episodes, -38.5362239302338, current eps 0.019505745461893376 0\n",
      "178319, done 31100 episodes, -44.0874725465761, current eps 0.019502852524163946 0\n",
      "179081, done 31200 episodes, -53.927822693233885, current eps 0.019500286262755535 0\n",
      "179756, done 31300 episodes, -62.029202781142196, current eps 0.0194980129996969 0\n",
      "Evaluation score: 0.8582534538752575\n",
      "0 of 20 episodes ended close to / at the final state.\n",
      "180538, done 31400 episodes, -47.95667176862876, current eps 0.01949537938234601 0\n",
      "181301, done 31500 episodes, -49.65245723441496, current eps 0.019492809753140473 0\n",
      "181995, done 31600 episodes, -56.60274483922048, current eps 0.019490472501936484 0\n",
      "182914, done 31700 episodes, -46.87008081913391, current eps 0.019487377496379618 0\n",
      "183809, done 31800 episodes, -41.78220478813149, current eps 0.019484363317953727 0\n",
      "184627, done 31900 episodes, -37.95604330983515, current eps 0.019481608459906378 0\n",
      "185408, done 32000 episodes, -44.95326245133215, current eps 0.01947897821035261 0\n",
      "186469, done 32100 episodes, -24.461966805281563, current eps 0.019475404977604152 0\n",
      "187474, done 32200 episodes, -30.070440304452305, current eps 0.01947202034149463 0\n",
      "188468, done 32300 episodes, -30.697915662895173, current eps 0.019468672751153472 0\n",
      "189375, done 32400 episodes, -48.16697859467342, current eps 0.019465618159162094 0\n",
      "190249, done 32500 episodes, -51.63785950971643, current eps 0.019462674704475803 0\n",
      "191151, done 32600 episodes, -42.486697974577446, current eps 0.019459636951470044 0\n",
      "192176, done 32700 episodes, -35.30396389980895, current eps 0.019456184959418046 0\n",
      "193065, done 32800 episodes, -36.43475454256006, current eps 0.0194531909877749 0\n",
      "193934, done 32900 episodes, -47.600016920587315, current eps 0.019450264372074227 0\n",
      "194859, done 33000 episodes, -34.84211156393312, current eps 0.01944714915973462 0\n",
      "195813, done 33100 episodes, -42.72060966158604, current eps 0.01944393628127842 0\n",
      "196519, done 33200 episodes, -70.81838120527284, current eps 0.019441558616508942 0\n",
      "197242, done 33300 episodes, -56.48487887476014, current eps 0.019439123699188363 0\n",
      "198097, done 33400 episodes, -29.532372423079863, current eps 0.019436244232647427 0\n",
      "199046, done 33500 episodes, -29.094631732857728, current eps 0.019433048193176844 0\n",
      "199814, done 33600 episodes, -54.82795646294179, current eps 0.01943046172498569 0\n",
      "Evaluation score: 62.07696538377188\n",
      "0 of 20 episodes ended close to / at the final state.\n",
      "200591, done 33700 episodes, -49.503065923769846, current eps 0.019427844946620417 0\n",
      "201321, done 33800 episodes, -50.6141239152554, current eps 0.01942538645471997 0\n",
      "202104, done 33900 episodes, -50.295353640267514, current eps 0.019422749469571954 0\n",
      "203046, done 34000 episodes, -31.065113028111792, current eps 0.01941957700468124 0\n",
      "203893, done 34100 episodes, -47.84259926240357, current eps 0.019416724480517296 0\n",
      "204868, done 34200 episodes, -25.944390508499026, current eps 0.019413440878321492 0\n",
      "205706, done 34300 episodes, -43.17561165986437, current eps 0.019410618664331664 0\n",
      "206402, done 34400 episodes, -51.45701368498427, current eps 0.01940827467753343 0\n",
      "207129, done 34500 episodes, -52.20243676027474, current eps 0.01940582628902435 0\n",
      "207892, done 34600 episodes, -57.193862138726985, current eps 0.019403256659818815 0\n",
      "208579, done 34700 episodes, -58.53962104148917, current eps 0.019400942983194693 0\n",
      "209324, done 34800 episodes, -50.633864741826116, current eps 0.019398433974337388 0\n",
      "210154, done 34900 episodes, -44.80025208101395, current eps 0.01939563870272455 0\n",
      "210921, done 35000 episodes, -49.47365968756756, current eps 0.019393055602330518 0\n",
      "211719, done 35100 episodes, -43.64808969699975, current eps 0.019390368100225645 0\n",
      "212519, done 35200 episodes, -53.07547117592947, current eps 0.019387673862526524 0\n",
      "213316, done 35300 episodes, -49.3472268059563, current eps 0.019384989728218775 0\n",
      "214066, done 35400 episodes, -62.36877257327741, current eps 0.019382463880375848 0\n",
      "214849, done 35500 episodes, -54.155871391385354, current eps 0.019379826895227832 0\n",
      "215655, done 35600 episodes, -49.980072558269455, current eps 0.01937711245074597 0\n",
      "216423, done 35700 episodes, -54.575063041698684, current eps 0.019374525982554812 0\n",
      "217311, done 35800 episodes, -43.089983196567644, current eps 0.01937153537870879 0\n",
      "218076, done 35900 episodes, -54.96509727479247, current eps 0.019368959013909004 0\n",
      "218848, done 36000 episodes, -50.764892370022764, current eps 0.01936635907452935 0\n",
      "219699, done 36100 episodes, -48.42111258761799, current eps 0.01936349307917691 0\n",
      "Evaluation score: -2.8087935404724207\n",
      "0 of 20 episodes ended close to / at the final state.\n",
      "220385, done 36200 episodes, -63.492564081144465, current eps 0.019361182770349917 0\n",
      "221148, done 36300 episodes, -50.0793807738591, current eps 0.019358613141144378 0\n",
      "221923, done 36400 episodes, -53.209278531017006, current eps 0.019356003098373355 0\n",
      "222643, done 36500 episodes, -50.829004971962156, current eps 0.019353578284444145 0\n",
      "223372, done 36600 episodes, -44.990266265770785, current eps 0.019351123160340822 0\n",
      "224040, done 36700 episodes, -57.9167270673957, current eps 0.019348873471862055 0\n",
      "224813, done 36800 episodes, -45.61681249418192, current eps 0.01934627016468528 0\n",
      "225410, done 36900 episodes, -61.87937689630294, current eps 0.01934425958980231 0\n",
      "226103, done 37000 episodes, -54.25421811190638, current eps 0.01934192570639545 0\n",
      "226837, done 37100 episodes, -46.730161857222875, current eps 0.019339453743306506 0\n",
      "227554, done 37200 episodes, -54.49242677220505, current eps 0.019337039032768667 0\n",
      "228273, done 37300 episodes, -49.77197920938634, current eps 0.01933461758663658 0\n",
      "229002, done 37400 episodes, -59.46876242420269, current eps 0.019332162462533258 0\n",
      "229665, done 37500 episodes, -68.60532985271621, current eps 0.01932992961304011 0\n",
      "230338, done 37600 episodes, -53.377837473879126, current eps 0.019327663085575725 0\n",
      "230987, done 37700 episodes, -59.83626411859419, current eps 0.019325477385242314 0\n",
      "231677, done 37800 episodes, -56.310081505010906, current eps 0.019323153605226823 0\n",
      "232256, done 37900 episodes, -68.98965552816034, current eps 0.019321203650692086 0\n",
      "232944, done 38000 episodes, -62.99206931528423, current eps 0.01931888660627084 0\n",
      "233671, done 38100 episodes, -53.6613264781808, current eps 0.019316438217761765 0\n",
      "234323, done 38200 episodes, -66.60457533850365, current eps 0.01931424241403698 0\n",
      "235043, done 38300 episodes, -53.33863851315513, current eps 0.01931181760010777 0\n",
      "235732, done 38400 episodes, -59.97701074341284, current eps 0.019309497187889404 0\n",
      "236406, done 38500 episodes, -53.27508520044905, current eps 0.019307227292627894 0\n",
      "237108, done 38600 episodes, -55.158250291134145, current eps 0.019304863099046915 0\n",
      "237732, done 38700 episodes, -55.01062982818527, current eps 0.0193027615936416 0\n",
      "238279, done 38800 episodes, -62.42093601035747, current eps 0.019300919408614826 0\n",
      "238997, done 38900 episodes, -54.915436373301624, current eps 0.019298501330279865 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "239843, done 39000 episodes, -43.12455876657601, current eps 0.019295652173913044 0\n",
      "Evaluation score: -25.95837259242673\n",
      "0 of 20 episodes ended close to / at the final state.\n",
      "240618, done 39100 episodes, -48.433289230432145, current eps 0.01929304213114202 0\n",
      "241362, done 39200 episodes, -50.21661618254795, current eps 0.01929053649008184 0\n",
      "242069, done 39300 episodes, -54.89383004727318, current eps 0.019288155457515242 0\n",
      "242801, done 39400 episodes, -46.25634921598822, current eps 0.019285690230020545 0\n",
      "243529, done 39500 episodes, -61.29189695205896, current eps 0.019283238473714347 0\n",
      "244192, done 39600 episodes, -61.644041337099125, current eps 0.0192810056242212 0\n",
      "244749, done 39700 episodes, -74.89017372353686, current eps 0.019279129761223184 0\n",
      "245561, done 39800 episodes, -51.26810958706308, current eps 0.01927639510995858 0\n",
      "246348, done 39900 episodes, -48.84769586062059, current eps 0.01927374465362207 0\n",
      "247046, done 40000 episodes, -56.354983842138, current eps 0.019271393931229584 0\n",
      "247692, done 40100 episodes, -57.09531607560251, current eps 0.019269218334287543 0\n",
      "248264, done 40200 episodes, -74.53364706000468, current eps 0.019267291954332674 0\n",
      "249022, done 40300 episodes, -49.480252297163744, current eps 0.019264739164112754 0\n",
      "249714, done 40400 episodes, -59.119896206164974, current eps 0.019262408648503017 0\n",
      "250354, done 40500 episodes, -67.79487686142221, current eps 0.01926025325834372 0\n",
      "251058, done 40600 episodes, -65.49959034474786, current eps 0.01925788232916849 0\n",
      "251707, done 40700 episodes, -60.78046484085049, current eps 0.01925569662883508 0\n",
      "252335, done 40800 episodes, -68.60873409137837, current eps 0.01925358165224127 0\n",
      "252894, done 40900 episodes, -72.0938410446325, current eps 0.01925169905364901 0\n",
      "253518, done 41000 episodes, -70.2108537144952, current eps 0.019249597548243694 0\n",
      "254120, done 41100 episodes, -67.9726826531886, current eps 0.019247570134375107 0\n",
      "254717, done 41200 episodes, -70.89469570138729, current eps 0.01924555955949214 0\n",
      "255231, done 41300 episodes, -77.77839312953684, current eps 0.019243828511770453 0\n",
      "255863, done 41400 episodes, -70.17468806490636, current eps 0.01924170006398815 0\n",
      "256512, done 41500 episodes, -67.55919388445184, current eps 0.019239514363654734 0\n",
      "257110, done 41600 episodes, -67.72556026596762, current eps 0.01923750042097464 0\n",
      "257732, done 41700 episodes, -71.09811008757389, current eps 0.019235405651163575 0\n",
      "258430, done 41800 episodes, -48.51824655546319, current eps 0.019233054928771094 0\n",
      "259062, done 41900 episodes, -62.65753431189839, current eps 0.019230926480988786 0\n",
      "259718, done 42000 episodes, -56.53092566344073, current eps 0.019228717206075507 0\n",
      "Evaluation score: -16.921528234456968\n",
      "0 of 20 episodes ended close to / at the final state.\n",
      "260372, done 42100 episodes, -66.10549721016898, current eps 0.019226514666756477 0\n",
      "261078, done 42200 episodes, -55.1897484878557, current eps 0.019224137001987003 0\n",
      "261770, done 42300 episodes, -64.8172458094954, current eps 0.019221806486377262 0\n",
      "262501, done 42400 episodes, -62.845161266797284, current eps 0.01921934462667969 0\n",
      "263166, done 42500 episodes, -69.76614119417303, current eps 0.019217105041592297 0\n",
      "263861, done 42600 episodes, -55.3036949579294, current eps 0.019214764422591183 0\n",
      "264474, done 42700 episodes, -64.36728240517739, current eps 0.019212699962954233 0\n",
      "265110, done 42800 episodes, -74.13458745219401, current eps 0.01921055804398343 0\n",
      "265731, done 42900 episodes, -65.45454722419142, current eps 0.01920846664196949 0\n",
      "266407, done 43000 episodes, -63.88076767550601, current eps 0.019206190011113733 0\n",
      "267024, done 43100 episodes, -66.49769334920794, current eps 0.019204112080288285 0\n",
      "267722, done 43200 episodes, -57.55228357894166, current eps 0.0192017613578958 0\n",
      "268364, done 43300 episodes, -69.08312098623368, current eps 0.019199599232142258 0\n",
      "269011, done 43400 episodes, -62.2504759885107, current eps 0.019197420267403092 0\n",
      "269625, done 43500 episodes, -67.98175673196265, current eps 0.01919535243996902 0\n",
      "270290, done 43600 episodes, -59.45537676430982, current eps 0.019193112854881622 0\n",
      "270931, done 43700 episodes, -46.955886845062594, current eps 0.019190954096925204 0\n",
      "271614, done 43800 episodes, -52.67361334794411, current eps 0.019188653891489577 0\n",
      "272220, done 43900 episodes, -65.67315806050142, current eps 0.019186613006432495 0\n",
      "272863, done 44000 episodes, -63.771879015980986, current eps 0.019184447512881824 0\n",
      "273502, done 44100 episodes, -63.67368000269597, current eps 0.01918229549051965 0\n",
      "274237, done 44200 episodes, -51.056747903335854, current eps 0.019179820159633584 0\n",
      "274974, done 44300 episodes, -49.16663470609864, current eps 0.01917733809315327 0\n",
      "275693, done 44400 episodes, -58.06595932670517, current eps 0.019174916647021187 0\n",
      "276349, done 44500 episodes, -55.86228087175401, current eps 0.019172707372107904 0\n",
      "276956, done 44600 episodes, -67.8569993835892, current eps 0.019170663119253698 0\n",
      "277634, done 44700 episodes, -67.67830502368899, current eps 0.019168379752803694 0\n",
      "278333, done 44800 episodes, -53.3110373809229, current eps 0.019166025662614085 0\n",
      "279045, done 44900 episodes, -55.89629309233812, current eps 0.019163627791061868 0\n",
      "279741, done 45000 episodes, -56.11566014577858, current eps 0.019161283804263633 0\n",
      "Evaluation score: -49.668966417496435\n",
      "0 of 20 episodes ended close to / at the final state.\n",
      "280589, done 45100 episodes, -55.300449583317366, current eps 0.019158427912302566 0\n",
      "281304, done 45200 episodes, -61.06000467697084, current eps 0.019156019937358975 0\n",
      "282218, done 45300 episodes, -37.391085197791455, current eps 0.01915294177078773 0\n",
      "283049, done 45400 episodes, -41.82986446794777, current eps 0.019150143131377768 0\n",
      "283855, done 45500 episodes, -52.5721800195642, current eps 0.019147428686895902 0\n",
      "284558, done 45600 episodes, -59.4011233206958, current eps 0.0191450611255178 0\n",
      "285251, done 45700 episodes, -55.20608410552628, current eps 0.019142727242110937 0\n",
      "285942, done 45800 episodes, -61.11617905257438, current eps 0.01914040009429832 0\n",
      "286552, done 45900 episodes, -64.17856927213548, current eps 0.01913834573805274 0\n",
      "287381, done 46000 episodes, -38.36946950276171, current eps 0.019135553834237026 0\n",
      "288135, done 46100 episodes, -61.119895097511396, current eps 0.019133014515205604 0\n",
      "288948, done 46200 episodes, -47.857363631880645, current eps 0.019130276496143874 0\n",
      "289692, done 46300 episodes, -51.82797012998738, current eps 0.01912777085508369 0\n",
      "290477, done 46400 episodes, -48.1917995279253, current eps 0.01912512713434143 0\n",
      "291225, done 46500 episodes, -56.551949096721934, current eps 0.01912260802209275 0\n",
      "292042, done 46600 episodes, -46.76630873034803, current eps 0.019119856531842522 0\n",
      "292800, done 46700 episodes, -47.282251328476015, current eps 0.019117303741622606 0\n",
      "293606, done 46800 episodes, -44.378113080598986, current eps 0.01911458929714074 0\n",
      "294342, done 46900 episodes, -62.428131733854336, current eps 0.01911211059845755 0\n",
      "295194, done 47000 episodes, -45.624079993734, current eps 0.019109241235307987 0\n",
      "295901, done 47100 episodes, -58.99678032226251, current eps 0.01910686020274139 0\n",
      "296540, done 47200 episodes, -66.86362363321976, current eps 0.019104708180379217 0\n",
      "297109, done 47300 episodes, -70.7499249794093, current eps 0.019102791903815714 0\n",
      "297863, done 47400 episodes, -51.88677842334814, current eps 0.019100252584784295 0\n",
      "298695, done 47500 episodes, -53.88834887256987, current eps 0.019097450577577207 0\n",
      "299476, done 47600 episodes, -51.02740072563733, current eps 0.01909482032802344 0\n",
      "Evaluation score: 4.9762991311708635\n",
      "0 of 20 episodes ended close to / at the final state.\n",
      "300290, done 47700 episodes, -50.514284744880854, current eps 0.019092078941164586 0\n",
      "301139, done 47800 episodes, -50.174568985928936, current eps 0.019089219681406394 0\n",
      "301940, done 47900 episodes, -61.887946988154056, current eps 0.019086522075910148 0\n",
      "302707, done 48000 episodes, -51.53440922391221, current eps 0.019083938975516117 0\n",
      "303510, done 48100 episodes, -61.066504095366945, current eps 0.019081234634425626 0\n",
      "304351, done 48200 episodes, -48.858611698251636, current eps 0.019078402317044423 0\n",
      "305048, done 48300 episodes, -58.42272088136262, current eps 0.019076054962449063 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "305716, done 48400 episodes, -57.990808941957795, current eps 0.019073805273970297 0\n",
      "306216, done 48500 episodes, -73.69926574769043, current eps 0.019072121375408348 0\n",
      "306753, done 48600 episodes, -72.22547091807046, current eps 0.019070312868352812 0\n",
      "307486, done 48700 episodes, -60.304883052327526, current eps 0.01906784427306099 0\n",
      "308043, done 48800 episodes, -73.21511058203131, current eps 0.01906596841006298 0\n",
      "308579, done 48900 episodes, -73.35327486481225, current eps 0.01906416327080457 0\n",
      "309176, done 49000 episodes, -73.69596426744077, current eps 0.0190621526959216 0\n",
      "309675, done 49100 episodes, -71.9124383830223, current eps 0.019060472165156772 0\n",
      "310215, done 49200 episodes, -72.12050416645721, current eps 0.019058653554709867 0\n",
      "310942, done 49300 episodes, -58.370833843404036, current eps 0.01905620516620079 0\n",
      "311691, done 49400 episodes, -57.845854518413, current eps 0.01905368268615499 0\n",
      "312482, done 49500 episodes, -54.101022219579406, current eps 0.01905101875862998 0\n",
      "313192, done 49600 episodes, -61.94837107059336, current eps 0.019048627622672013 0\n",
      "313797, done 49700 episodes, -69.0774962961831, current eps 0.019046590105412053 0\n",
      "314569, done 49800 episodes, -56.52779449410057, current eps 0.0190439901660324 0\n",
      "315309, done 49900 episodes, -59.063212303471076, current eps 0.019041497996160714 0\n",
      "316006, done 50000 episodes, -66.69330138014749, current eps 0.019039150641565354 0\n",
      "316761, done 50100 episodes, -51.63387512188876, current eps 0.019036607954736808 0\n",
      "317564, done 50200 episodes, -58.53638319986009, current eps 0.019033903613646316 0\n",
      "318340, done 50300 episodes, -54.16101062404323, current eps 0.019031290203078168 0\n",
      "319001, done 50400 episodes, -53.04881702801258, current eps 0.01902906408917927 0\n",
      "319723, done 50500 episodes, -56.11666081201899, current eps 0.01902663253965581 0\n",
      "Evaluation score: -17.952796229130875\n",
      "0 of 20 episodes ended close to / at the final state.\n",
      "320435, done 50600 episodes, -54.872641077571714, current eps 0.019024234668103594 0\n",
      "321165, done 50700 episodes, -56.05444054813757, current eps 0.019021776176203146 0\n",
      "321826, done 50800 episodes, -58.712975308724, current eps 0.019019550062304248 0\n",
      "322529, done 50900 episodes, -57.35962942438082, current eps 0.019017182500926145 0\n",
      "323173, done 51000 episodes, -58.404745195631456, current eps 0.019015013639578353 0\n",
      "323805, done 51100 episodes, -69.33157354707359, current eps 0.019012885191796048 0\n",
      "324455, done 51200 episodes, -61.73599025952815, current eps 0.019010696123665512 0\n",
      "325114, done 51300 episodes, -61.10227347799867, current eps 0.01900847674536086 0\n",
      "325821, done 51400 episodes, -58.5139915085167, current eps 0.019006095712794262 0\n",
      "326461, done 51500 episodes, -73.5557791569139, current eps 0.019003940322634964 0\n",
      "327144, done 51600 episodes, -60.9059763768495, current eps 0.01900164011719934 0\n",
      "327853, done 51700 episodes, -55.506257975894414, current eps 0.018999252349038494 0\n",
      "328456, done 51800 episodes, -59.65093054652164, current eps 0.018997221567372782 0\n",
      "329184, done 51900 episodes, -55.48875464480304, current eps 0.018994769811066584 0\n",
      "329779, done 52000 episodes, -68.78972350136692, current eps 0.01899276597177786 0\n",
      "330409, done 52100 episodes, -65.6395188518725, current eps 0.018990644259589805 0\n",
      "331113, done 52200 episodes, -61.31997960484893, current eps 0.018988273330414578 0\n",
      "331699, done 52300 episodes, -66.04083284157039, current eps 0.018986299801299972 0\n",
      "332408, done 52400 episodes, -58.99389701990106, current eps 0.018983912033139125 0\n",
      "332914, done 52500 episodes, -74.29230845300008, current eps 0.018982207927794433 0\n",
      "333464, done 52600 episodes, -70.95852894779563, current eps 0.018980355639376285 0\n",
      "334112, done 52700 episodes, -62.80940048091855, current eps 0.01897817330684 0\n",
      "334745, done 52800 episodes, -56.59326127993015, current eps 0.01897604149126057 0\n",
      "335255, done 52900 episodes, -69.53901965361257, current eps 0.01897432391472738 0\n",
      "335855, done 53000 episodes, -67.88217270561009, current eps 0.018972303236453037 0\n",
      "336511, done 53100 episodes, -66.75210862442871, current eps 0.018970093961539758 0\n",
      "337165, done 53200 episodes, -63.35425868583725, current eps 0.018967891422220728 0\n",
      "337868, done 53300 episodes, -50.88214292470022, current eps 0.018965523860842624 0\n",
      "338541, done 53400 episodes, -57.70146946083101, current eps 0.01896325733337824 0\n",
      "339228, done 53500 episodes, -56.25812040082795, current eps 0.018960943656754117 0\n",
      "339962, done 53600 episodes, -52.004196323519245, current eps 0.018958471693665175 0\n",
      "Evaluation score: -21.404450814514885\n",
      "0 of 20 episodes ended close to / at the final state.\n",
      "340646, done 53700 episodes, -60.503985079167435, current eps 0.018956168120432427 0\n",
      "341176, done 53800 episodes, -76.49116744072361, current eps 0.01895438318795676 0\n",
      "341767, done 53900 episodes, -68.56425854166358, current eps 0.018952392819856532 0\n",
      "342374, done 54000 episodes, -69.97123898254272, current eps 0.018950348567002326 0\n",
      "343016, done 54100 episodes, -59.51300108288983, current eps 0.01894818644124878 0\n",
      "343620, done 54200 episodes, -66.33266134545426, current eps 0.018946152291785943 0\n",
      "344242, done 54300 episodes, -64.73135902061108, current eps 0.018944057521974877 0\n",
      "344771, done 54400 episodes, -75.08683331528394, current eps 0.018942275957296334 0\n",
      "345543, done 54500 episodes, -64.73772570634588, current eps 0.01893967601791668 0\n",
      "346210, done 54600 episodes, -67.57389922406799, current eps 0.01893742969723504 0\n",
      "346895, done 54700 episodes, -59.69243088023242, current eps 0.018935122756205167 0\n",
      "347487, done 54800 episodes, -68.45589397704636, current eps 0.018933129020307818 0\n",
      "348080, done 54900 episodes, -68.29546410772764, current eps 0.018931131916613345 0\n",
      "348721, done 55000 episodes, -61.018428254396206, current eps 0.018928973158656923 0\n",
      "349480, done 55100 episodes, -52.22169366649341, current eps 0.01892641700063988 0\n",
      "350092, done 55200 episodes, -70.27056074647275, current eps 0.018924355908800056 0\n",
      "350693, done 55300 episodes, -68.53019624238925, current eps 0.01892233186272859 0\n",
      "351449, done 55400 episodes, -55.17844957076767, current eps 0.018919785808102923 0\n",
      "352141, done 55500 episodes, -67.0348080398572, current eps 0.018917455292493182 0\n",
      "352730, done 55600 episodes, -60.1947860156837, current eps 0.018915471659987203 0\n",
      "353449, done 55700 episodes, -54.01408883734662, current eps 0.01891305021385512 0\n",
      "354134, done 55800 episodes, -61.91291570984576, current eps 0.018910743272825246 0\n",
      "354906, done 55900 episodes, -44.770206864660764, current eps 0.018908143333445596 0\n",
      "355679, done 56000 episodes, -52.04081705163552, current eps 0.01890554002626882 0\n",
      "356382, done 56100 episodes, -57.360055744428266, current eps 0.018903172464890716 0\n",
      "357149, done 56200 episodes, -49.866456214937934, current eps 0.018900589364496685 0\n",
      "357895, done 56300 episodes, -39.76666267335816, current eps 0.018898076987842252 0\n",
      "358710, done 56400 episodes, -44.31386649804325, current eps 0.018895332233186273 0\n",
      "359245, done 56500 episodes, -63.76265497176655, current eps 0.018893530461724987 0\n",
      "359821, done 56600 episodes, -64.13607018519859, current eps 0.01889159061058162 0\n",
      "Evaluation score: -44.01985473818006\n",
      "0 of 20 episodes ended close to / at the final state.\n",
      "360388, done 56700 episodes, -69.64485378455194, current eps 0.01888968106961237 0\n",
      "361018, done 56800 episodes, -58.40843544750426, current eps 0.01888755935742431 0\n",
      "361678, done 56900 episodes, -55.70674489694902, current eps 0.018885336611322537 0\n",
      "362339, done 57000 episodes, -48.18648776734602, current eps 0.01888311049742364 0\n",
      "362958, done 57100 episodes, -50.73942112152662, current eps 0.018881025831003942 0\n",
      "363597, done 57200 episodes, -60.52302720888965, current eps 0.01887887380864177 0\n",
      "364157, done 57300 episodes, -60.28100832848509, current eps 0.018876987842252384 0\n",
      "364763, done 57400 episodes, -62.06226933712068, current eps 0.0188749469571953 0\n",
      "365394, done 57500 episodes, -60.59623693317981, current eps 0.01887282187721012 0\n",
      "366006, done 57600 episodes, -58.70715691567522, current eps 0.01887076078537029 0\n",
      "366632, done 57700 episodes, -63.96327938860316, current eps 0.01886865254437073 0\n",
      "367379, done 57800 episodes, -46.80601988762721, current eps 0.018866136799919175 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "368021, done 57900 episodes, -60.08037253226208, current eps 0.01886397467416563 0\n",
      "368693, done 58000 episodes, -54.71810936633633, current eps 0.018861711514498367 0\n",
      "369471, done 58100 episodes, -55.32595191789024, current eps 0.018859091368335974 0\n",
      "370125, done 58200 episodes, -66.14323410108591, current eps 0.01885688882901694 0\n",
      "370842, done 58300 episodes, -66.80121374144306, current eps 0.018854474118479105 0\n",
      "371551, done 58400 episodes, -55.14133429013578, current eps 0.018852086350318258 0\n",
      "372311, done 58500 episodes, -51.02728487133734, current eps 0.018849526824504092 0\n",
      "372899, done 58600 episodes, -67.09274174195923, current eps 0.018847546559795238 0\n",
      "373609, done 58700 episodes, -53.142934607819896, current eps 0.01884515542383727 0\n",
      "374272, done 58800 episodes, -64.13151657085724, current eps 0.018842922574344122 0\n",
      "374877, done 58900 episodes, -67.66140948014717, current eps 0.018840885057084162 0\n",
      "375695, done 59000 episodes, -40.90563950908305, current eps 0.018838130199036813 0\n",
      "376444, done 59100 episodes, -53.48640168155391, current eps 0.01883560771899101 0\n",
      "377185, done 59200 episodes, -56.518281393599274, current eps 0.0188331121813222 0\n",
      "377935, done 59300 episodes, -57.107631427826675, current eps 0.018830586333479272 0\n",
      "378531, done 59400 episodes, -66.50520105703748, current eps 0.01882857912639343 0\n",
      "379249, done 59500 episodes, -55.46631385069171, current eps 0.018826161048058468 0\n",
      "379857, done 59600 episodes, -61.377335599953376, current eps 0.018824113427407134 0\n",
      "Evaluation score: 17.060958012015398\n",
      "0 of 20 episodes ended close to / at the final state.\n",
      "380486, done 59700 episodes, -61.078667540493576, current eps 0.0188219950830162 0\n",
      "381228, done 59800 episodes, -58.48896047832297, current eps 0.018819496177550264 0\n",
      "381909, done 59900 episodes, -58.03821647631385, current eps 0.01881720270770889 0\n",
      "382640, done 60000 episodes, -50.45336517047154, current eps 0.018814740848011317 0\n",
      "383267, done 60100 episodes, -67.31519777795346, current eps 0.01881262923921463 0\n",
      "383823, done 60200 episodes, -79.10089577526799, current eps 0.01881075674401374 0\n",
      "384438, done 60300 episodes, -64.30832860431823, current eps 0.018808685548782542 0\n",
      "384994, done 60400 episodes, -70.12148603696649, current eps 0.018806813053581655 0\n",
      "385565, done 60500 episodes, -72.13853111822604, current eps 0.018804890041423907 0\n",
      "386101, done 60600 episodes, -71.05961801251291, current eps 0.018803084902165496 0\n",
      "386706, done 60700 episodes, -63.44745949092145, current eps 0.018801047384905536 0\n",
      "387216, done 60800 episodes, -69.45548307081245, current eps 0.018799329808372345 0\n",
      "387724, done 60900 episodes, -75.45297104382561, current eps 0.018797618967433403 0\n",
      "388358, done 61000 episodes, -61.36668388733899, current eps 0.01879548378405685 0\n",
      "388872, done 61100 episodes, -73.8495113600082, current eps 0.018793752736335163 0\n",
      "389528, done 61200 episodes, -64.5276857787236, current eps 0.018791543461421884 0\n",
      "390252, done 61300 episodes, -62.86614373356406, current eps 0.01878910517630418 0\n",
      "390863, done 61400 episodes, -69.01956305339941, current eps 0.018787047452261476 0\n"
     ]
    }
   ],
   "source": [
    "#state = env.reset()\n",
    "#agent = Agent(n_actions=n_actions, inp_size=state.shape, device=device, hidden=10, agent_history_length=agent_history_length, memory_size=replay_memory_size, batch_size=32, learning_rate=learning_rate)\n",
    "\n",
    "transition = init_transition()\n",
    "agent = Agent(n_actions=n_actions, inp_size=np.array(transition).shape, device=device, hidden=10, agent_history_length=agent_history_length, memory_size=replay_memory_size)#, batch_size=512, learning_rate=learning_rate)\n",
    "action_scheduler = Action_Scheduler(num_actions=n_actions, max_steps=max_steps, eps_annealing_steps=30000, eps_final=0.02, replay_memory_start_size=start_learning, model=agent.main_dqn)\n",
    "\n",
    "step_counter = 0\n",
    "\n",
    "eps_rewards = []\n",
    "\n",
    "\n",
    "\n",
    "print(\"Start training...\")\n",
    "while step_counter < max_steps:\n",
    "    epoch_step = 0\n",
    "    #agent.main_dqn.train()\n",
    "######## fill memory begins here\n",
    "    while (epoch_step < evaluate_every) or (step_counter < start_learning):\n",
    "        state = env.reset()\n",
    "        #transition = init_transition()\n",
    "        referenceLine = env.referenceStreamline_ijk\n",
    "        episode_reward_sum = 0\n",
    "        terminal = False\n",
    "        #fill replay memory while interacting with env\n",
    "        #for episode_counter in range(max_episode_length):\n",
    "        episode_step_counter = 0\n",
    "        positive_run = 0\n",
    "\n",
    "        while not terminal:\n",
    "            # get action with epsilon-greedy strategy       \n",
    "            action = action_scheduler.get_action(step_counter, torch.FloatTensor([np.array(transition)]).to(device))\n",
    "            \n",
    "            next_state, reward, terminal = env.step(action)\n",
    "            next_transition = add_to_transition(next_state, transition)\n",
    "            \n",
    "            step_counter += 1\n",
    "            epoch_step += 1\n",
    "            episode_step_counter += 1\n",
    "\n",
    "            # accumulate reward for current episode\n",
    "            episode_reward_sum += reward\n",
    "\n",
    "\n",
    "            agent.replay_memory.add_experience(action=action,\n",
    "                                state=np.array(transition),\n",
    "                                reward=reward,\n",
    "                                new_state=np.array(next_transition),\n",
    "                                terminal=terminal)\n",
    "\n",
    "\n",
    "            #state = next_state\n",
    "            transition = next_transition\n",
    "\n",
    "\n",
    "\n",
    "            ####### optimization is happening here\n",
    "            if step_counter > start_learning:\n",
    "                #if reward > 0.:\n",
    "                #    print(\"reward was positive: \", reward)\n",
    "                loss = agent.optimize()\n",
    "\n",
    "\n",
    "            ####### target network update\n",
    "            if step_counter > start_learning and step_counter % network_update_every == 0:\n",
    "                #print(\"Update net\")\n",
    "                #print(agent.main_dqn(torch.tensor(state).to(device).unsqueeze(0)))\n",
    "                #print(agent.target_dqn(torch.tensor(state).to(device).unsqueeze(0)))\n",
    "                agent.target_dqn.load_state_dict(agent.main_dqn.state_dict())\n",
    "\n",
    "            # if episode ended before maximum step\n",
    "            if terminal:\n",
    "                terminal = False\n",
    "                #state = env.reset()\n",
    "                transition = init_transition()\n",
    "                #eps_steps[len(eps_rewards)%10] = episode_step_counter\n",
    "                #print(\"Positive rewards of this episode: \", positive_run)\n",
    "                break\n",
    "\n",
    "        eps_rewards.append(episode_reward_sum)\n",
    "\n",
    "        if len(eps_rewards) % 100 == 0:\n",
    "            #with open(path+'/logs/rewards.dat', 'a') as reward_file:\n",
    "                #print(\"[{}] {}, {}\".format(len(eps_rewards), step_counter, np.mean(eps_rewards[-100:])), file=reward_file)\n",
    "            print(\"{}, done {} episodes, {}, current eps {}\".format(step_counter, len(eps_rewards), np.mean(eps_rewards[-100:]), action_scheduler.eps_current), env.points_visited)\n",
    "    #torch.save(agent.main_dqn.state_dict(), path+'/checkpoints/fibre_agent_{}_reward_{:.2f}.pth'.format(step_counter, np.mean(eps_rewards[-100:])))\n",
    "\n",
    "########## evaluation starting here\n",
    "    eval_rewards = []\n",
    "    #agent.main_dqn.eval()\n",
    "    for _ in range(eval_runs):\n",
    "        eval_steps = 0\n",
    "        #state = env.reset()\n",
    "        transition = init_transition()\n",
    "        eval_episode_reward = 0\n",
    "        episode_final = 0\n",
    "        while eval_steps < max_episode_length:\n",
    "            action = action_scheduler.get_action(step_counter, torch.FloatTensor([np.array(transition)]).to(device), evaluation=True)\n",
    "\n",
    "            next_state, reward, terminal = env.step(action)\n",
    "            #next_state = next_state\n",
    "            next_transition = add_to_transition(next_state, transition)\n",
    "\n",
    "            eval_steps += 1\n",
    "            eval_episode_reward += reward\n",
    "            #state = next_state\n",
    "            transition = next_transition\n",
    "            if terminal:\n",
    "                terminal = False\n",
    "                if reward == 100:\n",
    "                    episode_final += 1\n",
    "                break\n",
    "\n",
    "        eval_rewards.append(eval_episode_reward)\n",
    "\n",
    "    print(\"Evaluation score:\", np.mean(eval_rewards))\n",
    "    print(\"{} of {} episodes ended close to / at the final state.\".format(episode_final, eval_runs))\n",
    "    if np.mean(eval_rewards) > 10.:\n",
    "        torch.save(agent.main_dqn.state_dict(), 'trained_agents/multiple/fibre_agent_{}_reward_{:.2f}.pth'.format(step_counter, np.mean(eval_rewards)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[43.4998, 94.5377, 24.2661],\n",
      "        [42.8719, 94.4639, 24.7563],\n",
      "        [42.2316, 94.5010, 25.2345],\n",
      "        [41.5913, 94.5380, 25.7127],\n",
      "        [40.8953, 94.6029, 26.1017],\n",
      "        [40.2550, 94.6399, 26.5799]])\n",
      "0 97 [42.54434095 94.81857103 24.1755491 ] [43.4998  94.53768 24.26608] 23.15581395348837\n",
      "1 2 [42.52571373 94.58630794 25.14802371] [42.871902 94.46394  24.75629 ] 23.15581395348837\n",
      "2 12 [41.93567501 94.22950766 25.87228048] [42.231613 94.50095  25.234474] 23.155813953488362\n",
      "3 25 [41.07959335 93.97352899 26.32127839] [41.591324 94.537964 25.712658] 23.155813953488376\n",
      "4 25 [40.2235117  93.71755031 26.7702763 ] [40.895332 94.602905 26.101734] 23.155813953488376\n",
      "5 17 [39.52677332 93.715097   27.48759744] [40.255043 94.639915 26.579918] 23.155813953488376\n",
      "6 25 [38.67069167 93.45911832 27.93659534] [39.614754 94.676926 27.058102] -0.09999999999999432\n",
      "7 14 [38.21292173 94.05692495 28.59467886] [39.11261  94.762375 27.674988] -0.09999999999999432\n",
      "8 11 [38.4159608  94.74804132 29.2883166 ] [38.87813  94.92816  28.421669] 23.155813953488376\n",
      "9 3 [38.59327622 95.13100013 30.19490459] [38.840546 95.24009  29.157393] 23.155813953488376\n",
      "10 3 [38.77059165 95.51395894 31.10149258] [38.900295 95.595894 29.87142 ] 23.155813953488376\n",
      "11 16 [39.26181934 95.96763983 31.8450442 ] [39.081455 95.959694 30.5605  ] 23.155813953488376\n",
      "12 16 [39.75304704 96.42132073 32.58859581] [39.26261  96.323494 31.249578] 23.155813953488376\n",
      "13 16 [40.24427473 96.87500162 33.33214742] [39.53401  96.71169  31.894283] 23.155813953488348\n",
      "14 29 [40.98367069 97.33624798 33.82260171] [39.908535 97.07485  32.50079 ] 23.155813953488348\n",
      "15 29 [41.72306665 97.79749434 34.313056  ] [40.328    97.50328  33.030415] -0.10000000000002274\n",
      "16 65 [41.78103445 98.52084217 33.62500975] [40.68503  98.02276  33.523026] 23.155813953488348\n",
      "17 65 [41.83900225 99.24419    32.93696349] [41.074257 98.59413  33.925568] -0.10000000000002274\n",
      "18 16 [42.33022994 99.69787089 33.6805151 ] [41.5581   99.12595  34.276367] 23.155813953488348\n",
      "19 11 [ 42.53326901 100.38898726  34.37415284] [42.061546 99.69463  34.527657] 23.155813953488348\n",
      "20 11 [ 42.73630807 101.08010362  35.06779057] [ 42.564987 100.26331   34.77895 ] 23.155813953488348\n",
      "21 16 [ 43.22753577 101.53378451  35.81134218] [ 43.08232  100.85572   34.925304] 23.155813953488348\n",
      "22 37 [ 43.99132279 102.15158032  35.99831203] [ 43.466732 101.550224  34.825832] -0.10000000000002274\n",
      "23 65 [ 44.04929059 102.87492815  35.31026578] [ 43.773514 102.26846   34.652588] 23.155813953488348\n",
      "24 65 [ 44.10725839 103.59827598  34.62221952] [ 43.99389  102.997765  34.408543] 23.155813953488348\n",
      "25 94 [ 43.86360343 104.54880609  34.42954164] [ 44.119606 103.76202   34.208275] 23.155813953488348\n",
      "26 11 [ 44.0666425  105.23992246  35.12317938] [ 44.148006 104.51739   33.946335] -0.10000000000002274\n",
      "27 3 [ 44.24395793 105.62288127  36.02976737] [ 44.17641  105.27275   33.684395] -100\n",
      "Evaluation score: 385.6720930232555\n"
     ]
    }
   ],
   "source": [
    "eval_rewards = []\n",
    "all_distances = []\n",
    "all_states = []\n",
    "#agent.main_dqn.eval()\n",
    "for _ in range(1):\n",
    "    eval_steps = 0\n",
    "    #state = env.reset()\n",
    "    #print(state.getCoordinate())\n",
    "    #all_states.append(state.getCoordinate())\n",
    "    transition = init_transition()\n",
    "    all_states.append(torch.tensor(list(transition)[:3]))\n",
    "    eval_episode_reward = 0\n",
    "    episode_final = 0\n",
    "    print(env.referenceStreamline_ijk[:6])\n",
    "    \n",
    "    while eval_steps < max_episode_length:\n",
    "        action = action_scheduler.get_action(step_counter, torch.FloatTensor([np.array(transition)]).to(device), evaluation=True)\n",
    "\n",
    "        next_state, reward, terminal = env.step(action)\n",
    "        #next_state = next_state\n",
    "        next_transition = add_to_transition(next_state, transition)\n",
    "        #reward = 1 + (1+(reward/10))\n",
    "        #if reward > 1:\n",
    "        #    reward = 1\n",
    "        #elif reward > 0.:\n",
    "        #    reward = 0\n",
    "        #else:\n",
    "        #    reward = -1\n",
    "        eval_episode_reward += reward\n",
    "        print(eval_steps, action, next_state.getCoordinate().numpy(), env.referenceStreamline_ijk[np.min([eval_steps,len(env.referenceStreamline_ijk)-1])].numpy(), reward)\n",
    "        eval_steps += 1\n",
    "        all_distances.append(reward)\n",
    "        all_states.append(next_state.getCoordinate())\n",
    "        \n",
    "        #state = next_state\n",
    "        transition = next_transition\n",
    "        if terminal:\n",
    "            terminal = False\n",
    "            #if reward > 0.9:\n",
    "            #    episode_final += 1\n",
    "            break\n",
    "\n",
    "    eval_rewards.append(eval_episode_reward)\n",
    "\n",
    "print(\"Evaluation score:\", np.min(eval_rewards))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = env.reset()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sphere_dist(nextState):\n",
    "    current_index = np.min([env.stepCounter,len(env.referenceStreamline_ijk)-1])\n",
    "    x_dist = (nextState.getCoordinate()[0] - env.referenceStreamline_ijk[current_index][0]) **2\n",
    "    y_dist = (nextState.getCoordinate()[1] - env.referenceStreamline_ijk[current_index][1]) **2\n",
    "    z_dist = (nextState.getCoordinate()[2] - env.referenceStreamline_ijk[current_index][2]) **2\n",
    "    return x_dist + y_dist + z_dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.99\n"
     ]
    }
   ],
   "source": [
    "print(agent.gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "47 11.394252873563218 tensor(0.1970, dtype=torch.float64) tensor(0.1970, dtype=torch.float64)\n",
      "67 11.394252873563218 tensor(0.2640, dtype=torch.float64) tensor(0.2640, dtype=torch.float64)\n",
      "72 11.394252873563218 tensor(0.2650, dtype=torch.float64) tensor(0.2650, dtype=torch.float64)\n",
      "75 11.394252873563218 tensor(0.1352, dtype=torch.float64) tensor(0.1352, dtype=torch.float64)\n",
      "80 11.394252873563218 tensor(0.0623, dtype=torch.float64) tensor(0.0623, dtype=torch.float64)\n",
      "88 11.394252873563218 tensor(0.0726, dtype=torch.float64) tensor(0.0726, dtype=torch.float64)\n",
      "93 11.394252873563218 tensor(0.1499, dtype=torch.float64) tensor(0.1499, dtype=torch.float64)\n",
      "96 11.394252873563218 tensor(0.1986, dtype=torch.float64) tensor(0.1986, dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "for i in range(n_actions):\n",
    "    state = env.reset()\n",
    "    next_state, reward, done = env.step(i)\n",
    "    s_dist = sphere_dist(next_state)\n",
    "    old_dist = torch.sum((env.referenceStreamline_ijk[env.stepCounter] - next_state.getCoordinate())**2)\n",
    "    if s_dist <= 0.52**2:\n",
    "        print(i, reward, s_dist, old_dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 77.07567091 108.90497243  91.49815967] -100\n"
     ]
    }
   ],
   "source": [
    "next_state, reward, done = env.step(75)\n",
    "print(next_state.getCoordinate(), reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[30.283401924575482, 102.46417647849394, 66.32755479299973]\n"
     ]
    }
   ],
   "source": [
    "print(list(transition)[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 24.1166, 103.8659,  64.9889])\n"
     ]
    }
   ],
   "source": [
    "state = env.reset()\n",
    "#referenceLine = env.referenceStreamline_ijk\n",
    "print(state.getCoordinate())\n",
    "#print(referenceLine[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 23.13776944 102.38550419  64.06677327] [ 30.125023 102.08756   66.46997 ] -100\n"
     ]
    }
   ],
   "source": [
    "next_state, reward, done = env.step(74)\n",
    "print(next_state.getCoordinate().numpy(), env.referenceStreamline_ijk[np.min([env.stepCounter, len(referenceLine)])].numpy(), reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "71\n"
     ]
    }
   ],
   "source": [
    "optimal_steps =  [80, 88, 54, 96, 100, 67, 83, 75, 83, 75, 100, 83, 70, 67, 59, 100, 67, 59, 59, 59, 51, 100, 59, 56, 51, 61, 100, 66, 71, 66, 71, 71, 100, 71, 71, 71, 71, 100, 92, 84, 84, 38, 100, 97, 97, 97, 38, 100, 97, 30, 43, 43, 48, 100, 94, 81, 94, 35, 97, 100, 35, 22, 35, 35, 6, 100, 19, 3, 16, 3, 21, 100, 21, 16, 34, 21, 98, 34, 100, 39, 93, 39, 72, 72, 100, 69, 100]\n",
    "transition = init_transition()\n",
    "referenceLine = env.referenceStreamline_ijk\n",
    "print(len(referenceLine))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80 -0.09999999999999964\n"
     ]
    }
   ],
   "source": [
    "#action = action_scheduler.get_action(step_counter, torch.FloatTensor([np.array(transition)]).to(device), evaluation=True)\n",
    "next_state, reward, terminal = env.step(88)\n",
    "next_transition = add_to_transition(next_state, transition)\n",
    "print(action, reward)\n",
    "transition = next_transition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State:  [ 73.651344 107.88106   93.29415 ]\n",
      "Next State:  [ 74.56195007 107.80595503  92.88775652]\n",
      "7.450580596923828e-09\n"
     ]
    }
   ],
   "source": [
    "# Debugging the reward function\n",
    "referenceLine = env.referenceStreamline_ijk\n",
    "stepCounter = 0\n",
    "maxSteps=200\n",
    "state = env.reset()\n",
    "print(\"State: \", state.getCoordinate().numpy())\n",
    "next_state, _, terminal = env.step(80)\n",
    "print(\"Next State: \", next_state.getCoordinate().numpy())\n",
    "\n",
    "def lineseg_dist(p, a, b):\n",
    "\n",
    "    # normalized tangent vector\n",
    "    d = np.divide(b - a, np.linalg.norm(b - a))\n",
    "\n",
    "    # signed parallel distance components\n",
    "    s = np.dot(a - p, d)\n",
    "    t = np.dot(p - b, d)\n",
    "\n",
    "    # clamped parallel distance\n",
    "    h = np.maximum.reduce([s, t, 0])\n",
    "\n",
    "    # perpendicular distance component\n",
    "    c = np.cross(p - a, d)\n",
    "\n",
    "    return np.hypot(h, np.linalg.norm(c))\n",
    "\n",
    "distance = lineseg_dist(referenceLine[86].numpy(), referenceLine[85].numpy(), referenceLine[86].numpy())\n",
    "print(distance)\n",
    "\n",
    "#print(\"Diff: \", next_state.getCoordinate().numpy()-state.getCoordinate().numpy())\n",
    "#qry_pt = next_state.getCoordinate().view(-1,3)\n",
    "#print(\"Reference next state: \", referenceLine[stepCounter+1])\n",
    "#print(\"Diff to reference state: \", referenceLine[stepCounter+1]-next_state.getCoordinate().numpy())\n",
    "#distance = torch.min(torch.sum((referenceLine[np.min([stepCounter+1, maxSteps-1])] - qry_pt)**2, dim=1))\n",
    "#print(distance)\n",
    "#reward = torch.tanh(-distance+5.3)\n",
    "\n",
    "#if distance == -1:\n",
    "#    reward = 0.5\n",
    "#elif distance < 0.8:\n",
    "#    reward = 1+ (1-distance)\n",
    "#else:\n",
    "#    reward = np.max([1 - distance, -1])\n",
    "#print(reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.19954145509142476\n",
      "0.03981679230000309\n",
      "0.07041062249999892\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "state = np.array([ 75.6, 107.95,  92.22])\n",
    "line = np.array([ 75.78847, 107.96255,  92.28433])\n",
    "\n",
    "print(np.linalg.norm(line - state, 2))\n",
    "\n",
    "sphere_dist = ((state[0] - line[0])**2 + (state[1]-line[1])**2 + (state[2]-line[2])**2)\n",
    "print(sphere_dist)\n",
    "normal_diff = np.sum(state-line)**2\n",
    "print(normal_diff)\n",
    "if sphere_dist < 0.2**2:\n",
    "    print(True)\n",
    "else:\n",
    "    print(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action:  100 Reward:  -0.6400096416473389\n",
      "Action:  80 Reward:  -0.3780286503520091\n",
      "Action:  75 Reward:  -0.17094774926353554\n",
      "Action:  80 Reward:  -0.06020208127816557\n",
      "Action:  75 Reward:  -0.023724592605490286\n",
      "Action:  100 Reward:  -0.023724592605490286\n",
      "Action:  62 Reward:  -0.031680450691759385\n",
      "Action:  75 Reward:  -0.10966306950569177\n",
      "Action:  83 Reward:  -0.2621558822401104\n",
      "Action:  75 Reward:  -0.4853828474102172\n",
      "Action:  83 Reward:  -0.7713330234194417\n",
      "Action:  100 Reward:  -0.7713330234194417\n",
      "Action:  83 Reward:  -1.158927472394806\n",
      "Action:  62 Reward:  -1.6468544226974164\n",
      "Action:  67 Reward:  -2.2078664481054533\n",
      "Action:  51 Reward:  -2.459970885856384\n",
      "Action:  67 Reward:  -3.0430611441644246\n",
      "Action:  100 Reward:  -3.0430611441644246\n",
      "Action:  59 Reward:  -3.7319386628026487\n",
      "Action:  59 Reward:  -4.531517914723884\n",
      "Action:  59 Reward:  -5.427621034792285\n",
      "Action:  100 Reward:  -5.427621034792285\n",
      "Action:  59 Reward:  -6.420164664306009\n",
      "Action:  56 Reward:  -7.21012573500936\n",
      "Action:  51 Reward:  -8.185668593823168\n",
      "Action:  56 Reward:  -9.264514952082235\n",
      "Action:  66 Reward:  -9.504376152250732\n",
      "Action:  100 Reward:  -9.504376152250732\n",
      "Action:  66 Reward:  -10.878316642589324\n",
      "Action:  71 Reward:  -11.684246290994608\n",
      "Action:  71 Reward:  -13.482882171224585\n",
      "Action:  79 Reward:  -15.013766894750662\n",
      "Action:  58 Reward:  -16.29778288166172\n",
      "Action:  100 Reward:  -16.29778288166172\n",
      "Action:  71 Reward:  -17.927867464472456\n",
      "Action:  71 Reward:  -19.616943063578457\n",
      "Action:  84 Reward:  -20.74882253322799\n",
      "Action:  71 Reward:  -22.68542229369059\n",
      "Action:  100 Reward:  -22.68542229369059\n",
      "Action:  92 Reward:  -24.094491148196\n",
      "Action:  84 Reward:  -26.036910111309087\n",
      "Action:  92 Reward:  -27.894909786068872\n",
      "Action:  97 Reward:  -29.22946434143986\n",
      "Action:  100 Reward:  -29.22946434143986\n",
      "Action:  97 Reward:  -30.93770942329201\n",
      "Action:  38 Reward:  -33.06379059780991\n",
      "Action:  97 Reward:  -35.48076469151409\n",
      "Action:  43 Reward:  -37.24656758094286\n",
      "Action:  38 Reward:  -39.87506653295411\n",
      "Action:  100 Reward:  -39.87506653295411\n",
      "Action:  43 Reward:  -42.07728895704025\n",
      "Action:  43 Reward:  -44.59950388329224\n",
      "Action:  89 Reward:  -46.525702788416744\n",
      "Action:  48 Reward:  -46.51577793318299\n",
      "Action:  100 Reward:  -46.51577793318299\n",
      "Action:  94 Reward:  -44.74553361569235\n",
      "Action:  81 Reward:  -46.71395822922152\n",
      "Action:  48 Reward:  -50.26025222152221\n",
      "Action:  43 Reward:  -53.83576866536265\n",
      "Action:  100 Reward:  -53.83576866536265\n",
      "Action:  35 Reward:  -57.7679173101252\n",
      "Action:  43 Reward:  -61.07110670119785\n",
      "Action:  35 Reward:  -64.29611023518841\n",
      "Action:  22 Reward:  -64.69258594426614\n",
      "Action:  27 Reward:  -67.94753644296517\n",
      "Action:  100 Reward:  -67.94753644296517\n",
      "Action:  6 Reward:  -68.60859514506335\n",
      "Action:  11 Reward:  -70.03265506052335\n",
      "Action:  3 Reward:  -69.6074991211795\n",
      "Action:  16 Reward:  -66.45768588248448\n",
      "Action:  100 Reward:  -66.45768588248448\n",
      "Action:  8 Reward:  -63.980960033928525\n",
      "Action:  29 Reward:  -63.97912872209463\n",
      "Action:  21 Reward:  -65.2877329760679\n",
      "Action:  21 Reward:  -71.00622766156863\n",
      "Action:  100 Reward:  -71.00622766156863\n",
      "Action:  34 Reward:  -75.30646175774287\n",
      "Action:  26 Reward:  -79.7251625711022\n",
      "Action:  26 Reward:  -85.7736700190284\n",
      "Action:  93 Reward:  -86.29779314738443\n",
      "Action:  39 Reward:  -89.76066176762737\n",
      "Action:  100 Reward:  -89.76066176762737\n",
      "Action:  93 Reward:  -92.76729682665923\n",
      "Action:  72 Reward:  -91.66762326274807\n",
      "Action:  77 Reward:  -91.52645978577566\n",
      "Action:  77 Reward:  -94.77110103827272\n",
      "Action:  100 Reward:  -94.77110103827272\n",
      "-3134.679829616308\n"
     ]
    }
   ],
   "source": [
    "#optimal_steps = [80, 75, 80, 75, 62, 75, 83, 96, 51, 24, 62, 62, 62, 77, 65, 64, 67, 59, 56, 83, 82, 54, 56, 53, 56, 38, 56, 84, 66, 71, 71, 64, 97, 84, 71, 71, 38, 51, 30, 92, 97, 84, 43, 79, 27, 46, 89, 25, 81, 25, 48, 43, 86, 48, 57, 14, 89, 43, 43, 19, 92, 14, 27, 9, 78, 4, 16, 3, 29, 3, 47, 6, 42, 21, 39, 5, 72, 34, 98, 88, 90, 75, 77, 59, 49, 32, 82, 100]\n",
    "eps_reward = 0\n",
    "state = env.reset()\n",
    "for i in optimal_steps:\n",
    "    next_state, reward, terminal = env.step(i)\n",
    "    state = next_state\n",
    "    eps_reward += reward.item()\n",
    "    print(\"Action: \", i, \"Reward: \", reward.item())\n",
    "print(eps_reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Init environment..\n",
      "Loading precomputed streamlines (data/HCP307200_DTI_smallSet.vtk) for ID 100307\n",
      "..done!\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(\"Init environment..\")\n",
    "env = RLTe.RLtractEnvironment(device = 'cpu')\n",
    "print(\"..done!\")\n",
    "n_actions = env.action_space.n\n",
    "#print(n_actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([87, 3])\n"
     ]
    }
   ],
   "source": [
    "referenceLine = env.referenceStreamline_ijk\n",
    "print(referenceLine.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([47.8702, 74.8030, 26.6401])\n",
      "tensor([47.8702, 74.8030, 26.6401])\n"
     ]
    }
   ],
   "source": [
    "print(referenceLine[0])\n",
    "state = TractographyState(referenceLine[0], env.interpolateDWIatState)\n",
    "print(state.getCoordinate())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step:  0 Action:  41 Distance:  tensor(1.3545, dtype=torch.float64)\n",
      "Step:  0 Action:  66 Distance:  tensor(1.2187, dtype=torch.float64)\n",
      "Step:  0 Action:  74 Distance:  tensor(1.5895, dtype=torch.float64)\n",
      "Step:  0 Action:  79 Distance:  tensor(1.4856, dtype=torch.float64)\n",
      "Step:  0 Action:  82 Distance:  tensor(1.2496, dtype=torch.float64)\n",
      "Step:  0 Action:  87 Distance:  tensor(1.5944, dtype=torch.float64)\n",
      "Step:  0 Action:  95 Distance:  tensor(1.5184, dtype=torch.float64)\n",
      "0 []\n",
      "Step:  1 Action:  61 Distance:  tensor(1.3162, dtype=torch.float64)\n",
      "Step:  1 Action:  66 Distance:  tensor(1.2710, dtype=torch.float64)\n",
      "Step:  1 Action:  74 Distance:  tensor(1.6272, dtype=torch.float64)\n",
      "Step:  1 Action:  79 Distance:  tensor(1.3920, dtype=torch.float64)\n",
      "Step:  1 Action:  82 Distance:  tensor(1.4070, dtype=torch.float64)\n",
      "Step:  1 Action:  87 Distance:  tensor(1.4471, dtype=torch.float64)\n",
      "Step:  1 Action:  95 Distance:  tensor(1.5094, dtype=torch.float64)\n",
      "1 []\n",
      "Step:  2 Action:  41 Distance:  tensor(1.2821, dtype=torch.float64)\n",
      "Step:  2 Action:  61 Distance:  tensor(1.2315, dtype=torch.float64)\n",
      "Step:  2 Action:  74 Distance:  tensor(1.6046, dtype=torch.float64)\n",
      "Step:  2 Action:  79 Distance:  tensor(1.3549, dtype=torch.float64)\n",
      "Step:  2 Action:  82 Distance:  tensor(1.4040, dtype=torch.float64)\n",
      "Step:  2 Action:  87 Distance:  tensor(1.4843, dtype=torch.float64)\n",
      "Step:  2 Action:  95 Distance:  tensor(1.5651, dtype=torch.float64)\n",
      "2 []\n",
      "Step:  3 Action:  41 Distance:  tensor(1.2908, dtype=torch.float64)\n",
      "Step:  3 Action:  49 Distance:  tensor(1.2627, dtype=torch.float64)\n",
      "Step:  3 Action:  61 Distance:  tensor(1.2172, dtype=torch.float64)\n",
      "Step:  3 Action:  74 Distance:  tensor(1.5797, dtype=torch.float64)\n",
      "Step:  3 Action:  79 Distance:  tensor(1.2516, dtype=torch.float64)\n",
      "Step:  3 Action:  82 Distance:  tensor(1.4621, dtype=torch.float64)\n",
      "Step:  3 Action:  87 Distance:  tensor(1.4322, dtype=torch.float64)\n",
      "Step:  3 Action:  95 Distance:  tensor(1.6009, dtype=torch.float64)\n",
      "3 []\n",
      "Step:  4 Action:  49 Distance:  tensor(1.3226, dtype=torch.float64)\n",
      "Step:  4 Action:  61 Distance:  tensor(1.3563, dtype=torch.float64)\n",
      "Step:  4 Action:  74 Distance:  tensor(1.5702, dtype=torch.float64)\n",
      "Step:  4 Action:  82 Distance:  tensor(1.5784, dtype=torch.float64)\n",
      "Step:  4 Action:  87 Distance:  tensor(1.2502, dtype=torch.float64)\n",
      "Step:  4 Action:  95 Distance:  tensor(1.5545, dtype=torch.float64)\n",
      "4 []\n",
      "Step:  5 Action:  49 Distance:  tensor(1.2380, dtype=torch.float64)\n",
      "Step:  5 Action:  61 Distance:  tensor(1.4426, dtype=torch.float64)\n",
      "Step:  5 Action:  69 Distance:  tensor(1.2523, dtype=torch.float64)\n",
      "Step:  5 Action:  74 Distance:  tensor(1.5910, dtype=torch.float64)\n",
      "Step:  5 Action:  82 Distance:  tensor(1.5806, dtype=torch.float64)\n",
      "Step:  5 Action:  87 Distance:  tensor(1.2100, dtype=torch.float64)\n",
      "Step:  5 Action:  95 Distance:  tensor(1.4956, dtype=torch.float64)\n",
      "5 []\n",
      "Step:  6 Action:  61 Distance:  tensor(1.5273, dtype=torch.float64)\n",
      "Step:  6 Action:  66 Distance:  tensor(1.3657, dtype=torch.float64)\n",
      "Step:  6 Action:  69 Distance:  tensor(1.2500, dtype=torch.float64)\n",
      "Step:  6 Action:  74 Distance:  tensor(1.6194, dtype=torch.float64)\n",
      "Step:  6 Action:  79 Distance:  tensor(1.2759, dtype=torch.float64)\n",
      "Step:  6 Action:  82 Distance:  tensor(1.5115, dtype=torch.float64)\n",
      "Step:  6 Action:  87 Distance:  tensor(1.2051, dtype=torch.float64)\n",
      "Step:  6 Action:  95 Distance:  tensor(1.3869, dtype=torch.float64)\n",
      "6 []\n",
      "Step:  7 Action:  61 Distance:  tensor(1.5700, dtype=torch.float64)\n",
      "Step:  7 Action:  66 Distance:  tensor(1.4237, dtype=torch.float64)\n",
      "Step:  7 Action:  69 Distance:  tensor(1.2798, dtype=torch.float64)\n",
      "Step:  7 Action:  74 Distance:  tensor(1.5954, dtype=torch.float64)\n",
      "Step:  7 Action:  79 Distance:  tensor(1.2693, dtype=torch.float64)\n",
      "Step:  7 Action:  82 Distance:  tensor(1.4712, dtype=torch.float64)\n",
      "Step:  7 Action:  95 Distance:  tensor(1.2862, dtype=torch.float64)\n",
      "7 []\n",
      "Step:  8 Action:  53 Distance:  tensor(1.2474, dtype=torch.float64)\n",
      "Step:  8 Action:  61 Distance:  tensor(1.6099, dtype=torch.float64)\n",
      "Step:  8 Action:  66 Distance:  tensor(1.4011, dtype=torch.float64)\n",
      "Step:  8 Action:  69 Distance:  tensor(1.3809, dtype=torch.float64)\n",
      "Step:  8 Action:  74 Distance:  tensor(1.5549, dtype=torch.float64)\n",
      "Step:  8 Action:  82 Distance:  tensor(1.4958, dtype=torch.float64)\n",
      "Step:  8 Action:  95 Distance:  tensor(1.2269, dtype=torch.float64)\n",
      "8 []\n",
      "Step:  9 Action:  53 Distance:  tensor(1.2052, dtype=torch.float64)\n",
      "Step:  9 Action:  56 Distance:  tensor(1.2501, dtype=torch.float64)\n",
      "Step:  9 Action:  61 Distance:  tensor(1.6062, dtype=torch.float64)\n",
      "Step:  9 Action:  66 Distance:  tensor(1.2593, dtype=torch.float64)\n",
      "Step:  9 Action:  69 Distance:  tensor(1.5104, dtype=torch.float64)\n",
      "Step:  9 Action:  74 Distance:  tensor(1.4608, dtype=torch.float64)\n",
      "Step:  9 Action:  82 Distance:  tensor(1.5458, dtype=torch.float64)\n",
      "9 []\n",
      "Step:  10 Action:  56 Distance:  tensor(1.2257, dtype=torch.float64)\n",
      "Step:  10 Action:  61 Distance:  tensor(1.5655, dtype=torch.float64)\n",
      "Step:  10 Action:  69 Distance:  tensor(1.5418, dtype=torch.float64)\n",
      "Step:  10 Action:  74 Distance:  tensor(1.4100, dtype=torch.float64)\n",
      "Step:  10 Action:  82 Distance:  tensor(1.5741, dtype=torch.float64)\n",
      "Step:  10 Action:  90 Distance:  tensor(1.2631, dtype=torch.float64)\n",
      "10 []\n",
      "Step:  11 Action:  56 Distance:  tensor(1.2257, dtype=torch.float64)\n",
      "Step:  11 Action:  61 Distance:  tensor(1.5655, dtype=torch.float64)\n",
      "Step:  11 Action:  69 Distance:  tensor(1.5418, dtype=torch.float64)\n",
      "Step:  11 Action:  74 Distance:  tensor(1.4100, dtype=torch.float64)\n",
      "Step:  11 Action:  82 Distance:  tensor(1.5741, dtype=torch.float64)\n",
      "Step:  11 Action:  90 Distance:  tensor(1.2631, dtype=torch.float64)\n",
      "11 []\n",
      "Step:  12 Action:  56 Distance:  tensor(1.3080, dtype=torch.float64)\n",
      "Step:  12 Action:  61 Distance:  tensor(1.5581, dtype=torch.float64)\n",
      "Step:  12 Action:  69 Distance:  tensor(1.5947, dtype=torch.float64)\n",
      "Step:  12 Action:  74 Distance:  tensor(1.3253, dtype=torch.float64)\n",
      "Step:  12 Action:  77 Distance:  tensor(1.2564, dtype=torch.float64)\n",
      "Step:  12 Action:  82 Distance:  tensor(1.5506, dtype=torch.float64)\n",
      "Step:  12 Action:  90 Distance:  tensor(1.2967, dtype=torch.float64)\n",
      "12 []\n",
      "Step:  13 Action:  56 Distance:  tensor(1.3643, dtype=torch.float64)\n",
      "Step:  13 Action:  61 Distance:  tensor(1.5247, dtype=torch.float64)\n",
      "Step:  13 Action:  64 Distance:  tensor(1.2298, dtype=torch.float64)\n",
      "Step:  13 Action:  69 Distance:  tensor(1.6218, dtype=torch.float64)\n",
      "Step:  13 Action:  74 Distance:  tensor(1.2146, dtype=torch.float64)\n",
      "Step:  13 Action:  77 Distance:  tensor(1.3332, dtype=torch.float64)\n",
      "Step:  13 Action:  82 Distance:  tensor(1.5012, dtype=torch.float64)\n",
      "Step:  13 Action:  90 Distance:  tensor(1.3043, dtype=torch.float64)\n",
      "13 []\n",
      "Step:  14 Action:  56 Distance:  tensor(1.3236, dtype=torch.float64)\n",
      "Step:  14 Action:  61 Distance:  tensor(1.4642, dtype=torch.float64)\n",
      "Step:  14 Action:  64 Distance:  tensor(1.2667, dtype=torch.float64)\n",
      "Step:  14 Action:  69 Distance:  tensor(1.6333, dtype=torch.float64)\n",
      "Step:  14 Action:  77 Distance:  tensor(1.4185, dtype=torch.float64)\n",
      "Step:  14 Action:  82 Distance:  tensor(1.5103, dtype=torch.float64)\n",
      "Step:  14 Action:  90 Distance:  tensor(1.3923, dtype=torch.float64)\n",
      "14 []\n",
      "Step:  15 Action:  56 Distance:  tensor(1.2585, dtype=torch.float64)\n",
      "Step:  15 Action:  61 Distance:  tensor(1.3793, dtype=torch.float64)\n",
      "Step:  15 Action:  64 Distance:  tensor(1.2792, dtype=torch.float64)\n",
      "Step:  15 Action:  69 Distance:  tensor(1.6204, dtype=torch.float64)\n",
      "Step:  15 Action:  77 Distance:  tensor(1.4794, dtype=torch.float64)\n",
      "Step:  15 Action:  82 Distance:  tensor(1.4951, dtype=torch.float64)\n",
      "Step:  15 Action:  90 Distance:  tensor(1.4558, dtype=torch.float64)\n",
      "15 []\n",
      "Step:  16 Action:  61 Distance:  tensor(1.2885, dtype=torch.float64)\n",
      "Step:  16 Action:  64 Distance:  tensor(1.2906, dtype=torch.float64)\n",
      "Step:  16 Action:  69 Distance:  tensor(1.6013, dtype=torch.float64)\n",
      "Step:  16 Action:  77 Distance:  tensor(1.5375, dtype=torch.float64)\n",
      "Step:  16 Action:  82 Distance:  tensor(1.4737, dtype=torch.float64)\n",
      "Step:  16 Action:  90 Distance:  tensor(1.5160, dtype=torch.float64)\n",
      "16 []\n",
      "Step:  17 Action:  64 Distance:  tensor(1.2763, dtype=torch.float64)\n",
      "Step:  17 Action:  69 Distance:  tensor(1.5565, dtype=torch.float64)\n",
      "Step:  17 Action:  77 Distance:  tensor(1.5700, dtype=torch.float64)\n",
      "Step:  17 Action:  82 Distance:  tensor(1.4267, dtype=torch.float64)\n",
      "Step:  17 Action:  90 Distance:  tensor(1.5504, dtype=torch.float64)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step:  17 Action:  98 Distance:  tensor(1.2671, dtype=torch.float64)\n",
      "17 []\n",
      "Step:  18 Action:  64 Distance:  tensor(1.2763, dtype=torch.float64)\n",
      "Step:  18 Action:  69 Distance:  tensor(1.5565, dtype=torch.float64)\n",
      "Step:  18 Action:  77 Distance:  tensor(1.5700, dtype=torch.float64)\n",
      "Step:  18 Action:  82 Distance:  tensor(1.4267, dtype=torch.float64)\n",
      "Step:  18 Action:  90 Distance:  tensor(1.5504, dtype=torch.float64)\n",
      "Step:  18 Action:  98 Distance:  tensor(1.2671, dtype=torch.float64)\n",
      "18 []\n",
      "Step:  19 Action:  64 Distance:  tensor(1.2807, dtype=torch.float64)\n",
      "Step:  19 Action:  69 Distance:  tensor(1.5051, dtype=torch.float64)\n",
      "Step:  19 Action:  77 Distance:  tensor(1.6072, dtype=torch.float64)\n",
      "Step:  19 Action:  82 Distance:  tensor(1.3555, dtype=torch.float64)\n",
      "Step:  19 Action:  85 Distance:  tensor(1.2477, dtype=torch.float64)\n",
      "Step:  19 Action:  90 Distance:  tensor(1.5722, dtype=torch.float64)\n",
      "Step:  19 Action:  98 Distance:  tensor(1.3832, dtype=torch.float64)\n",
      "19 []\n",
      "Step:  20 Action:  44 Distance:  tensor(1.3385, dtype=torch.float64)\n",
      "Step:  20 Action:  49 Distance:  tensor(1.2286, dtype=torch.float64)\n",
      "Step:  20 Action:  69 Distance:  tensor(1.3823, dtype=torch.float64)\n",
      "Step:  20 Action:  77 Distance:  tensor(1.5818, dtype=torch.float64)\n",
      "Step:  20 Action:  82 Distance:  tensor(1.2872, dtype=torch.float64)\n",
      "Step:  20 Action:  85 Distance:  tensor(1.2530, dtype=torch.float64)\n",
      "Step:  20 Action:  90 Distance:  tensor(1.6020, dtype=torch.float64)\n",
      "Step:  20 Action:  98 Distance:  tensor(1.4867, dtype=torch.float64)\n",
      "20 []\n",
      "Step:  21 Action:  44 Distance:  tensor(1.4217, dtype=torch.float64)\n",
      "Step:  21 Action:  49 Distance:  tensor(1.2387, dtype=torch.float64)\n",
      "Step:  21 Action:  69 Distance:  tensor(1.2773, dtype=torch.float64)\n",
      "Step:  21 Action:  77 Distance:  tensor(1.5501, dtype=torch.float64)\n",
      "Step:  21 Action:  82 Distance:  tensor(1.2059, dtype=torch.float64)\n",
      "Step:  21 Action:  85 Distance:  tensor(1.2581, dtype=torch.float64)\n",
      "Step:  21 Action:  90 Distance:  tensor(1.5947, dtype=torch.float64)\n",
      "Step:  21 Action:  98 Distance:  tensor(1.5446, dtype=torch.float64)\n",
      "21 []\n",
      "Step:  22 Action:  44 Distance:  tensor(1.4126, dtype=torch.float64)\n",
      "Step:  22 Action:  69 Distance:  tensor(1.2116, dtype=torch.float64)\n",
      "Step:  22 Action:  77 Distance:  tensor(1.5488, dtype=torch.float64)\n",
      "Step:  22 Action:  85 Distance:  tensor(1.3282, dtype=torch.float64)\n",
      "Step:  22 Action:  90 Distance:  tensor(1.5470, dtype=torch.float64)\n",
      "Step:  22 Action:  98 Distance:  tensor(1.5829, dtype=torch.float64)\n",
      "22 []\n",
      "Step:  23 Action:  31 Distance:  tensor(1.2443, dtype=torch.float64)\n",
      "Step:  23 Action:  39 Distance:  tensor(1.2866, dtype=torch.float64)\n",
      "Step:  23 Action:  44 Distance:  tensor(1.3997, dtype=torch.float64)\n",
      "Step:  23 Action:  77 Distance:  tensor(1.5426, dtype=torch.float64)\n",
      "Step:  23 Action:  85 Distance:  tensor(1.3959, dtype=torch.float64)\n",
      "Step:  23 Action:  90 Distance:  tensor(1.4943, dtype=torch.float64)\n",
      "Step:  23 Action:  98 Distance:  tensor(1.6163, dtype=torch.float64)\n",
      "23 []\n",
      "Step:  24 Action:  31 Distance:  tensor(1.2934, dtype=torch.float64)\n",
      "Step:  24 Action:  39 Distance:  tensor(1.3726, dtype=torch.float64)\n",
      "Step:  24 Action:  44 Distance:  tensor(1.3641, dtype=torch.float64)\n",
      "Step:  24 Action:  77 Distance:  tensor(1.5137, dtype=torch.float64)\n",
      "Step:  24 Action:  85 Distance:  tensor(1.4408, dtype=torch.float64)\n",
      "Step:  24 Action:  90 Distance:  tensor(1.4189, dtype=torch.float64)\n",
      "Step:  24 Action:  98 Distance:  tensor(1.6270, dtype=torch.float64)\n",
      "24 []\n",
      "Step:  25 Action:  31 Distance:  tensor(1.3955, dtype=torch.float64)\n",
      "Step:  25 Action:  39 Distance:  tensor(1.4232, dtype=torch.float64)\n",
      "Step:  25 Action:  44 Distance:  tensor(1.4189, dtype=torch.float64)\n",
      "Step:  25 Action:  77 Distance:  tensor(1.4476, dtype=torch.float64)\n",
      "Step:  25 Action:  85 Distance:  tensor(1.4018, dtype=torch.float64)\n",
      "Step:  25 Action:  90 Distance:  tensor(1.3865, dtype=torch.float64)\n",
      "Step:  25 Action:  98 Distance:  tensor(1.6444, dtype=torch.float64)\n",
      "25 []\n",
      "Step:  26 Action:  23 Distance:  tensor(1.2010, dtype=torch.float64)\n",
      "Step:  26 Action:  31 Distance:  tensor(1.5438, dtype=torch.float64)\n",
      "Step:  26 Action:  39 Distance:  tensor(1.4724, dtype=torch.float64)\n",
      "Step:  26 Action:  44 Distance:  tensor(1.4670, dtype=torch.float64)\n",
      "Step:  26 Action:  77 Distance:  tensor(1.2564, dtype=torch.float64)\n",
      "Step:  26 Action:  85 Distance:  tensor(1.2705, dtype=torch.float64)\n",
      "Step:  26 Action:  90 Distance:  tensor(1.2592, dtype=torch.float64)\n",
      "Step:  26 Action:  98 Distance:  tensor(1.6194, dtype=torch.float64)\n",
      "26 []\n",
      "Step:  27 Action:  23 Distance:  tensor(1.2617, dtype=torch.float64)\n",
      "Step:  27 Action:  26 Distance:  tensor(1.3512, dtype=torch.float64)\n",
      "Step:  27 Action:  31 Distance:  tensor(1.6077, dtype=torch.float64)\n",
      "Step:  27 Action:  39 Distance:  tensor(1.5253, dtype=torch.float64)\n",
      "Step:  27 Action:  44 Distance:  tensor(1.3890, dtype=torch.float64)\n",
      "Step:  27 Action:  98 Distance:  tensor(1.5487, dtype=torch.float64)\n",
      "27 []\n",
      "Step:  28 Action:  18 Distance:  tensor(1.2351, dtype=torch.float64)\n",
      "Step:  28 Action:  23 Distance:  tensor(1.2164, dtype=torch.float64)\n",
      "Step:  28 Action:  26 Distance:  tensor(1.4285, dtype=torch.float64)\n",
      "Step:  28 Action:  31 Distance:  tensor(1.6014, dtype=torch.float64)\n",
      "Step:  28 Action:  39 Distance:  tensor(1.5558, dtype=torch.float64)\n",
      "Step:  28 Action:  44 Distance:  tensor(1.2939, dtype=torch.float64)\n",
      "Step:  28 Action:  98 Distance:  tensor(1.4966, dtype=torch.float64)\n",
      "28 []\n",
      "Step:  29 Action:  18 Distance:  tensor(1.3395, dtype=torch.float64)\n",
      "Step:  29 Action:  23 Distance:  tensor(1.2725, dtype=torch.float64)\n",
      "Step:  29 Action:  26 Distance:  tensor(1.4920, dtype=torch.float64)\n",
      "Step:  29 Action:  31 Distance:  tensor(1.6207, dtype=torch.float64)\n",
      "Step:  29 Action:  39 Distance:  tensor(1.5335, dtype=torch.float64)\n",
      "Step:  29 Action:  44 Distance:  tensor(1.2629, dtype=torch.float64)\n",
      "Step:  29 Action:  98 Distance:  tensor(1.4319, dtype=torch.float64)\n",
      "29 []\n",
      "Step:  30 Action:  18 Distance:  tensor(1.4184, dtype=torch.float64)\n",
      "Step:  30 Action:  23 Distance:  tensor(1.3032, dtype=torch.float64)\n",
      "Step:  30 Action:  26 Distance:  tensor(1.5302, dtype=torch.float64)\n",
      "Step:  30 Action:  31 Distance:  tensor(1.6147, dtype=torch.float64)\n",
      "Step:  30 Action:  39 Distance:  tensor(1.4859, dtype=torch.float64)\n",
      "Step:  30 Action:  44 Distance:  tensor(1.2065, dtype=torch.float64)\n",
      "Step:  30 Action:  98 Distance:  tensor(1.3418, dtype=torch.float64)\n",
      "30 []\n",
      "Step:  31 Action:  13 Distance:  tensor(1.2379, dtype=torch.float64)\n",
      "Step:  31 Action:  18 Distance:  tensor(1.4916, dtype=torch.float64)\n",
      "Step:  31 Action:  23 Distance:  tensor(1.3284, dtype=torch.float64)\n",
      "Step:  31 Action:  26 Distance:  tensor(1.5638, dtype=torch.float64)\n",
      "Step:  31 Action:  31 Distance:  tensor(1.6021, dtype=torch.float64)\n",
      "Step:  31 Action:  39 Distance:  tensor(1.4352, dtype=torch.float64)\n",
      "Step:  31 Action:  98 Distance:  tensor(1.2489, dtype=torch.float64)\n",
      "31 []\n",
      "Step:  32 Action:  0 Distance:  tensor(1.2495, dtype=torch.float64)\n",
      "Step:  32 Action:  13 Distance:  tensor(1.4117, dtype=torch.float64)\n",
      "Step:  32 Action:  18 Distance:  tensor(1.5820, dtype=torch.float64)\n",
      "Step:  32 Action:  23 Distance:  tensor(1.3226, dtype=torch.float64)\n",
      "Step:  32 Action:  26 Distance:  tensor(1.5775, dtype=torch.float64)\n",
      "Step:  32 Action:  31 Distance:  tensor(1.5211, dtype=torch.float64)\n",
      "Step:  32 Action:  39 Distance:  tensor(1.2828, dtype=torch.float64)\n",
      "32 []\n",
      "Step:  33 Action:  0 Distance:  tensor(1.4203, dtype=torch.float64)\n",
      "Step:  33 Action:  5 Distance:  tensor(1.3761, dtype=torch.float64)\n",
      "Step:  33 Action:  10 Distance:  tensor(1.2212, dtype=torch.float64)\n",
      "Step:  33 Action:  13 Distance:  tensor(1.5318, dtype=torch.float64)\n",
      "Step:  33 Action:  18 Distance:  tensor(1.6109, dtype=torch.float64)\n",
      "Step:  33 Action:  23 Distance:  tensor(1.2563, dtype=torch.float64)\n",
      "Step:  33 Action:  26 Distance:  tensor(1.5325, dtype=torch.float64)\n",
      "Step:  33 Action:  31 Distance:  tensor(1.3783, dtype=torch.float64)\n",
      "33 []\n",
      "Step:  34 Action:  0 Distance:  tensor(1.5741, dtype=torch.float64)\n",
      "Step:  34 Action:  5 Distance:  tensor(1.4806, dtype=torch.float64)\n",
      "Step:  34 Action:  8 Distance:  tensor(1.2429, dtype=torch.float64)\n",
      "Step:  34 Action:  10 Distance:  tensor(1.2187, dtype=torch.float64)\n",
      "Step:  34 Action:  13 Distance:  tensor(1.6071, dtype=torch.float64)\n",
      "Step:  34 Action:  18 Distance:  tensor(1.5614, dtype=torch.float64)\n",
      "Step:  34 Action:  21 Distance:  tensor(1.2155, dtype=torch.float64)\n",
      "Step:  34 Action:  26 Distance:  tensor(1.4623, dtype=torch.float64)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step:  34 Action:  34 Distance:  tensor(1.2363, dtype=torch.float64)\n",
      "34 []\n",
      "Step:  35 Action:  0 Distance:  tensor(1.6504, dtype=torch.float64)\n",
      "Step:  35 Action:  5 Distance:  tensor(1.4896, dtype=torch.float64)\n",
      "Step:  35 Action:  8 Distance:  tensor(1.3297, dtype=torch.float64)\n",
      "Step:  35 Action:  13 Distance:  tensor(1.6255, dtype=torch.float64)\n",
      "Step:  35 Action:  18 Distance:  tensor(1.5023, dtype=torch.float64)\n",
      "Step:  35 Action:  21 Distance:  tensor(1.3069, dtype=torch.float64)\n",
      "Step:  35 Action:  26 Distance:  tensor(1.4191, dtype=torch.float64)\n",
      "Step:  35 Action:  34 Distance:  tensor(1.2754, dtype=torch.float64)\n",
      "35 []\n",
      "Step:  36 Action:  0 Distance:  tensor(1.6504, dtype=torch.float64)\n",
      "Step:  36 Action:  5 Distance:  tensor(1.4574, dtype=torch.float64)\n",
      "Step:  36 Action:  8 Distance:  tensor(1.3736, dtype=torch.float64)\n",
      "Step:  36 Action:  13 Distance:  tensor(1.6424, dtype=torch.float64)\n",
      "Step:  36 Action:  18 Distance:  tensor(1.4433, dtype=torch.float64)\n",
      "Step:  36 Action:  21 Distance:  tensor(1.3961, dtype=torch.float64)\n",
      "Step:  36 Action:  26 Distance:  tensor(1.4171, dtype=torch.float64)\n",
      "Step:  36 Action:  34 Distance:  tensor(1.3541, dtype=torch.float64)\n",
      "36 []\n",
      "Step:  37 Action:  0 Distance:  tensor(1.6297, dtype=torch.float64)\n",
      "Step:  37 Action:  5 Distance:  tensor(1.3586, dtype=torch.float64)\n",
      "Step:  37 Action:  8 Distance:  tensor(1.3157, dtype=torch.float64)\n",
      "Step:  37 Action:  13 Distance:  tensor(1.6391, dtype=torch.float64)\n",
      "Step:  37 Action:  18 Distance:  tensor(1.3957, dtype=torch.float64)\n",
      "Step:  37 Action:  21 Distance:  tensor(1.4311, dtype=torch.float64)\n",
      "Step:  37 Action:  26 Distance:  tensor(1.4701, dtype=torch.float64)\n",
      "Step:  37 Action:  34 Distance:  tensor(1.4513, dtype=torch.float64)\n",
      "37 []\n",
      "Step:  38 Action:  0 Distance:  tensor(1.5064, dtype=torch.float64)\n",
      "Step:  38 Action:  5 Distance:  tensor(1.2675, dtype=torch.float64)\n",
      "Step:  38 Action:  8 Distance:  tensor(1.2816, dtype=torch.float64)\n",
      "Step:  38 Action:  13 Distance:  tensor(1.6227, dtype=torch.float64)\n",
      "Step:  38 Action:  18 Distance:  tensor(1.3224, dtype=torch.float64)\n",
      "Step:  38 Action:  21 Distance:  tensor(1.4697, dtype=torch.float64)\n",
      "Step:  38 Action:  26 Distance:  tensor(1.4794, dtype=torch.float64)\n",
      "Step:  38 Action:  34 Distance:  tensor(1.5213, dtype=torch.float64)\n",
      "Step:  38 Action:  47 Distance:  tensor(1.2133, dtype=torch.float64)\n",
      "38 []\n",
      "Step:  39 Action:  0 Distance:  tensor(1.5209, dtype=torch.float64)\n",
      "Step:  39 Action:  5 Distance:  tensor(1.2129, dtype=torch.float64)\n",
      "Step:  39 Action:  8 Distance:  tensor(1.3191, dtype=torch.float64)\n",
      "Step:  39 Action:  13 Distance:  tensor(1.5965, dtype=torch.float64)\n",
      "Step:  39 Action:  18 Distance:  tensor(1.2075, dtype=torch.float64)\n",
      "Step:  39 Action:  21 Distance:  tensor(1.5323, dtype=torch.float64)\n",
      "Step:  39 Action:  26 Distance:  tensor(1.4036, dtype=torch.float64)\n",
      "Step:  39 Action:  34 Distance:  tensor(1.5428, dtype=torch.float64)\n",
      "39 []\n",
      "Step:  40 Action:  0 Distance:  tensor(1.5209, dtype=torch.float64)\n",
      "Step:  40 Action:  5 Distance:  tensor(1.2129, dtype=torch.float64)\n",
      "Step:  40 Action:  8 Distance:  tensor(1.3191, dtype=torch.float64)\n",
      "Step:  40 Action:  13 Distance:  tensor(1.5965, dtype=torch.float64)\n",
      "Step:  40 Action:  18 Distance:  tensor(1.2075, dtype=torch.float64)\n",
      "Step:  40 Action:  21 Distance:  tensor(1.5323, dtype=torch.float64)\n",
      "Step:  40 Action:  26 Distance:  tensor(1.4036, dtype=torch.float64)\n",
      "Step:  40 Action:  34 Distance:  tensor(1.5428, dtype=torch.float64)\n",
      "40 []\n",
      "Step:  41 Action:  0 Distance:  tensor(1.5209, dtype=torch.float64)\n",
      "Step:  41 Action:  5 Distance:  tensor(1.2129, dtype=torch.float64)\n",
      "Step:  41 Action:  8 Distance:  tensor(1.3191, dtype=torch.float64)\n",
      "Step:  41 Action:  13 Distance:  tensor(1.5965, dtype=torch.float64)\n",
      "Step:  41 Action:  18 Distance:  tensor(1.2075, dtype=torch.float64)\n",
      "Step:  41 Action:  21 Distance:  tensor(1.5323, dtype=torch.float64)\n",
      "Step:  41 Action:  26 Distance:  tensor(1.4036, dtype=torch.float64)\n",
      "Step:  41 Action:  34 Distance:  tensor(1.5428, dtype=torch.float64)\n",
      "41 []\n",
      "Step:  42 Action:  0 Distance:  tensor(1.5209, dtype=torch.float64)\n",
      "Step:  42 Action:  5 Distance:  tensor(1.2129, dtype=torch.float64)\n",
      "Step:  42 Action:  8 Distance:  tensor(1.3191, dtype=torch.float64)\n",
      "Step:  42 Action:  13 Distance:  tensor(1.5965, dtype=torch.float64)\n",
      "Step:  42 Action:  18 Distance:  tensor(1.2075, dtype=torch.float64)\n",
      "Step:  42 Action:  21 Distance:  tensor(1.5323, dtype=torch.float64)\n",
      "Step:  42 Action:  26 Distance:  tensor(1.4036, dtype=torch.float64)\n",
      "Step:  42 Action:  34 Distance:  tensor(1.5428, dtype=torch.float64)\n",
      "42 []\n",
      "Step:  43 Action:  0 Distance:  tensor(1.5209, dtype=torch.float64)\n",
      "Step:  43 Action:  5 Distance:  tensor(1.2129, dtype=torch.float64)\n",
      "Step:  43 Action:  8 Distance:  tensor(1.3191, dtype=torch.float64)\n",
      "Step:  43 Action:  13 Distance:  tensor(1.5965, dtype=torch.float64)\n",
      "Step:  43 Action:  18 Distance:  tensor(1.2075, dtype=torch.float64)\n",
      "Step:  43 Action:  21 Distance:  tensor(1.5323, dtype=torch.float64)\n",
      "Step:  43 Action:  26 Distance:  tensor(1.4036, dtype=torch.float64)\n",
      "Step:  43 Action:  34 Distance:  tensor(1.5428, dtype=torch.float64)\n",
      "43 []\n",
      "Step:  44 Action:  0 Distance:  tensor(1.5209, dtype=torch.float64)\n",
      "Step:  44 Action:  5 Distance:  tensor(1.2129, dtype=torch.float64)\n",
      "Step:  44 Action:  8 Distance:  tensor(1.3191, dtype=torch.float64)\n",
      "Step:  44 Action:  13 Distance:  tensor(1.5965, dtype=torch.float64)\n",
      "Step:  44 Action:  18 Distance:  tensor(1.2075, dtype=torch.float64)\n",
      "Step:  44 Action:  21 Distance:  tensor(1.5323, dtype=torch.float64)\n",
      "Step:  44 Action:  26 Distance:  tensor(1.4036, dtype=torch.float64)\n",
      "Step:  44 Action:  34 Distance:  tensor(1.5428, dtype=torch.float64)\n",
      "44 []\n",
      "Step:  45 Action:  0 Distance:  tensor(1.5209, dtype=torch.float64)\n",
      "Step:  45 Action:  5 Distance:  tensor(1.2129, dtype=torch.float64)\n",
      "Step:  45 Action:  8 Distance:  tensor(1.3191, dtype=torch.float64)\n",
      "Step:  45 Action:  13 Distance:  tensor(1.5965, dtype=torch.float64)\n",
      "Step:  45 Action:  18 Distance:  tensor(1.2075, dtype=torch.float64)\n",
      "Step:  45 Action:  21 Distance:  tensor(1.5323, dtype=torch.float64)\n",
      "Step:  45 Action:  26 Distance:  tensor(1.4036, dtype=torch.float64)\n",
      "Step:  45 Action:  34 Distance:  tensor(1.5428, dtype=torch.float64)\n",
      "45 []\n",
      "Step:  46 Action:  0 Distance:  tensor(1.5209, dtype=torch.float64)\n",
      "Step:  46 Action:  5 Distance:  tensor(1.2129, dtype=torch.float64)\n",
      "Step:  46 Action:  8 Distance:  tensor(1.3191, dtype=torch.float64)\n",
      "Step:  46 Action:  13 Distance:  tensor(1.5965, dtype=torch.float64)\n",
      "Step:  46 Action:  18 Distance:  tensor(1.2075, dtype=torch.float64)\n",
      "Step:  46 Action:  21 Distance:  tensor(1.5323, dtype=torch.float64)\n",
      "Step:  46 Action:  26 Distance:  tensor(1.4036, dtype=torch.float64)\n",
      "Step:  46 Action:  34 Distance:  tensor(1.5428, dtype=torch.float64)\n",
      "46 []\n",
      "Step:  47 Action:  0 Distance:  tensor(1.5209, dtype=torch.float64)\n",
      "Step:  47 Action:  8 Distance:  tensor(1.2639, dtype=torch.float64)\n",
      "Step:  47 Action:  13 Distance:  tensor(1.5583, dtype=torch.float64)\n",
      "Step:  47 Action:  21 Distance:  tensor(1.5480, dtype=torch.float64)\n",
      "Step:  47 Action:  26 Distance:  tensor(1.3944, dtype=torch.float64)\n",
      "Step:  47 Action:  34 Distance:  tensor(1.5913, dtype=torch.float64)\n",
      "Step:  47 Action:  42 Distance:  tensor(1.2956, dtype=torch.float64)\n",
      "Step:  47 Action:  47 Distance:  tensor(1.2660, dtype=torch.float64)\n",
      "47 []\n",
      "Step:  48 Action:  0 Distance:  tensor(1.3885, dtype=torch.float64)\n",
      "Step:  48 Action:  8 Distance:  tensor(1.2964, dtype=torch.float64)\n",
      "Step:  48 Action:  13 Distance:  tensor(1.5198, dtype=torch.float64)\n",
      "Step:  48 Action:  21 Distance:  tensor(1.5907, dtype=torch.float64)\n",
      "Step:  48 Action:  26 Distance:  tensor(1.3027, dtype=torch.float64)\n",
      "Step:  48 Action:  29 Distance:  tensor(1.2582, dtype=torch.float64)\n",
      "Step:  48 Action:  34 Distance:  tensor(1.5862, dtype=torch.float64)\n",
      "Step:  48 Action:  42 Distance:  tensor(1.3674, dtype=torch.float64)\n",
      "Step:  48 Action:  47 Distance:  tensor(1.2157, dtype=torch.float64)\n",
      "48 []\n",
      "Step:  49 Action:  0 Distance:  tensor(1.5321, dtype=torch.float64)\n",
      "Step:  49 Action:  8 Distance:  tensor(1.3821, dtype=torch.float64)\n",
      "Step:  49 Action:  13 Distance:  tensor(1.5176, dtype=torch.float64)\n",
      "Step:  49 Action:  21 Distance:  tensor(1.6158, dtype=torch.float64)\n",
      "Step:  49 Action:  26 Distance:  tensor(1.2182, dtype=torch.float64)\n",
      "Step:  49 Action:  29 Distance:  tensor(1.3106, dtype=torch.float64)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step:  49 Action:  34 Distance:  tensor(1.5309, dtype=torch.float64)\n",
      "Step:  49 Action:  42 Distance:  tensor(1.3322, dtype=torch.float64)\n",
      "49 []\n",
      "Step:  50 Action:  0 Distance:  tensor(1.5322, dtype=torch.float64)\n",
      "Step:  50 Action:  8 Distance:  tensor(1.3821, dtype=torch.float64)\n",
      "Step:  50 Action:  13 Distance:  tensor(1.5176, dtype=torch.float64)\n",
      "Step:  50 Action:  21 Distance:  tensor(1.6158, dtype=torch.float64)\n",
      "Step:  50 Action:  26 Distance:  tensor(1.2182, dtype=torch.float64)\n",
      "Step:  50 Action:  29 Distance:  tensor(1.3106, dtype=torch.float64)\n",
      "Step:  50 Action:  34 Distance:  tensor(1.5309, dtype=torch.float64)\n",
      "Step:  50 Action:  42 Distance:  tensor(1.3322, dtype=torch.float64)\n",
      "50 []\n",
      "Step:  51 Action:  0 Distance:  tensor(1.5321, dtype=torch.float64)\n",
      "Step:  51 Action:  5 Distance:  tensor(1.2367, dtype=torch.float64)\n",
      "Step:  51 Action:  8 Distance:  tensor(1.4490, dtype=torch.float64)\n",
      "Step:  51 Action:  13 Distance:  tensor(1.5675, dtype=torch.float64)\n",
      "Step:  51 Action:  21 Distance:  tensor(1.6079, dtype=torch.float64)\n",
      "Step:  51 Action:  26 Distance:  tensor(1.2407, dtype=torch.float64)\n",
      "Step:  51 Action:  29 Distance:  tensor(1.2570, dtype=torch.float64)\n",
      "Step:  51 Action:  34 Distance:  tensor(1.4905, dtype=torch.float64)\n",
      "Step:  51 Action:  42 Distance:  tensor(1.2329, dtype=torch.float64)\n",
      "51 []\n",
      "Step:  52 Action:  0 Distance:  tensor(1.6565, dtype=torch.float64)\n",
      "Step:  52 Action:  5 Distance:  tensor(1.3360, dtype=torch.float64)\n",
      "Step:  52 Action:  8 Distance:  tensor(1.4893, dtype=torch.float64)\n",
      "Step:  52 Action:  13 Distance:  tensor(1.5909, dtype=torch.float64)\n",
      "Step:  52 Action:  21 Distance:  tensor(1.5736, dtype=torch.float64)\n",
      "Step:  52 Action:  26 Distance:  tensor(1.2366, dtype=torch.float64)\n",
      "Step:  52 Action:  34 Distance:  tensor(1.4237, dtype=torch.float64)\n",
      "52 []\n",
      "Step:  53 Action:  0 Distance:  tensor(1.6565, dtype=torch.float64)\n",
      "Step:  53 Action:  5 Distance:  tensor(1.3360, dtype=torch.float64)\n",
      "Step:  53 Action:  8 Distance:  tensor(1.4893, dtype=torch.float64)\n",
      "Step:  53 Action:  13 Distance:  tensor(1.5909, dtype=torch.float64)\n",
      "Step:  53 Action:  21 Distance:  tensor(1.5736, dtype=torch.float64)\n",
      "Step:  53 Action:  26 Distance:  tensor(1.2366, dtype=torch.float64)\n",
      "Step:  53 Action:  34 Distance:  tensor(1.4237, dtype=torch.float64)\n",
      "53 []\n",
      "Step:  54 Action:  0 Distance:  tensor(1.6565, dtype=torch.float64)\n",
      "Step:  54 Action:  5 Distance:  tensor(1.4373, dtype=torch.float64)\n",
      "Step:  54 Action:  8 Distance:  tensor(1.5157, dtype=torch.float64)\n",
      "Step:  54 Action:  13 Distance:  tensor(1.6149, dtype=torch.float64)\n",
      "Step:  54 Action:  18 Distance:  tensor(1.2637, dtype=torch.float64)\n",
      "Step:  54 Action:  21 Distance:  tensor(1.5260, dtype=torch.float64)\n",
      "Step:  54 Action:  26 Distance:  tensor(1.2495, dtype=torch.float64)\n",
      "Step:  54 Action:  34 Distance:  tensor(1.3568, dtype=torch.float64)\n",
      "54 []\n",
      "Step:  55 Action:  0 Distance:  tensor(1.7441, dtype=torch.float64)\n",
      "Step:  55 Action:  5 Distance:  tensor(1.5640, dtype=torch.float64)\n",
      "Step:  55 Action:  8 Distance:  tensor(1.5016, dtype=torch.float64)\n",
      "Step:  55 Action:  13 Distance:  tensor(1.5992, dtype=torch.float64)\n",
      "Step:  55 Action:  18 Distance:  tensor(1.3915, dtype=torch.float64)\n",
      "Step:  55 Action:  21 Distance:  tensor(1.3772, dtype=torch.float64)\n",
      "Step:  55 Action:  26 Distance:  tensor(1.2197, dtype=torch.float64)\n",
      "55 []\n",
      "Step:  56 Action:  0 Distance:  tensor(1.7894, dtype=torch.float64)\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 58 is out of bounds for dimension 0 with size 58",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-a7dce26ea5fa>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mz\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_actions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTractographyState\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreferenceLine\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minterpolateDWIatState\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m         \u001b[0mnext_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m         \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstepCounter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0;31m#if reward == -1:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/fibre_tracking/deepFibreTracking/dfibert/envs/RLtractEnvironment.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    138\u001b[0m         \u001b[0;31m#else:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m         \u001b[0;31m#    rewardNextState = rewardDistance\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 140\u001b[0;31m         \u001b[0mrewardNextState\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrewardForState\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnextState\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    141\u001b[0m         \u001b[0;31m#if rewardNextState < 0.:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m         \u001b[0;31m#    rewardNextState = -1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/fibre_tracking/deepFibreTracking/dfibert/envs/RLtractEnvironment.py\u001b[0m in \u001b[0;36mrewardForState\u001b[0;34m(self, state)\u001b[0m\n\u001b[1;32m    160\u001b[0m         \u001b[0mqry_pt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetCoordinate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    161\u001b[0m         \u001b[0;31m#distance = torch.min(torch.sum( (self.referenceStreamline_ijk[np.max([self.stepCounter-1-2,0]):np.min([self.stepCounter-1+1,self.maxSteps])] - qry_pt)**2, dim =1 ))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 162\u001b[0;31m         \u001b[0mdistance\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreferenceStreamline_ijk\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstepCounter\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m86\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mqry_pt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    163\u001b[0m         \u001b[0;31m#reward = torch.tanh(-distance+5.3)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdistance\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: index 58 is out of bounds for dimension 0 with size 58"
     ]
    }
   ],
   "source": [
    "possible_actions = []\n",
    "past_state = env.reset()\n",
    "all_next_states = []\n",
    "for i in range(len(referenceLine)):\n",
    "    best_actions = []\n",
    "    next_states = []\n",
    "    for z in range(n_actions):\n",
    "        env.state = TractographyState(referenceLine[i], env.interpolateDWIatState)\n",
    "        next_state, reward, _ = env.step(z)\n",
    "        env.stepCounter = i\n",
    "        #if reward == -1:\n",
    "        #    reward = 0\n",
    "        #elif reward < 0.2:\n",
    "        if reward > 1.0:\n",
    "            print(\"Step: \", i, \"Action: \", z, \"Distance: \", reward)\n",
    "        #    reward = 1\n",
    "        #elif reward < 1.:\n",
    "        #    reward = 0\n",
    "        #else:\n",
    "        #    reward = -1\n",
    "        #if reward == 1:\n",
    "        #    best_actions.append(z)\n",
    "            #print(i, z, referenceLine[i].numpy(), next_state.getCoordinate().numpy(), reward)\n",
    "    print(i, best_actions)\n",
    "    #print(i, reward)\n",
    "    #if reward > 0.9:\n",
    "    #    best_actions.append(i)\n",
    "    possible_actions.append(best_actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "step_distance = []\n",
    "optimal_steps = []#[100, 80, 75, 80, 75, 100, 62, 75, 83, 75, 83, 100, 83, 62, 67, 51, 67, 100, 59, 59, 59, 100, 59, 56, 51, 56, 66, 100, 66, 71, 71, 79, 58, 100, 71, 71, 84, 71, 100, 92, 84, 92, 97, 100, 97, 38, 97, 43, 38, 100, 43, 43, 89, 48]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "43\n"
     ]
    }
   ],
   "source": [
    "last_state = env.reset()\n",
    "print(len(env.referenceStreamline_ijk))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hey, hey, hey we finally stopped at the terminal state! :D\n",
      "Hey, hey, hey we finally stopped at the terminal state! :D\n",
      "Hey, hey, hey we finally stopped at the terminal state! :D\n",
      "Hey, hey, hey we finally stopped at the terminal state! :D\n",
      "Hey, hey, hey we finally stopped at the terminal state! :D\n",
      "Hey, hey, hey we finally stopped at the terminal state! :D\n",
      "Hey, hey, hey we finally stopped at the terminal state! :D\n",
      "Hey, hey, hey we finally stopped at the terminal state! :D\n",
      "Hey, hey, hey we finally stopped at the terminal state! :D\n",
      "Hey, hey, hey we finally stopped at the terminal state! :D\n",
      "Hey, hey, hey we finally stopped at the terminal state! :D\n",
      "Hey, hey, hey we finally stopped at the terminal state! :D\n",
      "Hey, hey, hey we finally stopped at the terminal state! :D\n",
      "Hey, hey, hey we finally stopped at the terminal state! :D\n",
      "Hey, hey, hey we finally stopped at the terminal state! :D\n",
      "Hey, hey, hey we finally stopped at the terminal state! :D\n",
      "Hey, hey, hey we finally stopped at the terminal state! :D\n",
      "Hey, hey, hey we finally stopped at the terminal state! :D\n",
      "Hey, hey, hey we finally stopped at the terminal state! :D\n",
      "Hey, hey, hey we finally stopped at the terminal state! :D\n",
      "Hey, hey, hey we finally stopped at the terminal state! :D\n",
      "Hey, hey, hey we finally stopped at the terminal state! :D\n",
      "Hey, hey, hey we finally stopped at the terminal state! :D\n",
      "Hey, hey, hey we finally stopped at the terminal state! :D\n",
      "Hey, hey, hey we finally stopped at the terminal state! :D\n",
      "Hey, hey, hey we finally stopped at the terminal state! :D\n",
      "Hey, hey, hey we finally stopped at the terminal state! :D\n",
      "Hey, hey, hey we finally stopped at the terminal state! :D\n",
      "Hey, hey, hey we finally stopped at the terminal state! :D\n",
      "Hey, hey, hey we finally stopped at the terminal state! :D\n",
      "Hey, hey, hey we finally stopped at the terminal state! :D\n",
      "Hey, hey, hey we finally stopped at the terminal state! :D\n",
      "Hey, hey, hey we finally stopped at the terminal state! :D\n",
      "Hey, hey, hey we finally stopped at the terminal state! :D\n",
      "Hey, hey, hey we finally stopped at the terminal state! :D\n",
      "Hey, hey, hey we finally stopped at the terminal state! :D\n",
      "Hey, hey, hey we finally stopped at the terminal state! :D\n",
      "Hey, hey, hey we finally stopped at the terminal state! :D\n",
      "Hey, hey, hey we finally stopped at the terminal state! :D\n",
      "Hey, hey, hey we finally stopped at the terminal state! :D\n",
      "Hey, hey, hey we finally stopped at the terminal state! :D\n",
      "Hey, hey, hey we finally stopped at the terminal state! :D\n",
      "Hey, hey, hey we finally stopped at the terminal state! :D\n",
      "Hey, hey, hey we finally stopped at the terminal state! :D\n",
      "Hey, hey, hey we finally stopped at the terminal state! :D\n",
      "Hey, hey, hey we finally stopped at the terminal state! :D\n",
      "Hey, hey, hey we finally stopped at the terminal state! :D\n",
      "Hey, hey, hey we finally stopped at the terminal state! :D\n",
      "Hey, hey, hey we finally stopped at the terminal state! :D\n",
      "Hey, hey, hey we finally stopped at the terminal state! :D\n",
      "Hey, hey, hey we finally stopped at the terminal state! :D\n",
      "Hey, hey, hey we finally stopped at the terminal state! :D\n",
      "Hey, hey, hey we finally stopped at the terminal state! :D\n",
      "Hey, hey, hey we finally stopped at the terminal state! :D\n",
      "Hey, hey, hey we finally stopped at the terminal state! :D\n",
      "Hey, hey, hey we finally stopped at the terminal state! :D\n",
      "Hey, hey, hey we finally stopped at the terminal state! :D\n",
      "Hey, hey, hey we finally stopped at the terminal state! :D\n",
      "Hey, hey, hey we finally stopped at the terminal state! :D\n",
      "Hey, hey, hey we finally stopped at the terminal state! :D\n",
      "Hey, hey, hey we finally stopped at the terminal state! :D\n",
      "Hey, hey, hey we finally stopped at the terminal state! :D\n",
      "Hey, hey, hey we finally stopped at the terminal state! :D\n",
      "Hey, hey, hey we finally stopped at the terminal state! :D\n",
      "Hey, hey, hey we finally stopped at the terminal state! :D\n",
      "Hey, hey, hey we finally stopped at the terminal state! :D\n",
      "Hey, hey, hey we finally stopped at the terminal state! :D\n",
      "Hey, hey, hey we finally stopped at the terminal state! :D\n",
      "Hey, hey, hey we finally stopped at the terminal state! :D\n",
      "Hey, hey, hey we finally stopped at the terminal state! :D\n",
      "Hey, hey, hey we finally stopped at the terminal state! :D\n",
      "Hey, hey, hey we finally stopped at the terminal state! :D\n",
      "Hey, hey, hey we finally stopped at the terminal state! :D\n",
      "Hey, hey, hey we finally stopped at the terminal state! :D\n",
      "Hey, hey, hey we finally stopped at the terminal state! :D\n",
      "Hey, hey, hey we finally stopped at the terminal state! :D\n",
      "Point outside of brain mask :(\n",
      "Point outside of brain mask :(\n",
      "Hey, hey, hey we finally stopped at the terminal state! :D\n",
      "Hey, hey, hey we finally stopped at the terminal state! :D\n",
      "Hey, hey, hey we finally stopped at the terminal state! :D\n",
      "Hey, hey, hey we finally stopped at the terminal state! :D\n",
      "Hey, hey, hey we finally stopped at the terminal state! :D\n",
      "Hey, hey, hey we finally stopped at the terminal state! :D\n",
      "Hey, hey, hey we finally stopped at the terminal state! :D\n",
      "Hey, hey, hey we finally stopped at the terminal state! :D\n",
      "Hey, hey, hey we finally stopped at the terminal state! :D\n",
      "Hey, hey, hey we finally stopped at the terminal state! :D\n",
      "Hey, hey, hey we finally stopped at the terminal state! :D\n",
      "Hey, hey, hey we finally stopped at the terminal state! :D\n",
      "Hey, hey, hey we finally stopped at the terminal state! :D\n",
      "Hey, hey, hey we finally stopped at the terminal state! :D\n",
      "Hey, hey, hey we finally stopped at the terminal state! :D\n",
      "Hey, hey, hey we finally stopped at the terminal state! :D\n",
      "Hey, hey, hey we finally stopped at the terminal state! :D\n",
      "Hey, hey, hey we finally stopped at the terminal state! :D\n",
      "Hey, hey, hey we finally stopped at the terminal state! :D\n",
      "Hey, hey, hey we finally stopped at the terminal state! :D\n",
      "Hey, hey, hey we finally stopped at the terminal state! :D\n",
      "Hey, hey, hey we finally stopped at the terminal state! :D\n",
      "Hey, hey, hey we finally stopped at the terminal state! :D\n",
      "Hey, hey, hey we finally stopped at the terminal state! :D\n",
      "Hey, hey, hey we finally stopped at the terminal state! :D\n",
      "Hey, hey, hey we finally stopped at the terminal state! :D\n",
      "Hey, hey, hey we finally stopped at the terminal state! :D\n",
      "Hey, hey, hey we finally stopped at the terminal state! :D\n",
      "Hey, hey, hey we finally stopped at the terminal state! :D\n",
      "Hey, hey, hey we finally stopped at the terminal state! :D\n",
      "Hey, hey, hey we finally stopped at the terminal state! :D\n",
      "Point outside of brain mask :(\n",
      "Point outside of brain mask :(\n",
      "Point outside of brain mask :(\n",
      "Point outside of brain mask :(\n",
      "Hey, hey, hey we finally stopped at the terminal state! :D\n",
      "Hey, hey, hey we finally stopped at the terminal state! :D\n",
      "Point outside of brain mask :(\n",
      "Point outside of brain mask :(\n",
      "Point outside of brain mask :(\n",
      "Point outside of brain mask :(\n",
      "Point outside of brain mask :(\n",
      "Point outside of brain mask :(\n",
      "Point outside of brain mask :(\n",
      "Point outside of brain mask :(\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-f400cd1278cb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mstep_distance\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_actions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m         \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimal_steps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m>\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mz\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimal_steps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/fibre_tracking/deepFibreTracking/dfibert/envs/RLtractEnvironment.py\u001b[0m in \u001b[0;36mreset\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    159\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m         \u001b[0mfile_sl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mStreamlinesFromFileTracker\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpReferenceStreamlines\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 161\u001b[0;31m         \u001b[0mfile_sl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    162\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m         \u001b[0mtracked_streamlines\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfile_sl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_streamlines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/fibre_tracking/deepFibreTracking/dfibert/tracker/__init__.py\u001b[0m in \u001b[0;36mtrack\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    209\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtrack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m         \u001b[0mTracker\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 211\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstreamlines\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_vtk_streamlines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# TODO catch exception if path does not exist\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    212\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mISMRMReferenceStreamlinesTracker\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTracker\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/atari/lib/python3.8/site-packages/dipy/io/vtk.py\u001b[0m in \u001b[0;36mload_vtk_streamlines\u001b[0;34m(filename, to_lps)\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m     \"\"\"\n\u001b[0;32m---> 93\u001b[0;31m     \u001b[0mpolydata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_polydata\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m     \u001b[0mlines\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfury\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_polydata_lines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpolydata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mto_lps\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/atari/lib/python3.8/site-packages/dipy/io/vtk.py\u001b[0m in \u001b[0;36mload_polydata\u001b[0;34m(file_name)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \"\"\"\n\u001b[0;32m---> 26\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mfury\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_polydata\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/atari/lib/python3.8/site-packages/fury/io.py\u001b[0m in \u001b[0;36mload_polydata\u001b[0;34m(file_name)\u001b[0m\n\u001b[1;32m    218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m     \u001b[0mreader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSetFileName\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 220\u001b[0;31m     \u001b[0mreader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mUpdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    221\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mreader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGetOutput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    222\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "steps = 0\n",
    "while len(optimal_steps) < 44:\n",
    "    step_distance = []\n",
    "    for i in range(n_actions):\n",
    "        env.reset()\n",
    "        if len(optimal_steps)>0:\n",
    "            for z in range(len(optimal_steps)):\n",
    "                _,_,_ = env.step(optimal_steps[z])\n",
    "        next_state, _, terminal = env.step(i)\n",
    "        #distance = lineseg_dist(next_state.getCoordinate().numpy(), referenceLine[np.min([len(optimal_steps), 85])].numpy(), referenceLine[np.min([len(optimal_steps)+1, len(referenceLine)-1])].numpy())\n",
    "        #distance = ((next_state.getCoordinate()[0] - env.referenceStreamline_ijk[np.min([env.stepCounter, 58])][0])**2 \\\n",
    "        #              + (next_state.getCoordinate()[1] - env.referenceStreamline_ijk[np.min([env.stepCounter, 58])][1])**2 \\\n",
    "        #              + (next_state.getCoordinate()[2] - env.referenceStreamline_ijk[np.min([env.stepCounter, 58])][2])**2)\n",
    "        current_index = np.min([env.stepCounter, len(env.referenceStreamline_ijk)-1])\n",
    "        qry_pt = next_state.getCoordinate().view(-1,3)\n",
    "        distance = torch.sum((env.referenceStreamline_ijk[current_index] - qry_pt)**2)\n",
    "        \n",
    "        step_distance.append(distance)\n",
    "    optimal_steps.append(np.argmin(step_distance))\n",
    "print(optimal_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For Streamline with len 44 (index 4)\n",
    "#print(optimal_steps)\n",
    "optimal_steps = [17, 30, 100, 17, 30, 17, 17, 6, 0, 17, 37, 0, 0, 78, 24, 16, 24, 24, 100, 37, 45, 45, 70, 100, 99, 86, 86, 78, 94, 62, 100, 65, 70, 86, 65, 100, 86, 94, 99, 45, 99, 100, 100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with line distance\n",
    "#optimal_steps = [80, 88, 54, 96, 46, 75, 75, 75, 83, 75, 83, 83, 62, 54, 1, 59, 54, 59, 59, 67, 56, 59, 51, 59, 61, 11, 53, 61, 66, 71, 71, 79, 58, 71, 71, 71, 21, 71, 84, 92, 84, 92, 97, 84, 43, 84, 30, 97, 47, 97, 43, 30, 89, 35, 94, 73, 48, 89, 22, 72, 43, 35, 22, 35, 35, 6, 19, 3, 16, 16, 66, 16, 8, 21, 29, 21, 26, 26, 93, 26, 93, 85, 35, 85, 72, 77, 100]\n",
    "optimal_steps = [100, 80, 75, 80, 75, 100, 62, 75, 83, 75, 83, 100, 83, 62, 67, 51, 67, 100, 59, 59, 59, 100, 59, 56, 51, 56, 66, 100, 66, 71, 71, 79, 58, 100, 71, 71, 84, 71, 100, 92, 84, 92, 97, 100, 97, 38, 97, 43, 38, 100, 43, 43, 89, 48, 100, 94, 81, 48, 43, 100, 35, 43, 35, 22, 27, 100, 6, 11, 3, 16, 100, 8, 29, 21, 21, 100, 34, 26, 26, 93, 39, 100, 93, 72, 77, 77, 101]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#optimal_steps = [80, 75, 80, 75, 62, 75, 83, 96, 51, 24, 62, 62, 62, 77, 65, 64, 67, 59, 56, 83, 82, 54, 56, 53, 56, 38, 56, 84, 66, 71, 71, 64, 97, 84, 71, 71, 38, 51, 30, 92, 97, 84, 43, 79, 27, 46, 89, 25, 81, 25, 48, 43, 86, 48, 57, 14, 89, 43, 43, 19, 92, 14, 27, 9, 78, 4, 16, 3, 29, 3, 47, 6, 42, 21, 39, 5, 72, 34, 98, 88, 90, 75, 77, 59, 49, 32, 82]\n",
    "print(optimal_steps) # <-- min of max distance reward streamline 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[80, 75, 80, 75, 62, 75, 83, 96, 51, 24, 62, 62, 62, 77, 65, 64, 67, 59, 56, 83, 82, 54, 56, 53, 56, 38, 56, 84, 66, 71, 71, 64, 97, 84, 71, 71, 38, 51, 30, 92, 97, 84, 43, 79, 27, 46, 89, 25, 81, 25, 48, 43, 86, 48, 57, 14, 89, 43, 43, 19, 92, 14, 27, 9, 78, 4, 16, 3, 29, 3, 47, 6, 42, 21, 39, 5, 72, 34, 98, 88, 90, 75, 77, 59, 49, 32, 82]\n"
     ]
    }
   ],
   "source": [
    "optimal_steps = [80, 75, 80, 75, 62, 75, 83, 96, 51, 24, 62, 62, 62, 77, 65, 64, 67, 59, 56, 83, 82, 54, 56, 53, 56, 38, 56, 84, 66, 71, 71, 64, 97, 84, 71, 71, 38, 51, 30, 92, 97, 84, 43, 79, 27, 46, 89, 25, 81, 25, 48, 43, 86, 48, 57, 14, 89, 43, 43, 19, 92, 14, 27, 9, 78, 4, 16, 3, 29, 3, 47, 6, 42, 21, 39, 5, 72, 34, 98, 88, 90, 75, 77, 59, 49, 32, 82]\n",
    "print(optimal_steps) # <-- min of max distance reward streamline 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[80, 88, 54, 96, 59, 37, 54, 37, 67, 91, 62, 78, 64, 70, 64, 62, 69, 83, 56, 59, 59, 53, 42, 53, 53, 56, 79, 58, 87, 60, 87, 58, 92, 52, 46, 58, 38, 58, 30, 58, 38, 84, 17, 55, 30, 76, 25, 76, 17, 76, 17, 81, 30, 86, 48, 65, 27, 97, 14, 84, 6, 97, 14, 89, 3, 27, 8, 19, 13, 19, 13, 29, 8, 96, 2, 88, 31, 47, 26, 59, 5, 72, 72, 85, 61, 39]\n"
     ]
    }
   ],
   "source": [
    "optimal_steps = [80, 88, 54, 96, 59, 37, 54, 37, 67, 91, 62, 78, 64, 70, 64, 62, 69, 83, 56, 59, 59, 53, 42, 53, 53, 56, 79, 58, 87, 60, 87, 58, 92, 52, 46, 58, 38, 58, 30, 58, 38, 84, 17, 55, 30, 76, 25, 76, 17, 76, 17, 81, 30, 86, 48, 65, 27, 97, 14, 84, 6, 97, 14, 89, 3, 27, 8, 19, 13, 19, 13, 29, 8, 96, 2, 88, 31, 47, 26, 59, 5, 72, 72, 85, 61, 39]\n",
    "print(optimal_steps) # <-- min reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change optimal steps\n",
    "optimal_steps = [17, 30, 100, 17, 30, 17, 17, 6, 0, 6, 0, 0, 0, 78, 24, 16, 24, 24, 100, 37, 45, 45, 70, 100, 99, 86, 86, 78, 94, 62, 100, 65, 70, 86, 65, 100, 86, 94, 99, 45, 99, 100, 100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[43.4998  94.53768 24.26608] tensor([43.4998, 94.5377, 24.2661])\n",
      "tensor([[42.8031, 94.5352, 24.9834]], dtype=torch.float64)\n",
      "tensor(0.0614, dtype=torch.float64)\n",
      "1 23.15581395348837\n",
      "tensor([[41.8925, 94.6103, 25.3898]], dtype=torch.float64)\n",
      "tensor(0.1511, dtype=torch.float64)\n",
      "2 23.15581395348837\n",
      "tensor([[41.8925, 94.6103, 25.3898]], dtype=torch.float64)\n",
      "tensor(0.2002, dtype=torch.float64)\n",
      "3 23.155813953488362\n",
      "tensor([[41.1957, 94.6079, 26.1071]], dtype=torch.float64)\n",
      "tensor(0.0903, dtype=torch.float64)\n",
      "4 23.155813953488376\n",
      "tensor([[40.2851, 94.6830, 26.5135]], dtype=torch.float64)\n",
      "tensor(0.0072, dtype=torch.float64)\n",
      "5 23.155813953488376\n",
      "tensor([[39.5884, 94.6805, 27.2308]], dtype=torch.float64)\n",
      "tensor(0.0305, dtype=torch.float64)\n",
      "6 23.155813953488376\n",
      "tensor([[38.8916, 94.6781, 27.9482]], dtype=torch.float64)\n",
      "tensor(0.1306, dtype=torch.float64)\n",
      "7 23.155813953488376\n",
      "tensor([[38.7411, 95.2013, 28.7869]], dtype=torch.float64)\n",
      "tensor(0.2268, dtype=torch.float64)\n",
      "8 23.155813953488376\n",
      "tensor([[38.9493, 95.2347, 29.7644]], dtype=torch.float64)\n",
      "tensor(0.3804, dtype=torch.float64)\n",
      "9 -0.09999999999999432\n",
      "tensor([[38.7988, 95.7579, 30.6032]], dtype=torch.float64)\n",
      "tensor(0.5721, dtype=torch.float64)\n",
      "10 -0.09999999999999432\n",
      "tensor([[39.0070, 95.7913, 31.5807]], dtype=torch.float64)\n",
      "tensor(1.0748, dtype=torch.float64)\n",
      "11 -100\n",
      "tensor([[39.2153, 95.8246, 32.5582]], dtype=torch.float64)\n",
      "tensor(1.9637, dtype=torch.float64)\n",
      "12 -100\n",
      "tensor([[39.4235, 95.8580, 33.5358]], dtype=torch.float64)\n",
      "tensor(3.4354, dtype=torch.float64)\n",
      "13 -100\n",
      "tensor([[39.8229, 96.7330, 33.2622]], dtype=torch.float64)\n",
      "tensor(0.7040, dtype=torch.float64)\n",
      "14 -100\n",
      "tensor([[40.3123, 97.4732, 33.7233]], dtype=torch.float64)\n",
      "tensor(0.4812, dtype=torch.float64)\n",
      "15 -0.4999999999999716\n",
      "tensor([[40.8035, 97.9269, 34.4668]], dtype=torch.float64)\n",
      "tensor(0.9140, dtype=torch.float64)\n",
      "16 -100\n",
      "tensor([[41.2930, 98.6671, 34.9279]], dtype=torch.float64)\n",
      "tensor(1.0578, dtype=torch.float64)\n",
      "17 -100\n",
      "tensor([[41.7824, 99.4073, 35.3890]], dtype=torch.float64)\n",
      "tensor(1.3673, dtype=torch.float64)\n",
      "18 -100\n",
      "tensor([[41.7824, 99.4073, 35.3890]], dtype=torch.float64)\n",
      "tensor(0.9023, dtype=torch.float64)\n",
      "19 -100\n",
      "tensor([[ 42.5462, 100.0251,  35.5759]], dtype=torch.float64)\n",
      "tensor(0.6923, dtype=torch.float64)\n",
      "20 -0.4999999999999716\n",
      "tensor([[ 43.0052, 100.9048,  35.7003]], dtype=torch.float64)\n",
      "tensor(0.6090, dtype=torch.float64)\n",
      "21 -0.09999999999999432\n",
      "tensor([[ 43.4642, 101.7844,  35.8247]], dtype=torch.float64)\n",
      "tensor(1.0525, dtype=torch.float64)\n",
      "22 -100\n",
      "tensor([[ 43.8653, 102.4841,  35.2333]], dtype=torch.float64)\n",
      "tensor(0.3922, dtype=torch.float64)\n",
      "23 -0.19999999999998863\n",
      "tensor([[ 43.8653, 102.4841,  35.2333]], dtype=torch.float64)\n",
      "tensor(0.9607, dtype=torch.float64)\n",
      "24 -100\n",
      "tensor([[ 44.0032, 103.4741,  35.2054]], dtype=torch.float64)\n",
      "tensor(1.0906, dtype=torch.float64)\n",
      "25 -100\n",
      "tensor([[ 44.0636, 104.3936,  34.8169]], dtype=torch.float64)\n",
      "tensor(0.7803, dtype=torch.float64)\n",
      "26 -100\n",
      "tensor([[ 44.1240, 105.3131,  34.4284]], dtype=torch.float64)\n",
      "tensor(0.5579, dtype=torch.float64)\n",
      "27 -0.39999999999997726\n",
      "tensor([[ 44.5234, 106.1881,  34.1548]], dtype=torch.float64)\n",
      "tensor(0.6910, dtype=torch.float64)\n",
      "28 -0.09999999999999432\n",
      "tensor([[ 44.2797, 107.1386,  33.9622]], dtype=torch.float64)\n",
      "tensor(0.9922, dtype=torch.float64)\n",
      "29 -100\n",
      "tensor([[ 44.8697, 107.4954,  33.2379]], dtype=torch.float64)\n",
      "tensor(0.4440, dtype=torch.float64)\n",
      "30 -0.19999999999998863\n",
      "tensor([[ 44.8697, 107.4954,  33.2379]], dtype=torch.float64)\n",
      "tensor(1.3241, dtype=torch.float64)\n",
      "31 -100\n",
      "tensor([[ 44.9277, 108.2188,  32.5499]], dtype=torch.float64)\n",
      "tensor(0.9139, dtype=torch.float64)\n",
      "32 -100\n",
      "tensor([[ 45.3287, 108.9184,  31.9585]], dtype=torch.float64)\n",
      "tensor(0.7175, dtype=torch.float64)\n",
      "33 -100\n",
      "tensor([[ 45.3891, 109.8379,  31.5700]], dtype=torch.float64)\n",
      "tensor(0.9084, dtype=torch.float64)\n",
      "34 -100\n",
      "tensor([[ 45.4471, 110.5612,  30.8820]], dtype=torch.float64)\n",
      "tensor(0.5554, dtype=torch.float64)\n",
      "35 -0.4999999999999716\n",
      "tensor([[ 45.4471, 110.5612,  30.8820]], dtype=torch.float64)\n",
      "tensor(1.4992, dtype=torch.float64)\n",
      "36 -100\n",
      "tensor([[ 45.5075, 111.4807,  30.4935]], dtype=torch.float64)\n",
      "tensor(1.1073, dtype=torch.float64)\n",
      "37 -100\n",
      "tensor([[ 45.2638, 112.4312,  30.3008]], dtype=torch.float64)\n",
      "tensor(0.9724, dtype=torch.float64)\n",
      "38 -100\n",
      "tensor([[ 45.4017, 113.4213,  30.2728]], dtype=torch.float64)\n",
      "tensor(0.8211, dtype=torch.float64)\n",
      "39 -100\n",
      "tensor([[ 45.8608, 114.3009,  30.3972]], dtype=torch.float64)\n",
      "tensor(0.8646, dtype=torch.float64)\n",
      "40 -100\n",
      "tensor([[ 45.9987, 115.2910,  30.3692]], dtype=torch.float64)\n",
      "tensor(0.8250, dtype=torch.float64)\n",
      "41 -100\n",
      "Hey, hey, hey we finally stopped at the terminal state! :D\n",
      "42 100\n",
      "Hey, hey, hey we finally stopped at the terminal state! :D\n",
      "43 100\n",
      "0.0071684624022552434 3.435446943419497 34.245157352839115\n"
     ]
    }
   ],
   "source": [
    "state = env.reset()\n",
    "print(env.state.getCoordinate().numpy(), env.referenceStreamline_ijk[0])\n",
    "step = 1\n",
    "all_distances = []\n",
    "all_states = []\n",
    "len_line = len(env.referenceStreamline_ijk)-1\n",
    "all_states.append(state.getCoordinate())\n",
    "for i in optimal_steps:\n",
    "    next_state, reward, terminal = env.step(i)\n",
    "    print(step, reward)\n",
    "    #current_index = np.min([env.points_visited+1,len(env.referenceStreamline_ijk)-1])\n",
    "    #print(\"Reference Line at current index: \", env.referenceStreamline_ijk[current_index])\n",
    "    #distance = lineseg_dist(next_state.getCoordinate().numpy(), referenceLine[step-1].numpy(), referenceLine[np.min([step, len(referenceLine)-1])].numpy())\n",
    "    #distance = 2 + (distance/10)\n",
    "    distance = ((next_state.getCoordinate()[0] - env.referenceStreamline_ijk[np.min([env.stepCounter, len_line])][0])**2 \\\n",
    "                      + (next_state.getCoordinate()[1] - env.referenceStreamline_ijk[np.min([env.stepCounter, len_line])][1])**2 \\\n",
    "                      + (next_state.getCoordinate()[2] - env.referenceStreamline_ijk[np.min([env.stepCounter, len_line])][2])**2)\n",
    "    #print(step, i, next_state.getCoordinate().numpy(), env.referenceStreamline_ijk[np.min([env.stepCounter,len_line])].numpy(), reward, distance.item(), distance <= 0.55**2)\n",
    "    all_distances.append(distance)\n",
    "    all_states.append(next_state.getCoordinate())\n",
    "    #if distance < 0.71:\n",
    "    #    reward = 1 - distance\n",
    "    #    #print(reward)\n",
    "    #    if reward < 0.3:\n",
    "    #        reward = 1\n",
    "    step += 1\n",
    "\n",
    "print(np.min(all_distances), np.max(all_distances), np.sum(all_distances))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 76.4527, 108.1185,  91.8665])\n",
      "tensor(108.1185)\n"
     ]
    }
   ],
   "source": [
    "print(env.referenceStreamline_ijk[4])\n",
    "print(env.referenceStreamline_ijk.T[1][4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "/* Put everything inside the global mpl namespace */\n",
       "window.mpl = {};\n",
       "\n",
       "\n",
       "mpl.get_websocket_type = function() {\n",
       "    if (typeof(WebSocket) !== 'undefined') {\n",
       "        return WebSocket;\n",
       "    } else if (typeof(MozWebSocket) !== 'undefined') {\n",
       "        return MozWebSocket;\n",
       "    } else {\n",
       "        alert('Your browser does not have WebSocket support. ' +\n",
       "              'Please try Chrome, Safari or Firefox ≥ 6. ' +\n",
       "              'Firefox 4 and 5 are also supported but you ' +\n",
       "              'have to enable WebSockets in about:config.');\n",
       "    };\n",
       "}\n",
       "\n",
       "mpl.figure = function(figure_id, websocket, ondownload, parent_element) {\n",
       "    this.id = figure_id;\n",
       "\n",
       "    this.ws = websocket;\n",
       "\n",
       "    this.supports_binary = (this.ws.binaryType != undefined);\n",
       "\n",
       "    if (!this.supports_binary) {\n",
       "        var warnings = document.getElementById(\"mpl-warnings\");\n",
       "        if (warnings) {\n",
       "            warnings.style.display = 'block';\n",
       "            warnings.textContent = (\n",
       "                \"This browser does not support binary websocket messages. \" +\n",
       "                    \"Performance may be slow.\");\n",
       "        }\n",
       "    }\n",
       "\n",
       "    this.imageObj = new Image();\n",
       "\n",
       "    this.context = undefined;\n",
       "    this.message = undefined;\n",
       "    this.canvas = undefined;\n",
       "    this.rubberband_canvas = undefined;\n",
       "    this.rubberband_context = undefined;\n",
       "    this.format_dropdown = undefined;\n",
       "\n",
       "    this.image_mode = 'full';\n",
       "\n",
       "    this.root = $('<div/>');\n",
       "    this._root_extra_style(this.root)\n",
       "    this.root.attr('style', 'display: inline-block');\n",
       "\n",
       "    $(parent_element).append(this.root);\n",
       "\n",
       "    this._init_header(this);\n",
       "    this._init_canvas(this);\n",
       "    this._init_toolbar(this);\n",
       "\n",
       "    var fig = this;\n",
       "\n",
       "    this.waiting = false;\n",
       "\n",
       "    this.ws.onopen =  function () {\n",
       "            fig.send_message(\"supports_binary\", {value: fig.supports_binary});\n",
       "            fig.send_message(\"send_image_mode\", {});\n",
       "            if (mpl.ratio != 1) {\n",
       "                fig.send_message(\"set_dpi_ratio\", {'dpi_ratio': mpl.ratio});\n",
       "            }\n",
       "            fig.send_message(\"refresh\", {});\n",
       "        }\n",
       "\n",
       "    this.imageObj.onload = function() {\n",
       "            if (fig.image_mode == 'full') {\n",
       "                // Full images could contain transparency (where diff images\n",
       "                // almost always do), so we need to clear the canvas so that\n",
       "                // there is no ghosting.\n",
       "                fig.context.clearRect(0, 0, fig.canvas.width, fig.canvas.height);\n",
       "            }\n",
       "            fig.context.drawImage(fig.imageObj, 0, 0);\n",
       "        };\n",
       "\n",
       "    this.imageObj.onunload = function() {\n",
       "        fig.ws.close();\n",
       "    }\n",
       "\n",
       "    this.ws.onmessage = this._make_on_message_function(this);\n",
       "\n",
       "    this.ondownload = ondownload;\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._init_header = function() {\n",
       "    var titlebar = $(\n",
       "        '<div class=\"ui-dialog-titlebar ui-widget-header ui-corner-all ' +\n",
       "        'ui-helper-clearfix\"/>');\n",
       "    var titletext = $(\n",
       "        '<div class=\"ui-dialog-title\" style=\"width: 100%; ' +\n",
       "        'text-align: center; padding: 3px;\"/>');\n",
       "    titlebar.append(titletext)\n",
       "    this.root.append(titlebar);\n",
       "    this.header = titletext[0];\n",
       "}\n",
       "\n",
       "\n",
       "\n",
       "mpl.figure.prototype._canvas_extra_style = function(canvas_div) {\n",
       "\n",
       "}\n",
       "\n",
       "\n",
       "mpl.figure.prototype._root_extra_style = function(canvas_div) {\n",
       "\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._init_canvas = function() {\n",
       "    var fig = this;\n",
       "\n",
       "    var canvas_div = $('<div/>');\n",
       "\n",
       "    canvas_div.attr('style', 'position: relative; clear: both; outline: 0');\n",
       "\n",
       "    function canvas_keyboard_event(event) {\n",
       "        return fig.key_event(event, event['data']);\n",
       "    }\n",
       "\n",
       "    canvas_div.keydown('key_press', canvas_keyboard_event);\n",
       "    canvas_div.keyup('key_release', canvas_keyboard_event);\n",
       "    this.canvas_div = canvas_div\n",
       "    this._canvas_extra_style(canvas_div)\n",
       "    this.root.append(canvas_div);\n",
       "\n",
       "    var canvas = $('<canvas/>');\n",
       "    canvas.addClass('mpl-canvas');\n",
       "    canvas.attr('style', \"left: 0; top: 0; z-index: 0; outline: 0\")\n",
       "\n",
       "    this.canvas = canvas[0];\n",
       "    this.context = canvas[0].getContext(\"2d\");\n",
       "\n",
       "    var backingStore = this.context.backingStorePixelRatio ||\n",
       "\tthis.context.webkitBackingStorePixelRatio ||\n",
       "\tthis.context.mozBackingStorePixelRatio ||\n",
       "\tthis.context.msBackingStorePixelRatio ||\n",
       "\tthis.context.oBackingStorePixelRatio ||\n",
       "\tthis.context.backingStorePixelRatio || 1;\n",
       "\n",
       "    mpl.ratio = (window.devicePixelRatio || 1) / backingStore;\n",
       "\n",
       "    var rubberband = $('<canvas/>');\n",
       "    rubberband.attr('style', \"position: absolute; left: 0; top: 0; z-index: 1;\")\n",
       "\n",
       "    var pass_mouse_events = true;\n",
       "\n",
       "    canvas_div.resizable({\n",
       "        start: function(event, ui) {\n",
       "            pass_mouse_events = false;\n",
       "        },\n",
       "        resize: function(event, ui) {\n",
       "            fig.request_resize(ui.size.width, ui.size.height);\n",
       "        },\n",
       "        stop: function(event, ui) {\n",
       "            pass_mouse_events = true;\n",
       "            fig.request_resize(ui.size.width, ui.size.height);\n",
       "        },\n",
       "    });\n",
       "\n",
       "    function mouse_event_fn(event) {\n",
       "        if (pass_mouse_events)\n",
       "            return fig.mouse_event(event, event['data']);\n",
       "    }\n",
       "\n",
       "    rubberband.mousedown('button_press', mouse_event_fn);\n",
       "    rubberband.mouseup('button_release', mouse_event_fn);\n",
       "    // Throttle sequential mouse events to 1 every 20ms.\n",
       "    rubberband.mousemove('motion_notify', mouse_event_fn);\n",
       "\n",
       "    rubberband.mouseenter('figure_enter', mouse_event_fn);\n",
       "    rubberband.mouseleave('figure_leave', mouse_event_fn);\n",
       "\n",
       "    canvas_div.on(\"wheel\", function (event) {\n",
       "        event = event.originalEvent;\n",
       "        event['data'] = 'scroll'\n",
       "        if (event.deltaY < 0) {\n",
       "            event.step = 1;\n",
       "        } else {\n",
       "            event.step = -1;\n",
       "        }\n",
       "        mouse_event_fn(event);\n",
       "    });\n",
       "\n",
       "    canvas_div.append(canvas);\n",
       "    canvas_div.append(rubberband);\n",
       "\n",
       "    this.rubberband = rubberband;\n",
       "    this.rubberband_canvas = rubberband[0];\n",
       "    this.rubberband_context = rubberband[0].getContext(\"2d\");\n",
       "    this.rubberband_context.strokeStyle = \"#000000\";\n",
       "\n",
       "    this._resize_canvas = function(width, height) {\n",
       "        // Keep the size of the canvas, canvas container, and rubber band\n",
       "        // canvas in synch.\n",
       "        canvas_div.css('width', width)\n",
       "        canvas_div.css('height', height)\n",
       "\n",
       "        canvas.attr('width', width * mpl.ratio);\n",
       "        canvas.attr('height', height * mpl.ratio);\n",
       "        canvas.attr('style', 'width: ' + width + 'px; height: ' + height + 'px;');\n",
       "\n",
       "        rubberband.attr('width', width);\n",
       "        rubberband.attr('height', height);\n",
       "    }\n",
       "\n",
       "    // Set the figure to an initial 600x600px, this will subsequently be updated\n",
       "    // upon first draw.\n",
       "    this._resize_canvas(600, 600);\n",
       "\n",
       "    // Disable right mouse context menu.\n",
       "    $(this.rubberband_canvas).bind(\"contextmenu\",function(e){\n",
       "        return false;\n",
       "    });\n",
       "\n",
       "    function set_focus () {\n",
       "        canvas.focus();\n",
       "        canvas_div.focus();\n",
       "    }\n",
       "\n",
       "    window.setTimeout(set_focus, 100);\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._init_toolbar = function() {\n",
       "    var fig = this;\n",
       "\n",
       "    var nav_element = $('<div/>');\n",
       "    nav_element.attr('style', 'width: 100%');\n",
       "    this.root.append(nav_element);\n",
       "\n",
       "    // Define a callback function for later on.\n",
       "    function toolbar_event(event) {\n",
       "        return fig.toolbar_button_onclick(event['data']);\n",
       "    }\n",
       "    function toolbar_mouse_event(event) {\n",
       "        return fig.toolbar_button_onmouseover(event['data']);\n",
       "    }\n",
       "\n",
       "    for(var toolbar_ind in mpl.toolbar_items) {\n",
       "        var name = mpl.toolbar_items[toolbar_ind][0];\n",
       "        var tooltip = mpl.toolbar_items[toolbar_ind][1];\n",
       "        var image = mpl.toolbar_items[toolbar_ind][2];\n",
       "        var method_name = mpl.toolbar_items[toolbar_ind][3];\n",
       "\n",
       "        if (!name) {\n",
       "            // put a spacer in here.\n",
       "            continue;\n",
       "        }\n",
       "        var button = $('<button/>');\n",
       "        button.addClass('ui-button ui-widget ui-state-default ui-corner-all ' +\n",
       "                        'ui-button-icon-only');\n",
       "        button.attr('role', 'button');\n",
       "        button.attr('aria-disabled', 'false');\n",
       "        button.click(method_name, toolbar_event);\n",
       "        button.mouseover(tooltip, toolbar_mouse_event);\n",
       "\n",
       "        var icon_img = $('<span/>');\n",
       "        icon_img.addClass('ui-button-icon-primary ui-icon');\n",
       "        icon_img.addClass(image);\n",
       "        icon_img.addClass('ui-corner-all');\n",
       "\n",
       "        var tooltip_span = $('<span/>');\n",
       "        tooltip_span.addClass('ui-button-text');\n",
       "        tooltip_span.html(tooltip);\n",
       "\n",
       "        button.append(icon_img);\n",
       "        button.append(tooltip_span);\n",
       "\n",
       "        nav_element.append(button);\n",
       "    }\n",
       "\n",
       "    var fmt_picker_span = $('<span/>');\n",
       "\n",
       "    var fmt_picker = $('<select/>');\n",
       "    fmt_picker.addClass('mpl-toolbar-option ui-widget ui-widget-content');\n",
       "    fmt_picker_span.append(fmt_picker);\n",
       "    nav_element.append(fmt_picker_span);\n",
       "    this.format_dropdown = fmt_picker[0];\n",
       "\n",
       "    for (var ind in mpl.extensions) {\n",
       "        var fmt = mpl.extensions[ind];\n",
       "        var option = $(\n",
       "            '<option/>', {selected: fmt === mpl.default_extension}).html(fmt);\n",
       "        fmt_picker.append(option);\n",
       "    }\n",
       "\n",
       "    // Add hover states to the ui-buttons\n",
       "    $( \".ui-button\" ).hover(\n",
       "        function() { $(this).addClass(\"ui-state-hover\");},\n",
       "        function() { $(this).removeClass(\"ui-state-hover\");}\n",
       "    );\n",
       "\n",
       "    var status_bar = $('<span class=\"mpl-message\"/>');\n",
       "    nav_element.append(status_bar);\n",
       "    this.message = status_bar[0];\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.request_resize = function(x_pixels, y_pixels) {\n",
       "    // Request matplotlib to resize the figure. Matplotlib will then trigger a resize in the client,\n",
       "    // which will in turn request a refresh of the image.\n",
       "    this.send_message('resize', {'width': x_pixels, 'height': y_pixels});\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.send_message = function(type, properties) {\n",
       "    properties['type'] = type;\n",
       "    properties['figure_id'] = this.id;\n",
       "    this.ws.send(JSON.stringify(properties));\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.send_draw_message = function() {\n",
       "    if (!this.waiting) {\n",
       "        this.waiting = true;\n",
       "        this.ws.send(JSON.stringify({type: \"draw\", figure_id: this.id}));\n",
       "    }\n",
       "}\n",
       "\n",
       "\n",
       "mpl.figure.prototype.handle_save = function(fig, msg) {\n",
       "    var format_dropdown = fig.format_dropdown;\n",
       "    var format = format_dropdown.options[format_dropdown.selectedIndex].value;\n",
       "    fig.ondownload(fig, format);\n",
       "}\n",
       "\n",
       "\n",
       "mpl.figure.prototype.handle_resize = function(fig, msg) {\n",
       "    var size = msg['size'];\n",
       "    if (size[0] != fig.canvas.width || size[1] != fig.canvas.height) {\n",
       "        fig._resize_canvas(size[0], size[1]);\n",
       "        fig.send_message(\"refresh\", {});\n",
       "    };\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_rubberband = function(fig, msg) {\n",
       "    var x0 = msg['x0'] / mpl.ratio;\n",
       "    var y0 = (fig.canvas.height - msg['y0']) / mpl.ratio;\n",
       "    var x1 = msg['x1'] / mpl.ratio;\n",
       "    var y1 = (fig.canvas.height - msg['y1']) / mpl.ratio;\n",
       "    x0 = Math.floor(x0) + 0.5;\n",
       "    y0 = Math.floor(y0) + 0.5;\n",
       "    x1 = Math.floor(x1) + 0.5;\n",
       "    y1 = Math.floor(y1) + 0.5;\n",
       "    var min_x = Math.min(x0, x1);\n",
       "    var min_y = Math.min(y0, y1);\n",
       "    var width = Math.abs(x1 - x0);\n",
       "    var height = Math.abs(y1 - y0);\n",
       "\n",
       "    fig.rubberband_context.clearRect(\n",
       "        0, 0, fig.canvas.width / mpl.ratio, fig.canvas.height / mpl.ratio);\n",
       "\n",
       "    fig.rubberband_context.strokeRect(min_x, min_y, width, height);\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_figure_label = function(fig, msg) {\n",
       "    // Updates the figure title.\n",
       "    fig.header.textContent = msg['label'];\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_cursor = function(fig, msg) {\n",
       "    var cursor = msg['cursor'];\n",
       "    switch(cursor)\n",
       "    {\n",
       "    case 0:\n",
       "        cursor = 'pointer';\n",
       "        break;\n",
       "    case 1:\n",
       "        cursor = 'default';\n",
       "        break;\n",
       "    case 2:\n",
       "        cursor = 'crosshair';\n",
       "        break;\n",
       "    case 3:\n",
       "        cursor = 'move';\n",
       "        break;\n",
       "    }\n",
       "    fig.rubberband_canvas.style.cursor = cursor;\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_message = function(fig, msg) {\n",
       "    fig.message.textContent = msg['message'];\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_draw = function(fig, msg) {\n",
       "    // Request the server to send over a new figure.\n",
       "    fig.send_draw_message();\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_image_mode = function(fig, msg) {\n",
       "    fig.image_mode = msg['mode'];\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.updated_canvas_event = function() {\n",
       "    // Called whenever the canvas gets updated.\n",
       "    this.send_message(\"ack\", {});\n",
       "}\n",
       "\n",
       "// A function to construct a web socket function for onmessage handling.\n",
       "// Called in the figure constructor.\n",
       "mpl.figure.prototype._make_on_message_function = function(fig) {\n",
       "    return function socket_on_message(evt) {\n",
       "        if (evt.data instanceof Blob) {\n",
       "            /* FIXME: We get \"Resource interpreted as Image but\n",
       "             * transferred with MIME type text/plain:\" errors on\n",
       "             * Chrome.  But how to set the MIME type?  It doesn't seem\n",
       "             * to be part of the websocket stream */\n",
       "            evt.data.type = \"image/png\";\n",
       "\n",
       "            /* Free the memory for the previous frames */\n",
       "            if (fig.imageObj.src) {\n",
       "                (window.URL || window.webkitURL).revokeObjectURL(\n",
       "                    fig.imageObj.src);\n",
       "            }\n",
       "\n",
       "            fig.imageObj.src = (window.URL || window.webkitURL).createObjectURL(\n",
       "                evt.data);\n",
       "            fig.updated_canvas_event();\n",
       "            fig.waiting = false;\n",
       "            return;\n",
       "        }\n",
       "        else if (typeof evt.data === 'string' && evt.data.slice(0, 21) == \"data:image/png;base64\") {\n",
       "            fig.imageObj.src = evt.data;\n",
       "            fig.updated_canvas_event();\n",
       "            fig.waiting = false;\n",
       "            return;\n",
       "        }\n",
       "\n",
       "        var msg = JSON.parse(evt.data);\n",
       "        var msg_type = msg['type'];\n",
       "\n",
       "        // Call the  \"handle_{type}\" callback, which takes\n",
       "        // the figure and JSON message as its only arguments.\n",
       "        try {\n",
       "            var callback = fig[\"handle_\" + msg_type];\n",
       "        } catch (e) {\n",
       "            console.log(\"No handler for the '\" + msg_type + \"' message type: \", msg);\n",
       "            return;\n",
       "        }\n",
       "\n",
       "        if (callback) {\n",
       "            try {\n",
       "                // console.log(\"Handling '\" + msg_type + \"' message: \", msg);\n",
       "                callback(fig, msg);\n",
       "            } catch (e) {\n",
       "                console.log(\"Exception inside the 'handler_\" + msg_type + \"' callback:\", e, e.stack, msg);\n",
       "            }\n",
       "        }\n",
       "    };\n",
       "}\n",
       "\n",
       "// from http://stackoverflow.com/questions/1114465/getting-mouse-location-in-canvas\n",
       "mpl.findpos = function(e) {\n",
       "    //this section is from http://www.quirksmode.org/js/events_properties.html\n",
       "    var targ;\n",
       "    if (!e)\n",
       "        e = window.event;\n",
       "    if (e.target)\n",
       "        targ = e.target;\n",
       "    else if (e.srcElement)\n",
       "        targ = e.srcElement;\n",
       "    if (targ.nodeType == 3) // defeat Safari bug\n",
       "        targ = targ.parentNode;\n",
       "\n",
       "    // jQuery normalizes the pageX and pageY\n",
       "    // pageX,Y are the mouse positions relative to the document\n",
       "    // offset() returns the position of the element relative to the document\n",
       "    var x = e.pageX - $(targ).offset().left;\n",
       "    var y = e.pageY - $(targ).offset().top;\n",
       "\n",
       "    return {\"x\": x, \"y\": y};\n",
       "};\n",
       "\n",
       "/*\n",
       " * return a copy of an object with only non-object keys\n",
       " * we need this to avoid circular references\n",
       " * http://stackoverflow.com/a/24161582/3208463\n",
       " */\n",
       "function simpleKeys (original) {\n",
       "  return Object.keys(original).reduce(function (obj, key) {\n",
       "    if (typeof original[key] !== 'object')\n",
       "        obj[key] = original[key]\n",
       "    return obj;\n",
       "  }, {});\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.mouse_event = function(event, name) {\n",
       "    var canvas_pos = mpl.findpos(event)\n",
       "\n",
       "    if (name === 'button_press')\n",
       "    {\n",
       "        this.canvas.focus();\n",
       "        this.canvas_div.focus();\n",
       "    }\n",
       "\n",
       "    var x = canvas_pos.x * mpl.ratio;\n",
       "    var y = canvas_pos.y * mpl.ratio;\n",
       "\n",
       "    this.send_message(name, {x: x, y: y, button: event.button,\n",
       "                             step: event.step,\n",
       "                             guiEvent: simpleKeys(event)});\n",
       "\n",
       "    /* This prevents the web browser from automatically changing to\n",
       "     * the text insertion cursor when the button is pressed.  We want\n",
       "     * to control all of the cursor setting manually through the\n",
       "     * 'cursor' event from matplotlib */\n",
       "    event.preventDefault();\n",
       "    return false;\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._key_event_extra = function(event, name) {\n",
       "    // Handle any extra behaviour associated with a key event\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.key_event = function(event, name) {\n",
       "\n",
       "    // Prevent repeat events\n",
       "    if (name == 'key_press')\n",
       "    {\n",
       "        if (event.which === this._key)\n",
       "            return;\n",
       "        else\n",
       "            this._key = event.which;\n",
       "    }\n",
       "    if (name == 'key_release')\n",
       "        this._key = null;\n",
       "\n",
       "    var value = '';\n",
       "    if (event.ctrlKey && event.which != 17)\n",
       "        value += \"ctrl+\";\n",
       "    if (event.altKey && event.which != 18)\n",
       "        value += \"alt+\";\n",
       "    if (event.shiftKey && event.which != 16)\n",
       "        value += \"shift+\";\n",
       "\n",
       "    value += 'k';\n",
       "    value += event.which.toString();\n",
       "\n",
       "    this._key_event_extra(event, name);\n",
       "\n",
       "    this.send_message(name, {key: value,\n",
       "                             guiEvent: simpleKeys(event)});\n",
       "    return false;\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.toolbar_button_onclick = function(name) {\n",
       "    if (name == 'download') {\n",
       "        this.handle_save(this, null);\n",
       "    } else {\n",
       "        this.send_message(\"toolbar_button\", {name: name});\n",
       "    }\n",
       "};\n",
       "\n",
       "mpl.figure.prototype.toolbar_button_onmouseover = function(tooltip) {\n",
       "    this.message.textContent = tooltip;\n",
       "};\n",
       "mpl.toolbar_items = [[\"Home\", \"Reset original view\", \"fa fa-home icon-home\", \"home\"], [\"Back\", \"Back to previous view\", \"fa fa-arrow-left icon-arrow-left\", \"back\"], [\"Forward\", \"Forward to next view\", \"fa fa-arrow-right icon-arrow-right\", \"forward\"], [\"\", \"\", \"\", \"\"], [\"Pan\", \"Pan axes with left mouse, zoom with right\", \"fa fa-arrows icon-move\", \"pan\"], [\"Zoom\", \"Zoom to rectangle\", \"fa fa-square-o icon-check-empty\", \"zoom\"], [\"\", \"\", \"\", \"\"], [\"Download\", \"Download plot\", \"fa fa-floppy-o icon-save\", \"download\"]];\n",
       "\n",
       "mpl.extensions = [\"eps\", \"jpeg\", \"pdf\", \"png\", \"ps\", \"raw\", \"svg\", \"tif\"];\n",
       "\n",
       "mpl.default_extension = \"png\";var comm_websocket_adapter = function(comm) {\n",
       "    // Create a \"websocket\"-like object which calls the given IPython comm\n",
       "    // object with the appropriate methods. Currently this is a non binary\n",
       "    // socket, so there is still some room for performance tuning.\n",
       "    var ws = {};\n",
       "\n",
       "    ws.close = function() {\n",
       "        comm.close()\n",
       "    };\n",
       "    ws.send = function(m) {\n",
       "        //console.log('sending', m);\n",
       "        comm.send(m);\n",
       "    };\n",
       "    // Register the callback with on_msg.\n",
       "    comm.on_msg(function(msg) {\n",
       "        //console.log('receiving', msg['content']['data'], msg);\n",
       "        // Pass the mpl event to the overridden (by mpl) onmessage function.\n",
       "        ws.onmessage(msg['content']['data'])\n",
       "    });\n",
       "    return ws;\n",
       "}\n",
       "\n",
       "mpl.mpl_figure_comm = function(comm, msg) {\n",
       "    // This is the function which gets called when the mpl process\n",
       "    // starts-up an IPython Comm through the \"matplotlib\" channel.\n",
       "\n",
       "    var id = msg.content.data.id;\n",
       "    // Get hold of the div created by the display call when the Comm\n",
       "    // socket was opened in Python.\n",
       "    var element = $(\"#\" + id);\n",
       "    var ws_proxy = comm_websocket_adapter(comm)\n",
       "\n",
       "    function ondownload(figure, format) {\n",
       "        window.open(figure.imageObj.src);\n",
       "    }\n",
       "\n",
       "    var fig = new mpl.figure(id, ws_proxy,\n",
       "                           ondownload,\n",
       "                           element.get(0));\n",
       "\n",
       "    // Call onopen now - mpl needs it, as it is assuming we've passed it a real\n",
       "    // web socket which is closed, not our websocket->open comm proxy.\n",
       "    ws_proxy.onopen();\n",
       "\n",
       "    fig.parent_element = element.get(0);\n",
       "    fig.cell_info = mpl.find_output_cell(\"<div id='\" + id + \"'></div>\");\n",
       "    if (!fig.cell_info) {\n",
       "        console.error(\"Failed to find cell for figure\", id, fig);\n",
       "        return;\n",
       "    }\n",
       "\n",
       "    var output_index = fig.cell_info[2]\n",
       "    var cell = fig.cell_info[0];\n",
       "\n",
       "};\n",
       "\n",
       "mpl.figure.prototype.handle_close = function(fig, msg) {\n",
       "    var width = fig.canvas.width/mpl.ratio\n",
       "    fig.root.unbind('remove')\n",
       "\n",
       "    // Update the output cell to use the data from the current canvas.\n",
       "    fig.push_to_output();\n",
       "    var dataURL = fig.canvas.toDataURL();\n",
       "    // Re-enable the keyboard manager in IPython - without this line, in FF,\n",
       "    // the notebook keyboard shortcuts fail.\n",
       "    IPython.keyboard_manager.enable()\n",
       "    $(fig.parent_element).html('<img src=\"' + dataURL + '\" width=\"' + width + '\">');\n",
       "    fig.close_ws(fig, msg);\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.close_ws = function(fig, msg){\n",
       "    fig.send_message('closing', msg);\n",
       "    // fig.ws.close()\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.push_to_output = function(remove_interactive) {\n",
       "    // Turn the data on the canvas into data in the output cell.\n",
       "    var width = this.canvas.width/mpl.ratio\n",
       "    var dataURL = this.canvas.toDataURL();\n",
       "    this.cell_info[1]['text/html'] = '<img src=\"' + dataURL + '\" width=\"' + width + '\">';\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.updated_canvas_event = function() {\n",
       "    // Tell IPython that the notebook contents must change.\n",
       "    IPython.notebook.set_dirty(true);\n",
       "    this.send_message(\"ack\", {});\n",
       "    var fig = this;\n",
       "    // Wait a second, then push the new image to the DOM so\n",
       "    // that it is saved nicely (might be nice to debounce this).\n",
       "    setTimeout(function () { fig.push_to_output() }, 1000);\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._init_toolbar = function() {\n",
       "    var fig = this;\n",
       "\n",
       "    var nav_element = $('<div/>');\n",
       "    nav_element.attr('style', 'width: 100%');\n",
       "    this.root.append(nav_element);\n",
       "\n",
       "    // Define a callback function for later on.\n",
       "    function toolbar_event(event) {\n",
       "        return fig.toolbar_button_onclick(event['data']);\n",
       "    }\n",
       "    function toolbar_mouse_event(event) {\n",
       "        return fig.toolbar_button_onmouseover(event['data']);\n",
       "    }\n",
       "\n",
       "    for(var toolbar_ind in mpl.toolbar_items){\n",
       "        var name = mpl.toolbar_items[toolbar_ind][0];\n",
       "        var tooltip = mpl.toolbar_items[toolbar_ind][1];\n",
       "        var image = mpl.toolbar_items[toolbar_ind][2];\n",
       "        var method_name = mpl.toolbar_items[toolbar_ind][3];\n",
       "\n",
       "        if (!name) { continue; };\n",
       "\n",
       "        var button = $('<button class=\"btn btn-default\" href=\"#\" title=\"' + name + '\"><i class=\"fa ' + image + ' fa-lg\"></i></button>');\n",
       "        button.click(method_name, toolbar_event);\n",
       "        button.mouseover(tooltip, toolbar_mouse_event);\n",
       "        nav_element.append(button);\n",
       "    }\n",
       "\n",
       "    // Add the status bar.\n",
       "    var status_bar = $('<span class=\"mpl-message\" style=\"text-align:right; float: right;\"/>');\n",
       "    nav_element.append(status_bar);\n",
       "    this.message = status_bar[0];\n",
       "\n",
       "    // Add the close button to the window.\n",
       "    var buttongrp = $('<div class=\"btn-group inline pull-right\"></div>');\n",
       "    var button = $('<button class=\"btn btn-mini btn-primary\" href=\"#\" title=\"Stop Interaction\"><i class=\"fa fa-power-off icon-remove icon-large\"></i></button>');\n",
       "    button.click(function (evt) { fig.handle_close(fig, {}); } );\n",
       "    button.mouseover('Stop Interaction', toolbar_mouse_event);\n",
       "    buttongrp.append(button);\n",
       "    var titlebar = this.root.find($('.ui-dialog-titlebar'));\n",
       "    titlebar.prepend(buttongrp);\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._root_extra_style = function(el){\n",
       "    var fig = this\n",
       "    el.on(\"remove\", function(){\n",
       "\tfig.close_ws(fig, {});\n",
       "    });\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._canvas_extra_style = function(el){\n",
       "    // this is important to make the div 'focusable\n",
       "    el.attr('tabindex', 0)\n",
       "    // reach out to IPython and tell the keyboard manager to turn it's self\n",
       "    // off when our div gets focus\n",
       "\n",
       "    // location in version 3\n",
       "    if (IPython.notebook.keyboard_manager) {\n",
       "        IPython.notebook.keyboard_manager.register_events(el);\n",
       "    }\n",
       "    else {\n",
       "        // location in version 2\n",
       "        IPython.keyboard_manager.register_events(el);\n",
       "    }\n",
       "\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._key_event_extra = function(event, name) {\n",
       "    var manager = IPython.notebook.keyboard_manager;\n",
       "    if (!manager)\n",
       "        manager = IPython.keyboard_manager;\n",
       "\n",
       "    // Check for shift+enter\n",
       "    if (event.shiftKey && event.which == 13) {\n",
       "        this.canvas_div.blur();\n",
       "        // select the cell after this one\n",
       "        var index = IPython.notebook.find_cell_index(this.cell_info[0]);\n",
       "        IPython.notebook.select(index + 1);\n",
       "    }\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_save = function(fig, msg) {\n",
       "    fig.ondownload(fig, null);\n",
       "}\n",
       "\n",
       "\n",
       "mpl.find_output_cell = function(html_output) {\n",
       "    // Return the cell and output element which can be found *uniquely* in the notebook.\n",
       "    // Note - this is a bit hacky, but it is done because the \"notebook_saving.Notebook\"\n",
       "    // IPython event is triggered only after the cells have been serialised, which for\n",
       "    // our purposes (turning an active figure into a static one), is too late.\n",
       "    var cells = IPython.notebook.get_cells();\n",
       "    var ncells = cells.length;\n",
       "    for (var i=0; i<ncells; i++) {\n",
       "        var cell = cells[i];\n",
       "        if (cell.cell_type === 'code'){\n",
       "            for (var j=0; j<cell.output_area.outputs.length; j++) {\n",
       "                var data = cell.output_area.outputs[j];\n",
       "                if (data.data) {\n",
       "                    // IPython >= 3 moved mimebundle to data attribute of output\n",
       "                    data = data.data;\n",
       "                }\n",
       "                if (data['text/html'] == html_output) {\n",
       "                    return [cell, data, j];\n",
       "                }\n",
       "            }\n",
       "        }\n",
       "    }\n",
       "}\n",
       "\n",
       "// Register the function which deals with the matplotlib target/channel.\n",
       "// The kernel may be null if the page has been refreshed.\n",
       "if (IPython.notebook.kernel != null) {\n",
       "    IPython.notebook.kernel.comm_manager.register_target('matplotlib', mpl.mpl_figure_comm);\n",
       "}\n"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<img src=\"data:image/png;base64,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\" width=\"640\">"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[<mpl_toolkits.mplot3d.art3d.Line3D at 0x2aab4f613e20>]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%matplotlib notebook\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#for i in range(len(states)):\n",
    "#    print(states[i], env.referenceStreamline_ijk[i])\n",
    "#    distance = ((states.T[0][i] - env.referenceStreamline_ijk.T[0][i])**2 \\\n",
    "#                      + (states.T[1][i] - env.referenceStreamline_ijk.T[1][i] )**2 \\\n",
    "#                      + (states.T[2][i] - env.referenceStreamline_ijk.T[2][i])**2)\n",
    "#    print(distance)\n",
    "\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = plt.axes(projection='3d')\n",
    "ax.plot3D(env.referenceStreamline_ijk.T[0], env.referenceStreamline_ijk.T[1], env.referenceStreamline_ijk.T[2])\n",
    "ax.plot3D(states.T[0][:], states.T[1][:], states.T[2][:])\n",
    "#print(optimal_steps[:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([43.4998, 42.5443, 42.5257], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "states = torch.stack(all_states)\n",
    "print(states.T[0][:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 71.7731, 113.9662,  79.6186])\n"
     ]
    }
   ],
   "source": [
    "print(referenceLine[86])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 73.651344 107.88106   93.29415 ] tensor([ 73.6513, 107.8811,  93.2942])\n",
      "tensor(66.2049, dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "env.reset()\n",
    "print(env.state.getCoordinate().numpy(), referenceLine[0])\n",
    "step = 0\n",
    "#all_rewards = []\n",
    "eps_reward = 0\n",
    "for i in optimal_steps:\n",
    "    next_state, distance, terminal = env.step(i)\n",
    "    if distance < 0.71:\n",
    "        reward = 1 - distance\n",
    "        #print(reward)\n",
    "        if reward < 0.3:\n",
    "            reward = 1\n",
    "    eps_reward += reward\n",
    "    #all_rewards.append(reward)\n",
    "    step += 1\n",
    "print(eps_reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(n_actions):\n",
    "    \n",
    "    next_state, distance, terminal = env.step(i)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action:  100 Step:  1 Coordinates:  [ 73.651344 107.88106   93.29415 ] [ 74.42344 107.87124  93.08491]\n",
      "Action:  67 Step:  2 Coordinates:  [ 74.34808267 107.88351105  92.57683017] [ 75.16057  107.88882   92.774536]\n",
      "Action:  100 Step:  3 Coordinates:  [ 74.34808267 107.88351105  92.57683017] [ 75.78847 107.96255  92.28433]\n",
      "Action:  100 Step:  4 Coordinates:  [ 74.34808267 107.88351105  92.57683017] [ 76.45265  108.118454  91.86654 ]\n",
      "Action:  100 Step:  5 Coordinates:  [ 74.34808267 107.88351105  92.57683017] [ 77.116844 108.27435   91.448746]\n",
      "Action:  100 Step:  6 Coordinates:  [ 74.34808267 107.88351105  92.57683017] [ 77.739716 108.54131   91.02359 ]\n",
      "Action:  100 Step:  7 Coordinates:  [ 74.34808267 107.88351105  92.57683017] [ 78.36259  108.80828   90.598434]\n",
      "Action:  100 Step:  8 Coordinates:  [ 74.34808267 107.88351105  92.57683017] [ 78.996666 109.15176   90.25207 ]\n",
      "Action:  100 Step:  9 Coordinates:  [ 74.34808267 107.88351105  92.57683017] [ 79.630745 109.495224  89.9057  ]\n",
      "Action:  100 Step:  10 Coordinates:  [ 74.34808267 107.88351105  92.57683017] [ 80.264824 109.8387    89.55933 ]\n",
      "Action:  100 Step:  11 Coordinates:  [ 74.34808267 107.88351105  92.57683017] [ 80.833374 110.28288   89.21371 ]\n",
      "Action:  100 Step:  12 Coordinates:  [ 74.34808267 107.88351105  92.57683017] [ 81.32385 110.75597  88.79464]\n",
      "Action:  100 Step:  13 Coordinates:  [ 74.34808267 107.88351105  92.57683017] [ 81.78852 111.07612  88.22755]\n",
      "Action:  100 Step:  14 Coordinates:  [ 74.34808267 107.88351105  92.57683017] [ 82.26274  111.20639   87.596565]\n",
      "Action:  100 Step:  15 Coordinates:  [ 74.34808267 107.88351105  92.57683017] [ 82.764885 111.12094   86.97968 ]\n",
      "Action:  100 Step:  16 Coordinates:  [ 74.34808267 107.88351105  92.57683017] [ 83.17989 111.072    86.2975 ]\n",
      "Action:  100 Step:  17 Coordinates:  [ 74.34808267 107.88351105  92.57683017] [ 83.60093  110.91725   85.635086]\n",
      "Action:  100 Step:  18 Coordinates:  [ 74.34808267 107.88351105  92.57683017] [ 84.02196  110.762505  84.97268 ]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-9b9d7b386c25>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mwhile\u001b[0m \u001b[0mterminal\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_actions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0mnext_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mterminal\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mreward\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mactions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/fibre_tracking/deepFibreTracking/dfibert/envs/RLtractEnvironment.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    110\u001b[0m         \u001b[0mpositionNextState\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetCoordinate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstepWidth\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0maction_vector\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m         \u001b[0mnextState\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTractographyState\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpositionNextState\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minterpolateDWIatState\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 112\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mnextState\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetValue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    113\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/fibre_tracking/deepFibreTracking/dfibert/envs/_state.py\u001b[0m in \u001b[0;36mgetValue\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minterpolatedDWI\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0;31m# interpolate DWI value at self.coordinate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minterpolatedDWI\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minterpolFuncHandle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcoordinate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minterpolatedDWI\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/fibre_tracking/deepFibreTracking/dfibert/envs/RLtractEnvironment.py\u001b[0m in \u001b[0;36minterpolateDWIatState\u001b[0;34m(self, stateCoordinates)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m             \u001b[0minterpolated_dwi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_interpolated_dwi\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mras_points\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpostprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdwi_postprocessor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/fibre_tracking/deepFibreTracking/dfibert/data/__init__.py\u001b[0m in \u001b[0;36mget_interpolated_dwi\u001b[0;34m(self, points, postprocessing, ignore_outside_points)\u001b[0m\n\u001b[1;32m    286\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    287\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mpostprocessing\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 288\u001b[0;31m             result = postprocessing(result, self.data.b0, \n\u001b[0m\u001b[1;32m    289\u001b[0m                                  \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbvecs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    290\u001b[0m                                  self.data.bvals)\n",
      "\u001b[0;32m~/fibre_tracking/deepFibreTracking/dfibert/data/postprocessing.py\u001b[0m in \u001b[0;36m_wrapper\u001b[0;34m(dwi, b0, bvecs, bvals)\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdwi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbvecs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbvals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m         \u001b[0mdata_sh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspherical_harmonics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msh_order\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msh_order\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msmooth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msmooth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdwi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbvecs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbvals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m         \u001b[0mdata_resampled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_sh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreal_sh\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/fibre_tracking/deepFibreTracking/dfibert/data/postprocessing.py\u001b[0m in \u001b[0;36m_wrapper\u001b[0;34m(dwi, _b0, bvecs, _bvals)\u001b[0m\n\u001b[1;32m     53\u001b[0m         \u001b[0mreal_sh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreal_sym_sh_mrtrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msh_order\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mraw_sphere\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtheta\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mraw_sphere\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mphi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m         \u001b[0ml\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mn\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m         \u001b[0minv_b\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msmooth_pinv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreal_sh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msmooth\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m         \u001b[0mdata_sh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdwi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minv_b\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/atari/lib/python3.8/site-packages/dipy/reconst/shm.py\u001b[0m in \u001b[0;36msmooth_pinv\u001b[0;34m(B, L)\u001b[0m\n\u001b[1;32m    661\u001b[0m     \"\"\"\n\u001b[1;32m    662\u001b[0m     \u001b[0mL\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdiag\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mL\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 663\u001b[0;31m     \u001b[0minv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinalg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpinv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mB\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mL\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    664\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0minv\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mB\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    665\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mpinv\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/atari/lib/python3.8/site-packages/numpy/linalg/linalg.py\u001b[0m in \u001b[0;36mpinv\u001b[0;34m(a, rcond, hermitian)\u001b[0m\n\u001b[1;32m   1959\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1960\u001b[0m     \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconjugate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1961\u001b[0;31m     \u001b[0mu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msvd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfull_matrices\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhermitian\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhermitian\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1962\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1963\u001b[0m     \u001b[0;31m# discard small singular values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36msvd\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/atari/lib/python3.8/site-packages/numpy/linalg/linalg.py\u001b[0m in \u001b[0;36msvd\u001b[0;34m(a, full_matrices, compute_uv, hermitian)\u001b[0m\n\u001b[1;32m   1624\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1625\u001b[0m         \u001b[0msignature\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'D->DdD'\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0misComplexType\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m'd->ddd'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1626\u001b[0;31m         \u001b[0mu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgufunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msignature\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mextobj\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mextobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1627\u001b[0m         \u001b[0mu\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mu\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult_t\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1628\u001b[0m         \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_realType\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult_t\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "env.reset()\n",
    "terminal = False\n",
    "step = 0\n",
    "actions = []\n",
    "past_state = env.state\n",
    "step = 1\n",
    "while terminal != True:\n",
    "    for i in range(n_actions)\n",
    "    action = np.random.randint(n_actions)\n",
    "    next_state, reward, terminal = env.step(action)\n",
    "    if reward < 1:\n",
    "        actions.append(action)\n",
    "        past_state = next_state\n",
    "        print(\"Action: \", action, \"Step: \",step, \"Coordinates: \", next_state.getCoordinate().numpy(), referenceLine[step].numpy())\n",
    "        step += 1\n",
    "    else:\n",
    "        env.state = past_state\n",
    "        env.stepCounter = step\n",
    "    #action = np.random.choice(possible_actions[step])\n",
    "    #next_state, reward, terminal = env.step(action)\n",
    "    #step += 1\n",
    "\n",
    "print(actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 77.7397, 108.5413,  91.0236])\n",
      "tensor([ 78.3626, 108.8083,  90.5984])\n"
     ]
    }
   ],
   "source": [
    "print(referenceLine[6])\n",
    "print(referenceLine[7])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 78.1077, 108.7354,  91.6977], dtype=torch.float64)\n",
      "tensor([ 77.7615, 108.8643,  91.6973], dtype=torch.float64)\n",
      "tensor([ 77.8808, 108.4698,  91.6927], dtype=torch.float64)\n",
      "tensor([ 78.0768, 109.0850,  91.6268], dtype=torch.float64)\n",
      "tensor([ 77.5148, 108.5987,  91.6375], dtype=torch.float64)\n",
      "tensor([ 78.3125, 108.4477,  91.5947], dtype=torch.float64)\n",
      "tensor([ 77.7489, 109.2253,  91.5590], dtype=torch.float64)\n",
      "tensor([ 77.6419, 108.2420,  91.5700], dtype=torch.float64)\n",
      "tensor([ 78.4297, 108.8083,  91.5614], dtype=torch.float64)\n",
      "tensor([ 77.4379, 108.9717,  91.5654], dtype=torch.float64)\n",
      "tensor([ 78.0813, 108.1849,  91.5566], dtype=torch.float64)\n",
      "tensor([ 78.1025, 109.3932,  91.4139], dtype=torch.float64)\n",
      "tensor([ 77.3094, 108.3452,  91.4445], dtype=torch.float64)\n",
      "tensor([ 78.6147, 108.4838,  91.3841], dtype=torch.float64)\n",
      "tensor([ 77.4417, 109.2998,  91.3783], dtype=torch.float64)\n",
      "tensor([ 77.8415, 107.9787,  91.4083], dtype=torch.float64)\n",
      "tensor([ 78.3907, 109.1557,  91.4638], dtype=torch.float64)\n",
      "tensor([ 77.2027, 108.6996,  91.4375], dtype=torch.float64)\n",
      "tensor([ 78.4256, 108.1555,  91.3717], dtype=torch.float64)\n",
      "tensor([ 77.7865, 109.5055,  91.3048], dtype=torch.float64)\n",
      "tensor([ 77.4984, 108.0024,  91.3116], dtype=torch.float64)\n",
      "tensor([ 78.6872, 108.8408,  91.3203], dtype=torch.float64)\n",
      "tensor([ 77.1729, 109.0570,  91.3086], dtype=torch.float64)\n",
      "tensor([ 78.1746, 107.9095,  91.2645], dtype=torch.float64)\n",
      "tensor([ 78.3889, 109.4422,  91.1813], dtype=torch.float64)\n",
      "tensor([ 77.0434, 108.4461,  91.1692], dtype=torch.float64)\n",
      "tensor([ 78.6965, 108.2442,  91.1141], dtype=torch.float64)\n",
      "tensor([ 77.5030, 109.5370,  91.1019], dtype=torch.float64)\n",
      "tensor([ 77.5000, 107.8270,  90.9938], dtype=torch.float64)\n",
      "tensor([ 78.6388, 109.1633,  91.2107], dtype=torch.float64)\n",
      "tensor([ 76.9888, 108.7771,  91.1266], dtype=torch.float64)\n",
      "tensor([ 78.4668, 107.9465,  91.0478], dtype=torch.float64)\n",
      "tensor([ 78.0655, 109.6183,  91.0848], dtype=torch.float64)\n",
      "tensor([ 77.2055, 108.1323,  91.1605], dtype=torch.float64)\n",
      "tensor([ 78.8286, 108.6225,  91.0812], dtype=torch.float64)\n",
      "tensor([ 77.2105, 109.3476,  91.0498], dtype=torch.float64)\n",
      "tensor([ 77.8390, 107.7826,  91.1087], dtype=torch.float64)\n",
      "tensor([ 78.6632, 109.3198,  90.9072], dtype=torch.float64)\n",
      "tensor([ 76.9030, 108.6561,  90.7911], dtype=torch.float64)\n",
      "tensor([ 78.6834, 108.0832,  90.7689], dtype=torch.float64)\n",
      "tensor([ 77.7496, 109.6751,  90.8953], dtype=torch.float64)\n",
      "tensor([ 77.2161, 107.9836,  90.8505], dtype=torch.float64)\n",
      "tensor([ 78.8374, 108.9642,  90.9474], dtype=torch.float64)\n",
      "tensor([ 77.0033, 109.0777,  90.9567], dtype=torch.float64)\n",
      "tensor([ 78.1431, 107.7515,  90.9129], dtype=torch.float64)\n",
      "tensor([ 78.3585, 109.5817,  90.8446], dtype=torch.float64)\n",
      "tensor([ 76.9926, 108.2972,  90.8376], dtype=torch.float64)\n",
      "tensor([ 78.8549, 108.4211,  90.8108], dtype=torch.float64)\n",
      "tensor([ 77.3852, 109.5591,  90.7525], dtype=torch.float64)\n",
      "tensor([ 77.7615, 107.7120,  90.7482], dtype=torch.float64)\n",
      "tensor([ 77.6912, 108.6687,  89.7427], dtype=torch.float64)\n",
      "tensor([ 78.0374, 108.5397,  89.7432], dtype=torch.float64)\n",
      "tensor([ 77.9181, 108.9343,  89.7477], dtype=torch.float64)\n",
      "tensor([ 77.7221, 108.3191,  89.8136], dtype=torch.float64)\n",
      "tensor([ 78.2841, 108.8053,  89.8029], dtype=torch.float64)\n",
      "tensor([ 77.4863, 108.9563,  89.8458], dtype=torch.float64)\n",
      "tensor([ 78.0500, 108.1788,  89.8814], dtype=torch.float64)\n",
      "tensor([ 78.1570, 109.1620,  89.8705], dtype=torch.float64)\n",
      "tensor([ 77.3692, 108.5958,  89.8791], dtype=torch.float64)\n",
      "tensor([ 78.3610, 108.4323,  89.8751], dtype=torch.float64)\n",
      "tensor([ 77.7176, 109.2191,  89.8838], dtype=torch.float64)\n",
      "tensor([ 77.6964, 108.0109,  90.0266], dtype=torch.float64)\n",
      "tensor([ 78.4895, 109.0588,  89.9960], dtype=torch.float64)\n",
      "tensor([ 77.1842, 108.9202,  90.0563], dtype=torch.float64)\n",
      "tensor([ 78.3572, 108.1042,  90.0621], dtype=torch.float64)\n",
      "tensor([ 77.9574, 109.4254,  90.0322], dtype=torch.float64)\n",
      "tensor([ 77.4082, 108.2484,  89.9767], dtype=torch.float64)\n",
      "tensor([ 78.5962, 108.7045,  90.0029], dtype=torch.float64)\n",
      "tensor([ 77.3733, 109.2486,  90.0688], dtype=torch.float64)\n",
      "tensor([ 78.0123, 107.8986,  90.1357], dtype=torch.float64)\n",
      "tensor([ 78.3005, 109.4017,  90.1289], dtype=torch.float64)\n",
      "tensor([ 77.1116, 108.5632,  90.1201], dtype=torch.float64)\n",
      "tensor([ 78.6259, 108.3471,  90.1318], dtype=torch.float64)\n",
      "tensor([ 77.6242, 109.4945,  90.1760], dtype=torch.float64)\n",
      "tensor([ 77.4100, 107.9618,  90.2592], dtype=torch.float64)\n",
      "tensor([ 78.7555, 108.9580,  90.2712], dtype=torch.float64)\n",
      "75 [ 78.7555186  108.95801267  90.27122457] [ 78.36259  108.80828   90.598434] 0.2838811622505798\n",
      "tensor([ 77.1024, 109.1599,  90.3263], dtype=torch.float64)\n",
      "tensor([ 78.2959, 107.8671,  90.3386], dtype=torch.float64)\n",
      "tensor([ 78.2988, 109.5771,  90.4467], dtype=torch.float64)\n",
      "tensor([ 77.1600, 108.2408,  90.2298], dtype=torch.float64)\n",
      "tensor([ 78.8100, 108.6269,  90.3138], dtype=torch.float64)\n",
      "tensor([ 77.3321, 109.4575,  90.3926], dtype=torch.float64)\n",
      "tensor([ 77.7333, 107.7858,  90.3557], dtype=torch.float64)\n",
      "tensor([ 78.5934, 109.2718,  90.2799], dtype=torch.float64)\n",
      "tensor([ 76.9703, 108.7816,  90.3592], dtype=torch.float64)\n",
      "tensor([ 78.5884, 108.0565,  90.3906], dtype=torch.float64)\n",
      "tensor([ 77.9598, 109.6215,  90.3317], dtype=torch.float64)\n",
      "tensor([ 77.1356, 108.0842,  90.5333], dtype=torch.float64)\n",
      "tensor([ 78.8959, 108.7480,  90.6493], dtype=torch.float64)\n",
      "88 [ 78.89586077 108.74800996  90.64932975] [ 78.36259  108.80828   90.598434] 0.2906038664155419\n",
      "tensor([ 77.1154, 109.3209,  90.6715], dtype=torch.float64)\n",
      "tensor([ 78.0493, 107.7290,  90.5451], dtype=torch.float64)\n",
      "tensor([ 78.5828, 109.4204,  90.5900], dtype=torch.float64)\n",
      "tensor([ 76.9615, 108.4399,  90.4931], dtype=torch.float64)\n",
      "tensor([ 78.7955, 108.3264,  90.4838], dtype=torch.float64)\n",
      "tensor([ 77.6558, 109.6526,  90.5275], dtype=torch.float64)\n",
      "tensor([ 77.4404, 107.8224,  90.5959], dtype=torch.float64)\n",
      "tensor([ 78.8063, 109.1069,  90.6029], dtype=torch.float64)\n",
      "96 [ 78.80625982 109.10688665  90.60289128] [ 78.36259  108.80828   90.598434] 0.28603082585170475\n",
      "tensor([ 76.9440, 108.9829,  90.6297], dtype=torch.float64)\n",
      "tensor([ 78.4137, 107.8450,  90.6879], dtype=torch.float64)\n",
      "tensor([ 78.0373, 109.6921,  90.6923], dtype=torch.float64)\n",
      "100 [ 77.89944  108.702034  90.72022 ] [ 78.36259  108.80828   90.598434] 0.24062869\n"
     ]
    }
   ],
   "source": [
    "state = TractographyState(torch.Tensor([ 77.8994346, 108.7020324, 90.72022516]), env.interpolateDWIatState)\n",
    "for i in range(n_actions):\n",
    "    env.state = state\n",
    "    env.stepCounter -= 1\n",
    "    next_state, _, terminal = env.step(i)\n",
    "    qry_pt = next_state.getCoordinate().view(-1,3)\n",
    "    distance = torch.min(torch.sum((referenceLine[7] - qry_pt)**2, dim=1))\n",
    "    if distance < 0.3:\n",
    "        print(i, next_state.getCoordinate().numpy(), referenceLine[7].numpy(), distance.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(187.0214)\n",
      "[ 73.651344 107.88106   93.29415 ]\n",
      "-1\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "env.reset()\n",
    "next_state, reward, terminal = env.step(100)\n",
    "print(next_state.getCoordinate().numpy())\n",
    "print(reward)\n",
    "print(terminal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[47, 75, 80, 88, 93, 96], [67, 75, 80, 88, 93], [62, 67, 75, 80], [62, 67, 75, 80, 83], [62, 67, 75, 80, 83], [62, 67, 75, 83, 96], [62, 67, 75, 83, 96], [62, 75, 83, 91, 96], [62, 75, 83, 91, 96], [62, 75, 83, 91, 96], [62, 70, 75, 83, 91, 96], [62, 70, 75, 78, 83, 91], [54, 57, 62, 67, 70, 75, 83], [54, 62, 67, 75], [54, 59, 67, 72], [51, 54, 59, 62, 67, 72], [51, 54, 59, 64, 67, 72], [51, 54, 59, 64, 67, 72], [51, 54, 59, 64, 67, 72], [51, 54, 56, 59, 64, 67, 72], [51, 54, 56, 59, 64, 67, 72], [51, 56, 59, 64], [51, 56, 59, 64], [51, 56, 59, 64], [50, 51, 53, 56, 59], [50, 51, 53, 56, 61, 66], [53, 58, 61, 66, 74, 79], [58, 66, 71, 74, 79], [58, 66, 71, 79], [58, 63, 66, 71, 79], [58, 63, 66, 71, 79], [58, 63, 66, 71, 79], [58, 63, 66, 71, 79], [58, 63, 66, 71, 79, 84], [58, 63, 71, 79, 84, 92], [58, 63, 71, 79, 84, 92], [63, 71, 79, 84, 92], [63, 71, 79, 84, 92], [38, 71, 79, 84, 92], [38, 63, 71, 84, 92, 97], [38, 71, 84, 92, 97], [38, 84, 92, 97], [38, 76, 84, 92, 97], [38, 76, 84, 92, 97], [38, 43, 84, 89, 97], [38, 43, 84, 89, 97], [30, 38, 43, 97], [30, 38, 43, 97], [30, 38, 43, 97], [22, 30, 38, 43, 97], [22, 30, 38, 43, 97], [22, 35, 43, 89, 97], [35, 43, 48, 89], [40, 48, 81, 89, 94], [40, 48, 73, 81, 86, 94], [73, 81, 86, 94], [40, 48, 81, 89, 94], [35, 43, 48, 89], [22, 35, 43, 89, 97], [22, 35, 43, 89, 97], [22, 30, 35, 43, 89], [22, 30, 35, 43, 89], [14, 22, 27, 35, 43], [14, 22, 27, 35], [6, 14, 19, 22, 27, 35], [6, 11, 14, 19, 27], [3, 6, 11, 19], [3, 6, 11, 16], [3, 8, 11, 16], [3, 8, 11, 16, 24, 29], [3, 8, 16, 21, 29], [8, 16, 21, 29], [8, 13, 16, 21, 29], [8, 13, 16, 21, 29], [21, 29, 34, 42], [13, 21, 26, 34, 47], [13, 18, 26, 31, 34, 39, 47], [26, 34, 39, 47, 93], [26, 34, 39, 47, 93], [26, 39, 47, 85, 93], [26, 39, 47, 85, 93], [39, 47, 72, 85, 93], [64, 72, 80, 85, 93], [64, 72, 77, 85, 93], [56, 64, 69, 72, 77, 85], [64, 69, 77, 85, 90, 98], [100]]\n"
     ]
    }
   ],
   "source": [
    "print(possible_actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 71.5173, 114.6476,  79.9506])\n",
      "tensor([ 71.7731, 113.9662,  79.6186])\n",
      "[64, 69, 77, 85, 90, 98]\n",
      "tensor([ 71.5173, 114.6476,  79.9506]) tensor(0.4643)\n",
      "tensor([ 71.5173, 114.6476,  79.9506]) tensor(0.4643)\n",
      "tensor([ 71.5173, 114.6476,  79.9506]) tensor(0.4643)\n",
      "tensor([ 71.5173, 114.6476,  79.9506]) tensor(0.4643)\n",
      "tensor([ 71.5173, 114.6476,  79.9506]) tensor(0.4643)\n",
      "tensor([ 71.5173, 114.6476,  79.9506]) tensor(0.4643)\n"
     ]
    }
   ],
   "source": [
    "env.state = TractographyState(referenceLine[85], env.interpolateDWIatState)\n",
    "print(env.state.getCoordinate())\n",
    "print(referenceLine[86])\n",
    "print(possible_actions[85])\n",
    "for i in possible_actions[85]:\n",
    "    env.state = TractographyState(referenceLine[85], env.interpolateDWIatState)\n",
    "    env.stepCounter = 84\n",
    "    next_state, reward, _ = env.step(z)\n",
    "    print(next_state.getCoordinate(), reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 71.773056 113.966225  79.618576] [ 72.30127204 114.02878755  79.99932066] 0.27901215525974304\n",
      "[ 71.773056 113.966225  79.618576] [ 71.7609279  113.6971063   80.14330044] 0.2753356828217112\n",
      "[ 71.773056 113.966225  79.618576] [ 71.37937604 113.65758474  79.97858924] 0.15498393104601757\n",
      "[ 71.773056 113.966225  79.618576] [ 71.66780472 114.12438903  79.11184227] 0.2567791198339029\n",
      "[ 71.773056 113.966225  79.618576] [ 71.97880832 114.37794723  79.10548775] 0.26325960220840583\n",
      "[ 71.773056 113.966225  79.618576] [ 71.31423388 113.95652005  79.25698482] 0.2105177635246191\n",
      "[ 71.773056 113.966225  79.618576] [ 71.97504289 114.04982978  79.29253904] 0.10630013104166292\n",
      "[ 71.773056 113.966225  79.618576] [ 71.63016002 113.84417747  79.3660627 ] 0.06376299386222199\n",
      "[ 71.773056 113.966225  79.618576] [ 72.24376411 114.29266559  79.36222956] 0.2215660937612901\n",
      "[ 71.773056 113.966225  79.618576] [ 71.91375289 113.81268975  79.56895814] 0.023572970787664616\n",
      "[ 71.773056 113.966225  79.618576] [ 71.35118583 113.73139254  79.58605126] 0.1779744651043897\n",
      "[ 71.773056 113.966225  79.618576] [ 72.20619251 114.00206886  79.62102829] 0.18760720625139998\n",
      "[ 71.773056 113.966225  79.618576] [ 71.66712842 113.67455586  79.7755295 ] 0.08507069391326497\n",
      "[ 71.773056 113.966225  79.618576] [ 72.03154546 113.7906186   79.91830767] 0.0898390464981324\n"
     ]
    }
   ],
   "source": [
    "#env.state = TractographyState(referenceLine[85], env.interpolateDWIatState)\n",
    "for i in range(n_actions):\n",
    "    env.reset()\n",
    "    env.state = TractographyState(referenceLine[85], env.interpolateDWIatState)\n",
    "    next_state, reward, _ = env.step(i)\n",
    "    distance = env.rewardForTerminalState(next_state)\n",
    "    if distance < 0.3:\n",
    "        print(referenceLine[86].numpy(), next_state.getCoordinate().numpy(), distance.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 71.7731, 113.9662,  79.6186])\n",
      "tensor(122.0777, dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "distance = env.rewardForTerminalState(next_state)\n",
    "print(referenceLine[86])\n",
    "print(distance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(n_actions):\n",
    "    env.state = TractographyState(torch.FloatTensor([ 74.64776812, 107.9270337, 93.22325858]), env.interpolateDWIatState)\n",
    "    next_state, reward, _ = env.step(i)\n",
    "    env.stepCounter = 2\n",
    "    if reward < 0.1:\n",
    "        reward = 1\n",
    "    elif reward < 0.5:\n",
    "        reward = 0\n",
    "    else:\n",
    "        reward = -1\n",
    "    if reward == 1:\n",
    "        #best_actions.append(i)\n",
    "        print(\"[{}]\".format(i), referenceLine[2].numpy(), next_state.getCoordinate().numpy(), reward)\n",
    "#print(best_actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = TractographyState(torch.FloatTensor(referenceLine[0]), env.interpolateDWIatState)\n",
    "coordinates = state.getCoordinate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(referenceLine[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(referenceLine[0])\n",
    "print(referenceLine[70])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = TractographyState(referenceLine[69], env.interpolateDWIatState)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shape = env.reset().getValue().reshape(-1).shape[0]\n",
    "print(shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = torch.FloatTensor(state.getValue()).unsqueeze(0).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_vals = agent.main_dqn(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(state.getValue().shape)\n",
    "shape = state.getValue().shape\n",
    "shape = np.prod(np.array(shape))\n",
    "print(shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = TractographyState(referenceLine[70], env.interpolateDWIatState)\n",
    "qry_pt = torch.FloatTensor(state.getCoordinate()).view(-1,3)\n",
    "distance = torch.min(torch.sum( (referenceLine - qry_pt)**2, dim =1 ))\n",
    "qry_pt = torch.FloatTensor(state.getCoordinate()).view(3)\n",
    "distance_terminal = torch.sum( (referenceLine[-1,:] - qry_pt)**2 )\n",
    "\n",
    "#print(distance)\n",
    "#print(distance_terminal)\n",
    "reward = (torch.tanh(-distance+5.3) + 2*torch.tanh(-distance_terminal+5.3))/2\n",
    "print(reward)\n",
    "\n",
    "print(torch.tanh(-distance+5.3))\n",
    "print(torch.tanh(-distance_terminal+5.3))\n",
    "\n",
    "reward += 200/20 * reward.sign()\n",
    "print(reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(torch.tanh(-distance_terminal+5.3)+2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = TractographyState([32., 84., 94.], env.interpolateDWIatState)\n",
    "qry_pt = torch.FloatTensor(state.getCoordinate()).view(-1,3)\n",
    "distance = torch.min(torch.sum( (referenceLine - qry_pt)**2, dim =1 ))\n",
    "print(torch.tanh(-distance+5.3))\n",
    "qry_pt = torch.FloatTensor(state.getCoordinate()).view(3)\n",
    "distance = torch.sum( (referenceLine[-1,:] - qry_pt)**2 )\n",
    "print(-distance)\n",
    "print(torch.tanh(-distance)+2)\n",
    "#print(torch.where(distance < env.maxL2dist_to_terminalState, 1, 0 ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(-1.5 + 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qry_pt = torch.FloatTensor(state.getCoordinate()).view(3)\n",
    "distance = torch.sum( (referenceLine[-1,:] - qry_pt)**2 )\n",
    "print(round(-distance.item(),4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Init agent\")\n",
    "#memory = ReplayMemory(size=replay_memory_size)\n",
    "state = env.reset()\n",
    "agent = Agent(n_actions=n_actions, inp_size=state.getValue().shape, device=device, hidden=256, agent_history_length=agent_history_length, memory_size=replay_memory_size, learning_rate=learning_rate)\n",
    "\n",
    "print(\"Init epsilon-greedy action scheduler\")\n",
    "action_scheduler = Action_Scheduler(num_actions=n_actions, max_steps=max_steps, eps_annealing_steps=100000, replay_memory_start_size=replay_memory_size, model=agent.main_dqn)\n",
    "\n",
    "step_counter = 0\n",
    "    \n",
    "eps_rewards = []\n",
    "\n",
    "print(\"Start training...\")\n",
    "while step_counter < max_steps:\n",
    "    epoch_step = 0\n",
    "\n",
    "######## fill memory begins here\n",
    "    while epoch_step < evaluate_every:  # To Do implement evaluation\n",
    "        state = env.reset()\n",
    "        episode_reward_sum = 0\n",
    "        \n",
    "        #fill replay memory while interacting with env\n",
    "        for episode_counter in range(max_episode_length):\n",
    "            # get action with epsilon-greedy strategy       \n",
    "            action = action_scheduler.get_action(step_counter, torch.FloatTensor(state.getValue()).to(device).unsqueeze(0))\n",
    "                    \n",
    "            next_state, reward, terminal = env.step(action)\n",
    "\n",
    "            if reward >= 1:\n",
    "                reward = 10\n",
    "            elif reward > -0.05:\n",
    "                reward = 1\n",
    "            \n",
    "            if episode_counter == max_episode_length-1:\n",
    "                reward = -100\n",
    "                terminal = True\n",
    "            # increase counter\n",
    "            step_counter += 1\n",
    "            epoch_step += 1\n",
    "\n",
    "            # accumulate reward for current episode\n",
    "            episode_reward_sum += reward\n",
    "\n",
    "\n",
    "            agent.replay_memory.add_experience(action=action,\n",
    "                                state=state.getValue(),\n",
    "                                reward=reward,\n",
    "                                new_state=next_state.getValue(),\n",
    "                                terminal=terminal)\n",
    "\n",
    "\n",
    "            state = next_state\n",
    "\n",
    "        \n",
    "\n",
    "            ####### optimization is happening here\n",
    "            if step_counter > replay_memory_size:\n",
    "                loss = agent.optimize()\n",
    "\n",
    "\n",
    "            ####### target network update\n",
    "            if step_counter > replay_memory_size and step_counter % network_update_every == 0:\n",
    "                agent.target_dqn.load_state_dict(agent.main_dqn.state_dict())\n",
    "            \n",
    "            # if episode ended before maximum step\n",
    "            if terminal:\n",
    "                terminal = False\n",
    "                state = env.reset()\n",
    "                break\n",
    "                \n",
    "        eps_rewards.append(episode_reward_sum)\n",
    "        \n",
    "        if len(eps_rewards) % 10 == 0:\n",
    "            with open(path+'/logs/rewards.dat', 'a') as reward_file:\n",
    "                print(\"[{}] {}, {}\".format(len(eps_rewards), step_counter, np.mean(eps_rewards[-100:])), file=reward_file)\n",
    "            print(\"[{}] {}, {}\".format(len(eps_rewards), step_counter, np.mean(eps_rewards[-100:])) )\n",
    "    torch.save(agent.main_dqn.state_dict(), path+'/checkpoints/fibre_agent_{}_reward_{:.2f}.pth'.format(step_counter, np.mean(eps_rewards[-100:])))\n",
    "########## evaluation starting here\n",
    "    eval_rewards = []\n",
    "    for _ in range(eval_runs):\n",
    "        eval_steps = 0\n",
    "        state = env.reset()\n",
    "        eval_episode_reward = 0\n",
    "        while eval_steps < max_episode_length:\n",
    "            action = action_scheduler.get_action(step_counter, torch.FloatTensor(state.getValue()).to(device).unsqueeze(0), evaluation=True)\n",
    "\n",
    "            next_state, reward, terminal = env.step(action)\n",
    "\n",
    "            eval_steps += 1\n",
    "            eval_episode_reward += reward\n",
    "            state = next_state\n",
    "\n",
    "            if terminal:\n",
    "                terminal = False\n",
    "                break\n",
    "\n",
    "        eval_rewards.append(eval_episode_reward)\n",
    "    \n",
    "    print(\"Evaluation score:\", np.mean(eval_rewards))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!mkdir -p 'checkpoints/'\n",
    "#torch.save(agent.main_dqn.state_dict(), 'checkpoints/fiber_agent_{}_reward_{:.2f}.pth'.format(step_counter, np.mean(rewards[-100:])))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CONDA (atari)",
   "language": "python",
   "name": "atari"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
