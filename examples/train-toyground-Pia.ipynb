{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import gym\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "\n",
    "import os, sys\n",
    "sys.path.insert(0,'..')\n",
    "\n",
    "from collections import deque \n",
    "\n",
    "from dfibert.tracker.nn.rl import Agent, Action_Scheduler, DQN\n",
    "import dfibert.envs.RLtractEnvironment as RLTe\n",
    "from dfibert.envs._state import TractographyState\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = RLTe.RLtractEnvironment(stepWidth=0.1, action_space=20, maxL2dist_to_State=0.2, device = 'cpu', pReferenceStreamlines='data/HCP307200_DTI_min40.vtk')\n",
    "#env = RLTe.RLtractEnvironment(stepWidth=0.3, action_space=20, device = 'cpu', pReferenceStreamlines='data/HCP307200_DTI_min40.vtk')\n",
    "n_actions = env.action_space.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_steps = 30000000\n",
    "replay_memory_size = 100000\n",
    "agent_history_length = 1\n",
    "evaluate_every = 200000\n",
    "eval_runs = 5#20\n",
    "network_update_every = 10000\n",
    "start_learning = 10000\n",
    "eps_annealing_steps = 400000\n",
    "\n",
    "max_episode_length = 2000\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "batch_size = 512\n",
    "learning_rate = 0.000001 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = env.reset().getValue()\n",
    "print(state.shape)\n",
    "agent = Agent(n_actions=n_actions, inp_size=state.shape, device=device, hidden=10, gamma=0.99, agent_history_length=agent_history_length, memory_size=replay_memory_size, batch_size=batch_size, learning_rate=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#### Fill replay memory with perfect actions for supervised approach\n",
    "\n",
    "from tqdm import trange\n",
    "state = env.reset().getValue()\n",
    "agent = Agent(n_actions=n_actions, inp_size=state.shape, device=device, hidden=10, gamma=0.99, agent_history_length=agent_history_length, memory_size=replay_memory_size, batch_size=512, learning_rate=learning_rate)\n",
    "\n",
    "overall_runs = 0\n",
    "overall_reward = []\n",
    "for overall_runs in trange(60):\n",
    "    state = env.reset(streamline_index=overall_runs)\n",
    "    #episode_step_counter = 0\n",
    "    episode_reward = 0\n",
    "    terminal = False\n",
    "    #print(\"New run\")\n",
    "    #print(env.stepCounter, state.getCoordinate().numpy())\n",
    "    while not terminal:\n",
    "        #print(env.stepCounter)\n",
    "        #if np.random.rand(1) < 0.1: \n",
    "        #    action = np.random.randint(0, n_actions)\n",
    "        #else:\n",
    "        action = env._get_best_action()\n",
    "        next_state, reward, terminal, _ = env.step(action)\n",
    "        \n",
    "            \n",
    "        agent.replay_memory.add_experience(action=action,\n",
    "                                state = state.getValue(),\n",
    "                                reward=reward,\n",
    "                                new_state = next_state.getValue(),\n",
    "                                terminal=terminal)\n",
    "        \n",
    "        episode_reward += reward\n",
    "        \n",
    "        state = next_state\n",
    "        \n",
    "        if terminal == True:\n",
    "            break\n",
    "            \n",
    "    overall_runs += 1\n",
    "    overall_reward.append(episode_reward)\n",
    "    print(overall_runs, np.mean(overall_reward[-100:]))\n",
    "print(\"Replay memory ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.optim.Adam(agent.target_dqn.parameters(), 0.0001)\n",
    "losses = []\n",
    "for i in trange(70000):\n",
    "    states, actions, _, _, _ = agent.replay_memory.get_minibatch()\n",
    "\n",
    "    states = torch.FloatTensor(states).to(agent.device)\n",
    "    actions = torch.LongTensor(actions).to(agent.device)\n",
    "    predicted_q = agent.main_dqn(states)\n",
    "    loss = torch.nn.functional.cross_entropy(predicted_q, actions)\n",
    "    #print(loss.item())\n",
    "    agent.optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    agent.optimizer.step()\n",
    "    losses.append(loss.item())\n",
    "    \n",
    "mean_losses = []\n",
    "for i in range(len(losses)):\n",
    "    mean_losses.append(np.mean(losses[i:i+99]))\n",
    "#print(mean_losses[-20:])\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(range(len(losses[:])), losses[:])\n",
    "ax.plot(range(len(losses[:])), mean_losses[:])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "states, actions, _, _, _ = agent.replay_memory.get_minibatch()\n",
    "states = torch.FloatTensor(states).to(agent.device)\n",
    "predicted_q = torch.argmax(agent.main_dqn(states), dim=1)\n",
    "\n",
    "false = 0\n",
    "for i in range(len(actions)):\n",
    "    if predicted_q[i] != actions[i]:\n",
    "        false += 1 \n",
    "    \n",
    "print(\"Accuracy =\", 1 - false / len(actions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "step_counter = 0\n",
    "eps_rewards = []\n",
    "episode_lengths = []\n",
    "\n",
    "eps = 1.0\n",
    "\n",
    "print(\"Start training...\")\n",
    "while step_counter < max_steps:\n",
    "    epoch_step = 0\n",
    "    while (epoch_step < evaluate_every) or (step_counter < start_learning):\n",
    "        state = env.reset()\n",
    "        episode_reward_sum = 0\n",
    "        terminal = False\n",
    "        episode_step_counter = 0\n",
    "        positive_run = 0\n",
    "        points_visited = 0\n",
    "        \n",
    "        negative_rewards = 0\n",
    "        \n",
    "        \n",
    "        # reduce epsilon\n",
    "        if step_counter > start_learning:\n",
    "            eps = max(eps * 0.999, 0.01)\n",
    "        \n",
    "        # play an episode\n",
    "        while episode_step_counter <= 1000.:\n",
    "            \n",
    "            # get an action with epsilon-greedy strategy\n",
    "            if random.random() < eps:                                 \n",
    "                action = np.random.randint(env.action_space.n)           # either random action\n",
    "                #action = env._get_best_action()\n",
    "            else:                                                        # or action from agent\n",
    "                agent.main_dqn.eval()\n",
    "                with torch.no_grad():\n",
    "                    state_v = torch.from_numpy(state.getValue()).unsqueeze(0).float().to(device)\n",
    "                    action = torch.argmax(agent.main_dqn(state_v)).item()\n",
    "                agent.main_dqn.train()\n",
    "            \n",
    "            # perform step on environment\n",
    "            next_state, reward, terminal, _ = env.step(action)\n",
    "\n",
    "            \n",
    "            episode_step_counter += 1\n",
    "            step_counter += 1\n",
    "            epoch_step += 1\n",
    "            \n",
    "            episode_reward_sum += reward\n",
    "            \n",
    "            # store experience in replay buffer\n",
    "            agent.replay_memory.add_experience(action=action, state = state.getValue(), reward=reward, new_state = next_state.getValue(), terminal=terminal)\n",
    "            \n",
    "            state = next_state\n",
    "            \n",
    "            # optimize agent after certain amount of steps\n",
    "            if step_counter > start_learning and step_counter % 4 == 0:\n",
    "                \n",
    "                # original optimization function\n",
    "                #agent.optimize()\n",
    "                \n",
    "                ### debugging optimization function\n",
    "                \n",
    "                states, actions, rewards, new_states, terminal_flags = agent.replay_memory.get_minibatch()\n",
    "                \n",
    "                #states = torch.tensor(states)#.view(replay_memory.batch_size, -1) # 1, -1\n",
    "                #next_states = torch.tensor(new_states)#.view(replay_memory.batch_size, -1)\n",
    "                #actions = torch.LongTensor(actions)\n",
    "                #rewards = torch.tensor(rewards)\n",
    "                #terminal_flags = torch.BoolTensor(terminal_flags)\n",
    "\n",
    "                states = torch.from_numpy(states).to(device)\n",
    "                next_states = torch.from_numpy(new_states).to(device)\n",
    "                actions = torch.from_numpy(actions).unsqueeze(1).long().to(device)\n",
    "                rewards = torch.from_numpy(rewards).to(device)\n",
    "                terminal_flags = torch.from_numpy(terminal_flags).to(device)\n",
    "                \n",
    "                \n",
    "                state_action_values = agent.main_dqn(states).gather(1, actions).squeeze(-1)\n",
    "                next_state_actions = torch.argmax(agent.main_dqn(next_states), dim=1)\n",
    "                next_state_values = agent.target_dqn(next_states).gather(1, next_state_actions.unsqueeze(-1)).squeeze(-1)\n",
    "                #\n",
    "                next_state_values[terminal_flags] = 0.0\n",
    "                #\n",
    "                expected_state_action_values = next_state_values.detach() * 0.9995 + rewards\n",
    "                #\n",
    "                loss = F.smooth_l1_loss(state_action_values, expected_state_action_values)\n",
    "                agent.optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                agent.optimizer.step()\n",
    "                \n",
    "            # update target network after certain amount of steps    \n",
    "            if step_counter > start_learning and step_counter % network_update_every == 0:\n",
    "                agent.target_dqn.load_state_dict(agent.main_dqn.state_dict())\n",
    "            \n",
    "            # if epsiode has ended, step out of the episode while loop\n",
    "            if terminal:\n",
    "                break\n",
    "                \n",
    "        # keep track of past episode rewards\n",
    "        eps_rewards.append(episode_reward_sum)\n",
    "        if len(eps_rewards) % 20 == 0:\n",
    "            print(\"{}, done {} episodes, {}, current eps {}\".format(step_counter, len(eps_rewards), np.mean(eps_rewards[-100:]), eps))#action_scheduler.eps_current))\n",
    "            \n",
    "    ## evaluation        \n",
    "    eval_rewards = []\n",
    "    episode_final = 0\n",
    "    agent.main_dqn.eval()\n",
    "    for _ in range(eval_runs):\n",
    "        eval_steps = 0\n",
    "        state = env.reset()\n",
    "        \n",
    "        eval_episode_reward = 0\n",
    "        negative_rewards = 0\n",
    "        \n",
    "        # play an episode\n",
    "        while eval_steps < 1000:\n",
    "            # get the action from the agent\n",
    "            with torch.no_grad():\n",
    "                    state_v = torch.from_numpy(state.getValue()).unsqueeze(0).float().to(device)\n",
    "                    action = torch.argmax(agent.main_dqn(state_v)).item()\n",
    "                  \n",
    "            # perform a step on the environment\n",
    "            next_state, reward, terminal, _ = env.step(action)\n",
    "            \n",
    "            eval_steps += 1\n",
    "            \n",
    "            eval_episode_reward += reward\n",
    "            state = next_state\n",
    "            \n",
    "            # step out of the episode while loop if \n",
    "            if terminal:\n",
    "                terminal = False\n",
    "                if reward == 1.:\n",
    "                    episode_final += 1\n",
    "                break\n",
    "\n",
    "        eval_rewards.append(eval_episode_reward)\n",
    "\n",
    "    print(\"Evaluation score:\", np.mean(eval_rewards))\n",
    "    print(\"{} of {} episodes ended close to / at the final state.\".format(episode_final, eval_runs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluating trained agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_rewards = []\n",
    "all_distances = []\n",
    "all_states = []\n",
    "#agent.main_dqn.eval()\n",
    "for _ in range(1):\n",
    "    eval_steps = 0\n",
    "    state = env.reset(streamline_index=50)    \n",
    "    #state = env.reset()\n",
    "    #print(state.getCoordinate())\n",
    "    all_states.append(state.getCoordinate())\n",
    "    #transition = init_transition()\n",
    "    #all_states.append(torch.tensor(list(transition)[:3]))\n",
    "    eval_episode_reward = 0\n",
    "    episode_final = 0\n",
    "    #print(env.referenceStreamline_ijk[:6])\n",
    "    \n",
    "    while eval_steps < max_episode_length:\n",
    "        #action = torch.argmax(main_dqn(torch.FloatTensor(state.getValue()).unsqueeze(0).to(device)))\n",
    "        #action = env._get_best_action()\n",
    "        with torch.no_grad():\n",
    "            state_v = torch.from_numpy(state.getValue()).unsqueeze(0).float().to(device)\n",
    "            action = torch.argmax(agent.main_dqn(state_v)).item()\n",
    "        next_state, reward, terminal, _ = env.step(action)\n",
    "        \n",
    "        eval_episode_reward += reward\n",
    "        print(eval_steps, action, next_state.getCoordinate().numpy(), env.referenceStreamline_ijk[np.min([env.stepCounter,len(env.referenceStreamline_ijk)-1])].numpy(), reward)       \n",
    "        eval_steps += 1\n",
    "        if eval_steps == 1000:\n",
    "            terminal = True\n",
    "        all_distances.append(reward)\n",
    "        all_states.append(next_state.getCoordinate())\n",
    "        \n",
    "        state = next_state\n",
    "        if terminal:\n",
    "            terminal = False\n",
    "            break\n",
    "\n",
    "    eval_rewards.append(eval_episode_reward)\n",
    "\n",
    "print(\"Evaluation score:\", np.min(eval_rewards))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "### vizualise streamline\n",
    "states = torch.stack(all_states)\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = plt.axes(projection='3d')\n",
    "ax.plot3D(env.referenceStreamline_ijk.T[0][:], env.referenceStreamline_ijk.T[1][:], env.referenceStreamline_ijk.T[2][:])\n",
    "ax.plot3D(states.T[0][:], states.T[1][:], states.T[2][:])"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "767d51c1340bd893661ea55ea3124f6de3c7a262a8b4abca0554b478b1e2ff90"
  },
  "kernelspec": {
   "display_name": "CONDA (atari)",
   "language": "python",
   "name": "atari"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
