{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import gym\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "import os, sys\n",
    "sys.path.insert(0,'..')\n",
    "\n",
    "from collections import deque \n",
    "\n",
    "from dfibert.tracker.nn.rl import Agent, Action_Scheduler, DQN\n",
    "import dfibert.envs.RLtractEnvironment as RLTe\n",
    "from dfibert.envs._state import TractographyState\n",
    "\n",
    "class Object(object):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test on Lunar Lander to check functionality of agent\n",
    "#env = gym.make('LunarLander-v2')\n",
    "#n_actions= env.action_space.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_steps = 30000000\n",
    "replay_memory_size = 60000\n",
    "agent_history_length = 1\n",
    "evaluate_every = 200000\n",
    "eval_runs = 5#20\n",
    "network_update_every = 10000\n",
    "start_learning = 20000\n",
    "eps_annealing_steps = 10000000\n",
    "\n",
    "max_episode_length = 2000\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "batch_size = 64\n",
    "learning_rate = 0.0000000625\n",
    "#batch_size = 512\n",
    "#learning_rate = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = RLTe.RLtractEnvironment(stepWidth=0.1, action_space=20, device = 'cpu')\n",
    "#env = RLTe.RLtractEnvironment(stepWidth=0.3, action_space=20, device = 'cpu', pReferenceStreamlines='data/HCP307200_DTI_min40.vtk')\n",
    "n_actions = env.action_space.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = env.reset()\n",
    "print(state)\n",
    "#print(state.getValue().flatten().shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = env.reset(streamline_index=0)\n",
    "#env.stepCounter += 1\n",
    "#state = env.state = TractographyState(env.referenceStreamline_ijk[1], env.interpolateDWIatState)\n",
    "best_actions = []\n",
    "    #path_vectors = []\n",
    "    #reference_vectors = []\n",
    "    #cosine_sims = []\n",
    "    #distances = []\n",
    "rewards = []\n",
    "all_states = []\n",
    "all_states.append(state.getCoordinate())\n",
    "for i in range(n_actions):\n",
    "    #print(state.getCoordinate(), env.state.getCoordinate())\n",
    "    #print(env.stepCounter)\n",
    "    next_state, reward,_,_ = env.step(i)\n",
    "    all_states.append(next_state.getCoordinate())\n",
    "    all_states.append(state.getCoordinate())\n",
    "    rewards.append(reward)\n",
    "    #print(reward)\n",
    "    best_actions.append(reward)\n",
    "    env.state = state\n",
    "    env.stepCounter -= 1\n",
    "best_action= torch.argmax(torch.tensor(best_actions))\n",
    "#return best_action, rewards[best_action]\n",
    "print(best_action, float(rewards[best_action]))\n",
    "#print(np.argmin(rewards))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading precomputed streamlines (data/HCP307200_DTI_smallSet.vtk) for ID 100307\n"
     ]
    }
   ],
   "source": [
    "env = RLTe.RLtractEnvironment(stepWidth=0.1, action_space=20, maxL2dist_to_State=0.1, device = 'cpu')\n",
    "\n",
    "def get_best_action(state, env):\n",
    "    rewards = []\n",
    "    points_before = env.points_visited\n",
    "    for i in range(n_actions):\n",
    "        next_state, reward,_, _ = env.step(i)\n",
    "        env.stepCounter -= 1\n",
    "        if env.points_visited > points_before:\n",
    "            env.points_visited = points_before\n",
    "        rewards.append(env.l2_distance)\n",
    "        env.state = state\n",
    "\n",
    "    best_action= torch.argmin(torch.tensor(rewards))\n",
    "    return best_action, rewards[best_action]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_actions = env.action_space.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n"
     ]
    }
   ],
   "source": [
    "state = env.reset(streamline_index=0)\n",
    "all_top_actions = []\n",
    "all_top_rewards = []\n",
    "terminal = False\n",
    "\n",
    "states = Object()\n",
    "states.x = [state.getCoordinate()[0]]\n",
    "states.y = [state.getCoordinate()[1]]\n",
    "states.z = [state.getCoordinate()[2]]\n",
    "i = 0\n",
    "while (terminal != True) and (i < 100):\n",
    "    best_actions, best_rewards = get_best_action(state, env)\n",
    "    all_top_actions.append(best_actions)\n",
    "    all_top_rewards.append(best_rewards)\n",
    "    next_state, reward, terminal, _ = env.step(best_actions)\n",
    "    state = next_state\n",
    "    i += 1\n",
    "    print(i)\n",
    "    states.x.append(state.getCoordinate()[0])\n",
    "    states.y.append(state.getCoordinate()[1])\n",
    "    states.z.append(state.getCoordinate()[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "no_timesteps = 3\n",
    "fig = plt.figure()\n",
    "ax = plt.axes(projection='3d')\n",
    "#ax.plot3D(env.referenceStreamline_ijk.T[0][0:no_timesteps], env.referenceStreamline_ijk.T[1][0:no_timesteps], env.referenceStreamline_ijk.T[2][0:no_timesteps])\n",
    "ax.plot3D(states.x, states.y, states.z)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transfer learning\n",
    "Fill replay memory with perfect actions for supervised approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "action, _ = get_best_action(state,env)\n",
    "next_state, reward, terminal, _ = env.step(action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dfibert.tracker import StreamlinesFromFileTracker\n",
    "\n",
    "file_sl = StreamlinesFromFileTracker(env.pReferenceStreamlines)\n",
    "file_sl.track()\n",
    "\n",
    "tracked_streamlines = file_sl.get_streamlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from tqdm import trange\n",
    "state = env.reset().getValue()\n",
    "agent = Agent(n_actions=n_actions, inp_size=state.shape, device=device, hidden=10, gamma=0.99, agent_history_length=agent_history_length, memory_size=replay_memory_size, batch_size=512, learning_rate=learning_rate)\n",
    "\n",
    "overall_runs = 0\n",
    "overall_reward = []\n",
    "for overall_runs in trange(15):\n",
    "    state = env.reset(streamline_index=overall_runs)\n",
    "    episode_reward = 0\n",
    "    terminal = False\n",
    "    for i in range(1000):#while not terminal:\n",
    "        action, _ = get_best_action(state,env)\n",
    "        next_state, reward, terminal, _ = env.step(action)\n",
    "            \n",
    "        agent.replay_memory.add_experience(action=action,\n",
    "                                state = state.getValue(),\n",
    "                                reward=reward,\n",
    "                                new_state = next_state.getValue(),\n",
    "                                terminal=terminal)\n",
    "        \n",
    "        episode_reward += reward\n",
    "        \n",
    "        state = next_state\n",
    "        \n",
    "        if terminal == True:\n",
    "            break\n",
    "            \n",
    "    overall_runs += 1\n",
    "    overall_reward.append(episode_reward)\n",
    "    print(overall_runs, np.mean(overall_reward[-100:]))\n",
    "print(\"Replay memory ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save memory\n",
    "import h5py\n",
    "hf = h5py.File('memory_dti.hdf5', 'w')\n",
    "hf.create_dataset('states', data=agent.replay_memory.states[:agent.replay_memory.count])\n",
    "hf.create_dataset('new_states', data=agent.replay_memory.new_states[:agent.replay_memory.count])\n",
    "hf.create_dataset('actions', data=agent.replay_memory.actions[:agent.replay_memory.count])\n",
    "hf.create_dataset('rewards', data=agent.replay_memory.rewards[:agent.replay_memory.count])\n",
    "hf.create_dataset('terminals', data=agent.replay_memory.terminal_flags[:agent.replay_memory.count])\n",
    "hf.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load memory\n",
    "#state = #env.reset().getValue()\n",
    "agent_learn = Agent(n_actions=20, inp_size=(642, 3, 3, 3), device=device, hidden=10, gamma=0.95, agent_history_length=agent_history_length, memory_size=replay_memory_size, batch_size=512, learning_rate=learning_rate)\n",
    "hf = h5py.File('memory_dti.hdf5', 'r')\n",
    "agent_learn.replay_memory.states = np.array(hf[\"states\"][:60000])\n",
    "agent_learn.replay_memory.new_states = np.array(hf[\"new_states\"][:60000])\n",
    "agent_learn.replay_memory.actions = np.array(hf[\"actions\"][:60000])\n",
    "agent_learn.replay_memory.rewards = np.array(hf[\"rewards\"][:60000])\n",
    "agent_learn.replay_memory.terminal_flags = np.array(hf[\"terminals\"][:60000])\n",
    "hf.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(agent_learn.replay_memory.states.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_learn.replay_memory.count = 60000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_learn = Agent(n_actions=101, inp_size=(100, 3, 3, 3), device=device, hidden=10, gamma=0.95, agent_history_length=agent_history_length, memory_size=replay_memory_size, batch_size=512, learning_rate=learning_rate)\n",
    "hf = h5py.File('memory_dti.hdf5', 'r')\n",
    "agent_learn.replay_memory.states = np.array(hf[\"states\"][:])\n",
    "agent_learn.replay_memory.new_states = np.array(hf[\"new_states\"][:])\n",
    "agent_learn.replay_memory.actions = np.array(hf[\"actions\"][:])\n",
    "agent_learn.replay_memory.rewards = np.array(hf[\"rewards\"][:])\n",
    "agent_learn.replay_memory.terminal_flags = np.array(hf[\"terminals\"][:])\n",
    "print(agent_learn.replay_memory.states.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_learn.replay_memory.rewards[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_learn.replay_memory.count = 30000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.optimizer = torch.optim.Adam(agent.main_dqn.parameters(), 0.00001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Supervised learning on perfect memory --> worked\n",
    "\n",
    "state = env.reset().getValue()\n",
    "agent_learn = Agent(n_actions=n_actions, inp_size=state.shape, device=device, hidden=10, gamma=0.99, agent_history_length=agent_history_length, memory_size=replay_memory_size, batch_size=64, learning_rate=0.00001)\n",
    "losses = []\n",
    "\n",
    "with trange(50000, unit=\"epochs\") as pbar:\n",
    "    for i in pbar:\n",
    "    \n",
    "        states, actions, _, _, _ = agent.replay_memory.get_minibatch()\n",
    "\n",
    "        states = torch.FloatTensor(states).to(agent.device)\n",
    "        actions = torch.LongTensor(actions).to(agent.device)\n",
    "\n",
    "        predicted_q = agent.main_dqn(states)\n",
    "        loss = torch.nn.functional.cross_entropy(predicted_q, actions)\n",
    "        #print(loss.item())\n",
    "        agent.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        agent.optimizer.step()\n",
    "        losses.append(loss.item())\n",
    "\n",
    "        pbar.set_postfix(loss=loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_losses = []\n",
    "for i in range(len(losses)):\n",
    "    mean_losses.append(np.mean(losses[i:i+99]))\n",
    "print(mean_losses[-20:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "fig, ax = plt.subplots()\n",
    "plt.plot(range(len(losses[:])), losses[:])\n",
    "plt.plot(range(len(losses[:])), mean_losses[:])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.target_dqn.load_state_dict(agent.main_dqn.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "states, actions, _, _, _ = agent.replay_memory.get_minibatch()\n",
    "states = torch.FloatTensor(states).to(agent.device)\n",
    "predicted_q = torch.argmax(agent.main_dqn(states), dim=1)\n",
    "\n",
    "false = 0\n",
    "for i in range(len(actions)):\n",
    "    if predicted_q[i] != actions[i]:\n",
    "        false += 1 \n",
    "    \n",
    "print(\"Accuracy =\", 1 - false / len(actions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [],
   "source": [
    "#### test performance of pretrained agent\n",
    "\n",
    "state = env.reset(streamline_index=0)\n",
    "terminal = False\n",
    "all_states = []\n",
    "all_states.append(state.getCoordinate())\n",
    "\n",
    "\n",
    "states = Object()\n",
    "states.x = [state.getCoordinate()[0]]\n",
    "states.y = [state.getCoordinate()[1]]\n",
    "states.z = [state.getCoordinate()[2]]\n",
    "\n",
    "#while not terminal:\n",
    "for i in range(100):\n",
    "    action, optimal_reward  = get_best_action(state, env)\n",
    "    next_state, reward, terminal, _ = env.step(action)\n",
    "    #print(\"Step counter: \", env.stepCounter)\n",
    "    #current_index = np.min([env.stepCounter,len(env.referenceStreamline_ijk)-1])\n",
    "    #print(\"Current index: \", current_index)\n",
    "    #print(\"state.getCoordinate(): \", state.getCoordinate().numpy())\n",
    "    #print(\"env.state.getCoordinate(): \", env.state.getCoordinate().numpy())\n",
    "    #print(\"next_state.getCoordinate(): \", next_state.getCoordinate().numpy())\n",
    "    #path_vector = next_state.getCoordinate() - state.getCoordinate().squeeze(0)\n",
    "    #print(\"path vector: \", path_vector)\n",
    "    #reference_vector = env.referenceStreamline_ijk[current_index]-env.referenceStreamline_ijk[current_index-1]\n",
    "    #print(\"reference_vector: \", reference_vector)\n",
    "    #cosine_sim = F.cosine_similarity(path_vector, reference_vector, dim=0)\n",
    "    #print(\"cosine_sim: \", cosine_sim)\n",
    "    #dist = torch.dist(env.referenceStreamline_ijk[current_index], next_state.getCoordinate(), p=2)#torch.sum((env.referenceStreamline_ijk[current_index] - next_state.getCoordinate())**2)\n",
    "    #dist_past = torch.dist(env.referenceStreamline_ijk[current_index], state.getCoordinate(), p=2)\n",
    "    #if dist > 0.3:\n",
    "    #    env.stepCounter -= 1\n",
    "    #reward = 0.\n",
    "    #    #positive_run += 1 \n",
    "    #if dist > 2.:\n",
    "    #    reward = -1.\n",
    "\n",
    "    #if dist > 0.26:\n",
    "    #    env.stepCounter -= 1\n",
    "        #if dist < dist_past:\n",
    "        #    reward = 0.5\n",
    "    #else:\n",
    "    #    reward = 1.\n",
    "    #print(\"distance: \", dist)\n",
    "    all_states.append(next_state.getCoordinate())\n",
    "    state = next_state\n",
    "    \n",
    "    states.x.append(state.getCoordinate()[0])\n",
    "    states.y.append(state.getCoordinate()[1])\n",
    "    states.z.append(state.getCoordinate()[2])\n",
    "    \n",
    "    print(env.stepCounter, action, reward)#cosine_sim.item(), dist.item(), 1-(optimal_reward-(cosine_sim-dist)))\n",
    "    #if action == 100 and 1-(optimal_reward-(cosine_sim-dist)) == 1:\n",
    "    #    terminal = True\n",
    "    #else:\n",
    "    #    terminal = False\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validation of our agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#for i in range(len(states)):\n",
    "#    print(states[i], env.referenceStreamline_ijk[i])\n",
    "#    distance = ((states.T[0][i] - env.referenceStreamline_ijk.T[0][i])**2 \\\n",
    "#                      + (states.T[1][i] - env.referenceStreamline_ijk.T[1][i] )**2 \\\n",
    "#                      + (states.T[2][i] - env.referenceStreamline_ijk.T[2][i])**2)\n",
    "#    print(distance)\n",
    "no_timesteps = 5\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = plt.axes(projection='3d')\n",
    "ax.plot3D(env.referenceStreamline_ijk.T[0][0:no_timesteps], env.referenceStreamline_ijk.T[1][0:no_timesteps], env.referenceStreamline_ijk.T[2][0:no_timesteps])\n",
    "ax.plot3D(states.x, states.y, states.z)\n",
    "#plt.legend('gt','agent')\n",
    "#print(optimal_steps[:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(all_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = env.reset().getValue()\n",
    "agent = Agent(n_actions=n_actions, inp_size=state.shape, device=device, hidden=10, gamma=0.99, agent_history_length=agent_history_length, memory_size=replay_memory_size, batch_size=512, learning_rate=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.main_dqn.load_state_dict(torch.load(\"defi_pretrained_95pacc.pth\"))\n",
    "agent.target_dqn.load_state_dict(agent.main_dqn.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(state.shape)\n",
    "print(agent.main_dqn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "#### DQN training\n",
    "\n",
    "#uncomment next 2 lines for regular DQN trainig without pretrained network\n",
    "#state = env.reset().getValue()\n",
    "#agent = Agent(n_actions=n_actions, inp_size=state.shape, device=device, hidden=10, gamma=0.95, agent_history_length=agent_history_length, memory_size=replay_memory_size, batch_size=batch_size, learning_rate=learning_rate)\n",
    "\n",
    "#transition = init_transition()\n",
    "#agent = Agent(n_actions=n_actions, inp_size=np.array(transition).shape, device=device, hidden=10, agent_history_length=agent_history_length, memory_size=replay_memory_size, batch_size=batch_size, learning_rate=learning_rate)\n",
    "action_scheduler = Action_Scheduler(num_actions=n_actions, max_steps=max_steps, eps_initial=0.5, eps_annealing_steps=eps_annealing_steps, eps_final=0.1, eps_final_step=0.02, replay_memory_start_size=start_learning, model=agent.main_dqn)\n",
    "\n",
    "step_counter = 0\n",
    "\n",
    "eps_rewards = []\n",
    "\n",
    "episode_lengths = []\n",
    "\n",
    "cos = torch.nn.CosineSimilarity(dim=0)\n",
    "\n",
    "print(\"Start training...\")\n",
    "while step_counter < max_steps:\n",
    "    epoch_step = 0\n",
    "    #agent.main_dqn.train()\n",
    "######## fill memory begins here\n",
    "    while (epoch_step < evaluate_every) or (step_counter < start_learning):\n",
    "        state = env.reset()\n",
    "        env.stepCounter = np.random.randint(len(env.referenceStreamline_ijk)-10)\n",
    "        env.state = TractographyState(env.referenceStreamline_ijk[env.stepCounter], env.interpolateDWIatState)\n",
    "        #transition = init_transition()\n",
    "        #referenceLine = env.referenceStreamline_ijk\n",
    "        episode_reward_sum = 0\n",
    "        terminal = False\n",
    "        #fill replay memory while interacting with env\n",
    "        #for episode_counter in range(max_episode_length):\n",
    "        episode_step_counter = 0\n",
    "        positive_run = 0\n",
    "        points_visited = 0\n",
    "        \n",
    "        dist = 0\n",
    "        #influential_action = None\n",
    "        while not terminal:\n",
    "            # get action with epsilon-greedy strategy\n",
    "            #if dist < 0.1:\n",
    "            #_, optimal_reward = get_best_action(state, env)\n",
    "               #print(influential_action)\n",
    "            #else:\n",
    "            #    influential_action = None\n",
    "            action = action_scheduler.get_action(step_counter, torch.FloatTensor(state.getValue()).unsqueeze(0).to(device)) #influential_action=influential_action)\n",
    "            #action = action_scheduler.get_action(step_counter, torch.FloatTensor([np.array(transition)]).to(device))\n",
    "            #print(\"Before step: \", env.stepCounter)\n",
    "            \n",
    "            next_state, reward, terminal = env.step(action)\n",
    "            episode_step_counter += 1\n",
    "            #print(episode_step_counter, action, reward, optimal_reward, torch.tanh(1-(optimal_reward - reward)))\n",
    "            #print(\"After step: \", env.stepCounter)\n",
    "            \n",
    "            #if reward < -1.:\n",
    "            #    reward = -1.\n",
    "            \n",
    "            #terminal = False\n",
    "            \n",
    "            #current_index = np.min([env.stepCounter,len(env.referenceStreamline_ijk)-1])\n",
    "            #path_vector = (next_state.getCoordinate() - state.getCoordinate()).squeeze(0)\n",
    "            #reference_vector = env.referenceStreamline_ijk[current_index]-env.referenceStreamline_ijk[current_index-1]\n",
    "            ##    #print(path_vector, reference_vector)\n",
    "            #cosine_sim = cos(path_vector, reference_vector)\n",
    "            #dist = torch.sum((env.referenceStreamline_ijk[current_index] - next_state.getCoordinate())**2)\n",
    "            #reward = -torch.dist(env.referenceStreamline_ijk[current_index], next_state.getCoordinate(), p=2)\n",
    "            #if reward == 0.:\n",
    "            #    reward = 1.\n",
    "            \n",
    "            #if reward < -0.05:\n",
    "            #    env.stepCounter -= 1\n",
    "            \n",
    "            #reward = torch.tanh(1- (optimal_reward - reward))\n",
    "            #if reward >= 0.76:\n",
    "            #    reward = 1.\n",
    "            #elif reward < 0.:\n",
    "            #    reward = -1.\n",
    "            #else:\n",
    "            #    reward = 0.\n",
    "            current_index = np.min([env.stepCounter,len(env.referenceStreamline_ijk)-1])\n",
    "            dist = torch.dist(env.referenceStreamline_ijk[current_index], next_state.getCoordinate(), p=2)\n",
    "            dist_past = torch.dist(env.referenceStreamline_ijk[current_index], state.getCoordinate(), p=2)\n",
    "            #if dist <= 0.09:\n",
    "            #    reward = 1.\n",
    "            #elif dist < 0.25:\n",
    "            #    reward = 0.5\n",
    "            #elif dist < 1.:\n",
    "            #    reward = 0.25\n",
    "            #elif dist_past < dist:\n",
    "            #    reward = -1\n",
    "            #    #env.stepCounter -= 1\n",
    "            #else:\n",
    "            #    reward = 0.\n",
    "                #env.stepCounter -= 1\n",
    "            \n",
    "            #if action == 19:\n",
    "            #    if dist <= 0.09:\n",
    "            #        reward = 1.\n",
    "            #    else:\n",
    "            #        reward = -1.\n",
    "                    \n",
    "            #if reward > 0.:\n",
    "            #    positive_run += 1\n",
    "            #print(\"Before test for dist: \", env.stepCounter, \"Dist: \", dist)\n",
    "            #print(\"After test for dist: \", env.stepCounter)\n",
    "            #if dist < dist_past:\n",
    "            reward = 0.\n",
    "                #positive_run += 1 \n",
    "            if dist > 2.:\n",
    "                reward = -1.\n",
    "                \n",
    "            if dist > 0.25:\n",
    "                env.stepCounter -= 1\n",
    "                #if dist < dist_past:\n",
    "                    #reward = 0.5\n",
    "                    #positive_run += 1\n",
    "            else:\n",
    "                points_visited += 1\n",
    "                reward = 1.\n",
    "                positive_run += 1\n",
    "                print(\"Reached referencePoint {} at step {}\".format(points_visited, episode_step_counter))\n",
    "                #env.stepCounter -= 1\n",
    "            #if reward < -5.0:\n",
    "            #    reward = -5.0\n",
    "            #if reward < -100:\n",
    "            #    reward = -100\n",
    "            #if dist < 0.1:\n",
    "            #    dist = 0\n",
    "            #else:\n",
    "            #    dist = dist - 0.1\n",
    "            #if dist > 3*0.81:\n",
    "            #    env.stepCounter -= 1\n",
    "            #reward = cosine_sim - dist\n",
    "            #reward = 1 - (optimal_reward - reward)\n",
    "            #reward = 1- (optimal_reward - dist)\n",
    "            #if reward == optimal_reward:\n",
    "            #    reward = 1\n",
    "            #if action == 100 and dist < 0.1:\n",
    "            #    terminal = True\n",
    "            #print(\"From function: \", influential_action, optimal_reward)\n",
    "            #print(\"From scheduler: \", action, reward,  terminal)\n",
    "            #print(\"Cosine sim: \", cosine_sim)\n",
    "            #print(\"Dist: \", dist)\n",
    "            \n",
    "            #if episode_step_counter >= 200:\n",
    "            #    terminal = True\n",
    "            \n",
    "            #print(episode_step_counter, action, reward, terminal)\n",
    "            #print(reward)\n",
    "            #if dist > 0.7: # cosine_sim < 0.4 or\n",
    "            #    terminal = True\n",
    "            #next_state = next_state[:2]\n",
    "            #next_transition = add_to_transition(next_state, transition)\n",
    "            \n",
    "            step_counter += 1\n",
    "            epoch_step += 1\n",
    "\n",
    "            # accumulate reward for current episode\n",
    "            episode_reward_sum += reward\n",
    "\n",
    "\n",
    "            agent.replay_memory.add_experience(action=action,\n",
    "                                #state=np.array(transition),\n",
    "                                state = state.getValue(),\n",
    "                                reward=reward,\n",
    "                                #new_state=np.array(next_transition),\n",
    "                                new_state = next_state.getValue(),\n",
    "                                terminal=terminal)\n",
    "\n",
    "\n",
    "            state = next_state\n",
    "            #transition = next_transition\n",
    "\n",
    "\n",
    "\n",
    "            ####### optimization is happening here\n",
    "            if step_counter > start_learning and step_counter % 4 == 0:\n",
    "                #if reward > 0.:\n",
    "                #    print(\"reward was positive: \", reward)\n",
    "                loss = agent.optimize()\n",
    "\n",
    "\n",
    "            ####### target network update\n",
    "            if step_counter > start_learning and step_counter % network_update_every == 0:\n",
    "                #print(\"Update net\")\n",
    "                #print(agent.main_dqn(torch.tensor(state).to(device).unsqueeze(0)))\n",
    "                #print(agent.target_dqn(torch.tensor(state).to(device).unsqueeze(0)))\n",
    "                agent.target_dqn.load_state_dict(agent.main_dqn.state_dict())\n",
    "\n",
    "            # if episode ended before maximum step\n",
    "            if episode_step_counter >= 1000:\n",
    "                terminal = True\n",
    "            if terminal:\n",
    "                terminal = False\n",
    "                episode_lengths.append(episode_step_counter)\n",
    "                #state = env.reset()[:2]\n",
    "                #transition = init_transition()\n",
    "                break\n",
    "\n",
    "        eps_rewards.append(episode_reward_sum)\n",
    "\n",
    "        if len(eps_rewards) % 1 == 0:\n",
    "            #with open(path+'/logs/rewards.dat', 'a') as reward_file:\n",
    "                #print(\"[{}] {}, {}\".format(len(eps_rewards), step_counter, np.mean(eps_rewards[-100:])), file=reward_file)\n",
    "            print(\"{}, done {} episodes, {}, current eps {}\".format(step_counter, len(eps_rewards), np.mean(eps_rewards[-100:]), action_scheduler.eps_current), np.mean(episode_lengths[-100:]), positive_run, points_visited)\n",
    "    #torch.save(agent.main_dqn.state_dict(), path+'/checkpoints/fibre_agent_{}_reward_{:.2f}.pth'.format(step_counter, np.mean(eps_rewards[-100:])))\n",
    "\n",
    "########## evaluation starting here\n",
    "    eval_rewards = []\n",
    "    episode_final = 0\n",
    "    #agent.main_dqn.eval()\n",
    "    for _ in range(eval_runs):\n",
    "        eval_steps = 0\n",
    "        state = env.reset()\n",
    "        #transition = init_transition()\n",
    "        #env.state = TractographyState(env.referenceStreamline_ijk[0], env.interpolateDWIatState)\n",
    "        #env.stepCounter = 0\n",
    "        \n",
    "        eval_episode_reward = 0\n",
    "        while eval_steps < 1000:\n",
    "            #_, optimal_reward = get_best_action(state, env)\n",
    "            action = action_scheduler.get_action(step_counter, torch.FloatTensor(state.getValue()).unsqueeze(0).to(device), evaluation=True)\n",
    "            #action = action_scheduler.get_action(step_counter, torch.FloatTensor([np.array(transition)]).to(device), evaluation=True)\n",
    "            next_state, reward, terminal = env.step(action)\n",
    "            \n",
    "            eval_steps += 1\n",
    "            \n",
    "            #if reward < -0.05:\n",
    "            #    env.stepCounter -= 1\n",
    "            #reward = 1 - (optimal_reward-reward)\n",
    "            #if reward >= 0.76:\n",
    "            #    reward = 1\n",
    "            #elif reward < 0.:\n",
    "            #    reward = -1.\n",
    "            #else:\n",
    "            #    reward = 0.\n",
    "            current_index = np.min([env.stepCounter,len(env.referenceStreamline_ijk)-1])\n",
    "            dist = torch.dist(env.referenceStreamline_ijk[current_index], next_state.getCoordinate(), p=2)\n",
    "            dist_past = torch.dist(env.referenceStreamline_ijk[current_index], state.getCoordinate(), p=2)\n",
    "            \n",
    "            reward = 0.\n",
    "                #positive_run += 1 \n",
    "            if dist > 2.:\n",
    "                reward = -1.\n",
    "                \n",
    "            if dist > 0.25:\n",
    "                env.stepCounter -= 1\n",
    "                #if dist < dist_past:\n",
    "                #    reward = 0.5\n",
    "            else:\n",
    "                points_visited += 1\n",
    "                reward = 1.\n",
    "                positive_run += 1\n",
    "        \n",
    "            #if dist < dist_past:\n",
    "            #    reward = 1.\n",
    "            #else:\n",
    "            #    reward = -1.\n",
    "            #    \n",
    "            #if dist > 0.1:\n",
    "            #    env.stepCounter -= 1\n",
    "            #else:\n",
    "            #    reward += 4    \n",
    "            \n",
    "            #if dist <= 0.09:\n",
    "            #    reward = 1.\n",
    "            #elif dist < 0.25:\n",
    "            #    reward = 0.5\n",
    "            #elif dist < 1.:\n",
    "            #    reward = 0.25\n",
    "            #elif dist_past < dist:\n",
    "            #    reward = -1\n",
    "                #env.stepCounter -= 1\n",
    "            #else:\n",
    "            #    reward = 0.\n",
    "                #env.stepCounter -= 1\n",
    "            \n",
    "            #if action == 19:\n",
    "            #    if dist <= 0.09:\n",
    "            #        reward = 1.\n",
    "            #    else:\n",
    "            #        reward = -1.\n",
    "                    \n",
    "            #if reward != 1.:\n",
    "            #    env.stepCounter -= 1\n",
    "    \n",
    "\n",
    "            \n",
    "            #if reward < -5.0:\n",
    "            #    reward = -5.0\n",
    "            #if reward < -100:\n",
    "            #    reward = -100\n",
    "\n",
    "            #if reward < -1.:\n",
    "            #    reward = -1.\n",
    "            #terminal = False\n",
    "            #current_index = np.min([env.stepCounter,len(env.referenceStreamline_ijk)-1])\n",
    "            #path_vector = (next_state.getCoordinate() - state.getCoordinate()).squeeze(0)\n",
    "            #reference_vector = env.referenceStreamline_ijk[current_index]-env.referenceStreamline_ijk[current_index-1]\n",
    "            #    #print(path_vector, reference_vector)\n",
    "            #cosine_sim = cos(path_vector, reference_vector)\n",
    "            #dist = torch.sum((env.referenceStreamline_ijk[current_index] - next_state.getCoordinate())**2)\n",
    "            #if dist < 0.1:\n",
    "            #    dist = 0\n",
    "            #else:\n",
    "            #    dist = dist - 0.1\n",
    "            #if dist > 3*0.81:\n",
    "            #    env.stepCounter -= 1\n",
    "            #reward = cosine_sim - dist\n",
    "            #reward = 1- (optimal_reward - reward)\n",
    "            #if reward == optimal_reward:\n",
    "            #    reward = 1\n",
    "            #if action == 100 and env.rewardForTerminalState(next_state) < 0.1:\n",
    "            #    terminal = True\n",
    "\n",
    "            #if episode_step_counter == 200:\n",
    "            #    terminal = True\n",
    "            \n",
    "            #if cosine_sim < 0.9:\n",
    "            #    terminal = True\n",
    "            \n",
    "            eval_episode_reward += reward\n",
    "            state = next_state\n",
    "            #transition = next_transition\n",
    "            if terminal:\n",
    "                terminal = False\n",
    "                if reward == 1.:\n",
    "                    print(reward)\n",
    "                    episode_final += 1\n",
    "                break\n",
    "\n",
    "        eval_rewards.append(eval_episode_reward)\n",
    "\n",
    "    print(\"Evaluation score:\", np.mean(eval_rewards))\n",
    "    print(\"{} of {} episodes ended close to / at the final state.\".format(episode_final, eval_runs))\n",
    "    #if np.mean(eval_rewards) > 500.:\n",
    "    #torch.save(agent.main_dqn.state_dict(), 'checkpoints/defi_{}_reward_{:.2f}.pth'.format(step_counter, np.mean(eval_rewards)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(agent.main_dqn.state_dict(), 'defi_pretrained_95pacc.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(path_vector.shape)\n",
    "print(reference_vector.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = env.reset()\n",
    "\n",
    "q_vals = agent.main_dqn(torch.FloatTensor(state.getValue()).unsqueeze(0).to(device))\n",
    "print(q_vals[0][80])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "defi = DQN(n_actions=n_actions, input_shape=513).to(device)\n",
    "action_scheduler = Action_Scheduler(num_actions=n_actions, max_steps=max_steps, eps_annealing_steps=eps_annealing_steps, eps_final=0.1, eps_final_step=0.02, replay_memory_start_size=start_learning, model=defi)\n",
    "\n",
    "defi.load_state_dict(torch.load('high_gamma/checkpoints/defi_487575_reward_1.00.pth'))\n",
    "defi.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.main_dqn(torch.FloatTensor(state.getValue()).unsqueeze(0).to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [],
   "source": [
    "eval_rewards = []\n",
    "all_distances = []\n",
    "all_states = []\n",
    "#agent.main_dqn.eval()\n",
    "for _ in range(1):\n",
    "    eval_steps = 0\n",
    "    state = env.reset(streamline_index=2)    \n",
    "    #state = env.reset()\n",
    "    #print(state.getCoordinate())\n",
    "    all_states.append(state.getCoordinate())\n",
    "    #transition = init_transition()\n",
    "    #all_states.append(torch.tensor(list(transition)[:3]))\n",
    "    eval_episode_reward = 0\n",
    "    episode_final = 0\n",
    "    #print(env.referenceStreamline_ijk[:6])\n",
    "    \n",
    "    while eval_steps < max_episode_length:\n",
    "        action = torch.argmax(agent.main_dqn(torch.FloatTensor(state.getValue()).unsqueeze(0).to(device)))\n",
    "        #action = action_scheduler.get_action(eval_steps, torch.FloatTensor(state.getValue()).unsqueeze(0).to(device), evaluation=True)\n",
    "        #action = action_scheduler.get_action(step_counter, torch.FloatTensor([np.array(transition)]).to(device), evaluation=True)\n",
    "        #action = torch.argmax(agent(torch.FloatTensor([np.array(transition)]).to(device)))\n",
    "        next_state, reward, terminal = env.step(action)\n",
    "        \n",
    "        \n",
    "        #_, optimal_reward = get_best_action(state, env)\n",
    "        #if reward < -0.05:\n",
    "        #        env.stepCounter -= 1\n",
    "        #    \n",
    "        #reward = torch.tanh(1- (optimal_reward - reward))\n",
    "        #if reward >= 0.76:\n",
    "        #    reward = 1.\n",
    "        #elif reward < 0.1:\n",
    "        #    reward = -1.\n",
    "        #else:\n",
    "        #    reward = 0.\n",
    "        current_index = np.min([env.stepCounter,len(env.referenceStreamline_ijk)-1])\n",
    "        dist = torch.dist(env.referenceStreamline_ijk[current_index], next_state.getCoordinate(), p=2)\n",
    "        dist_past = torch.dist(env.referenceStreamline_ijk[current_index], state.getCoordinate(), p=2)\n",
    "\n",
    "\n",
    "        reward = 0.\n",
    "        #positive_run += 1 \n",
    "        if dist > 2.:\n",
    "            reward = -1.\n",
    "\n",
    "        if dist > 0.25:\n",
    "            env.stepCounter -= 1\n",
    "            #if dist < dist_past:\n",
    "            #    reward = 0.5\n",
    "        else:\n",
    "            reward = 1.\n",
    "\n",
    "        #current_index = np.min([env.stepCounter,len(env.referenceStreamline_ijk)-1])\n",
    "        #path_vector = (next_state.getCoordinate() - state.getCoordinate()).squeeze(0)\n",
    "        #reference_vector = env.referenceStreamline_ijk[current_index]-env.referenceStreamline_ijk[current_index-1]\n",
    "        #    #print(path_vector, reference_vector)\n",
    "        #cosine_sim = cos(path_vector, reference_vector)\n",
    "        #dist = torch.sum((env.referenceStreamline_ijk[current_index] - next_state.getCoordinate())**2) * 10\n",
    "        #reward = cosine_sim - dist\n",
    "        #reward = 1 - (optimal_reward - reward)\n",
    "        #if dist > 3*0.81:\n",
    "        #    env.stepCounter -= 1\n",
    "        #if action == 100 and reward == 1:\n",
    "        #    terminal = False\n",
    "            \n",
    "        #if cosine_sim < 0.7:\n",
    "        #    terminal = True\n",
    "        #next_state = next_state\n",
    "        #next_transition = add_to_transition(next_state, transition)\n",
    "        #reward = 1 + (1+(reward/10))\n",
    "        #if reward > 1:\n",
    "        #    reward = 1\n",
    "        #elif reward > 0.:\n",
    "        #    reward = 0\n",
    "        #else:\n",
    "        #    reward = -1\n",
    "        eval_episode_reward += reward\n",
    "        print(eval_steps, action, next_state.getCoordinate().numpy(), env.referenceStreamline_ijk[np.min([env.stepCounter,len(env.referenceStreamline_ijk)-1])].numpy(), reward)\n",
    "        #print(eval_steps, action, next_state, env.referenceStreamline_ijk[np.min([eval_steps,len(env.referenceStreamline_ijk)-1])].numpy(), reward)\n",
    "        eval_steps += 1\n",
    "        if eval_steps == 1000:\n",
    "            terminal = True\n",
    "        all_distances.append(reward)\n",
    "        all_states.append(next_state.getCoordinate())\n",
    "        #all_states.append(next_state)\n",
    "        \n",
    "        state = next_state\n",
    "        #transition = next_transition\n",
    "        if terminal:\n",
    "            terminal = False\n",
    "            #if reward > 0.9:\n",
    "            #    episode_final += 1\n",
    "            break\n",
    "\n",
    "    eval_rewards.append(eval_episode_reward)\n",
    "\n",
    "print(\"Evaluation score:\", np.min(eval_rewards))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "states, actions, rewards, new_states, terminal_flags = agent.replay_memory.get_minibatch()\n",
    "print(np.array_equal(states[0], new_states[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sphere_dist(nextState):\n",
    "    current_index = np.min([env.stepCounter,len(env.referenceStreamline_ijk)-1])\n",
    "    x_dist = (nextState.getCoordinate()[0] - env.referenceStreamline_ijk[current_index][0]) **2\n",
    "    y_dist = (nextState.getCoordinate()[1] - env.referenceStreamline_ijk[current_index][1]) **2\n",
    "    z_dist = (nextState.getCoordinate()[2] - env.referenceStreamline_ijk[current_index][2]) **2\n",
    "    return x_dist + y_dist + z_dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(agent.gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(n_actions):\n",
    "    state = env.reset()\n",
    "    next_state, reward, done = env.step(i)\n",
    "    s_dist = sphere_dist(next_state)\n",
    "    old_dist = torch.sum((env.referenceStreamline_ijk[env.stepCounter] - next_state.getCoordinate())**2)\n",
    "    if s_dist <= 0.52**2:\n",
    "        print(i, reward, s_dist, old_dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "next_state, reward, done = env.step(75)\n",
    "print(next_state.getCoordinate(), reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(list(transition)[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = env.reset()\n",
    "#referenceLine = env.referenceStreamline_ijk\n",
    "print(state.getCoordinate())\n",
    "#print(referenceLine[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "next_state, reward, done = env.step(74)\n",
    "print(next_state.getCoordinate().numpy(), env.referenceStreamline_ijk[np.min([env.stepCounter, len(referenceLine)])].numpy(), reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimal_steps =  [80, 88, 54, 96, 100, 67, 83, 75, 83, 75, 100, 83, 70, 67, 59, 100, 67, 59, 59, 59, 51, 100, 59, 56, 51, 61, 100, 66, 71, 66, 71, 71, 100, 71, 71, 71, 71, 100, 92, 84, 84, 38, 100, 97, 97, 97, 38, 100, 97, 30, 43, 43, 48, 100, 94, 81, 94, 35, 97, 100, 35, 22, 35, 35, 6, 100, 19, 3, 16, 3, 21, 100, 21, 16, 34, 21, 98, 34, 100, 39, 93, 39, 72, 72, 100, 69, 100]\n",
    "transition = init_transition()\n",
    "referenceLine = env.referenceStreamline_ijk\n",
    "print(len(referenceLine))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#action = action_scheduler.get_action(step_counter, torch.FloatTensor([np.array(transition)]).to(device), evaluation=True)\n",
    "next_state, reward, terminal = env.step(88)\n",
    "next_transition = add_to_transition(next_state, transition)\n",
    "print(action, reward)\n",
    "transition = next_transition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Debugging the reward function\n",
    "referenceLine = env.referenceStreamline_ijk\n",
    "stepCounter = 0\n",
    "maxSteps=200\n",
    "state = env.reset()\n",
    "print(\"State: \", state.getCoordinate().numpy())\n",
    "next_state, _, terminal = env.step(80)\n",
    "print(\"Next State: \", next_state.getCoordinate().numpy())\n",
    "\n",
    "def lineseg_dist(p, a, b):\n",
    "\n",
    "    # normalized tangent vector\n",
    "    d = np.divide(b - a, np.linalg.norm(b - a))\n",
    "\n",
    "    # signed parallel distance components\n",
    "    s = np.dot(a - p, d)\n",
    "    t = np.dot(p - b, d)\n",
    "\n",
    "    # clamped parallel distance\n",
    "    h = np.maximum.reduce([s, t, 0])\n",
    "\n",
    "    # perpendicular distance component\n",
    "    c = np.cross(p - a, d)\n",
    "\n",
    "    return np.hypot(h, np.linalg.norm(c))\n",
    "\n",
    "distance = lineseg_dist(referenceLine[86].numpy(), referenceLine[85].numpy(), referenceLine[86].numpy())\n",
    "print(distance)\n",
    "\n",
    "#print(\"Diff: \", next_state.getCoordinate().numpy()-state.getCoordinate().numpy())\n",
    "#qry_pt = next_state.getCoordinate().view(-1,3)\n",
    "#print(\"Reference next state: \", referenceLine[stepCounter+1])\n",
    "#print(\"Diff to reference state: \", referenceLine[stepCounter+1]-next_state.getCoordinate().numpy())\n",
    "#distance = torch.min(torch.sum((referenceLine[np.min([stepCounter+1, maxSteps-1])] - qry_pt)**2, dim=1))\n",
    "#print(distance)\n",
    "#reward = torch.tanh(-distance+5.3)\n",
    "\n",
    "#if distance == -1:\n",
    "#    reward = 0.5\n",
    "#elif distance < 0.8:\n",
    "#    reward = 1+ (1-distance)\n",
    "#else:\n",
    "#    reward = np.max([1 - distance, -1])\n",
    "#print(reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = np.array([ 75.6, 107.95,  92.22])\n",
    "line = np.array([ 75.78847, 107.96255,  92.28433])\n",
    "\n",
    "print(np.linalg.norm(line - state, 2))\n",
    "\n",
    "sphere_dist = ((state[0] - line[0])**2 + (state[1]-line[1])**2 + (state[2]-line[2])**2)\n",
    "print(sphere_dist)\n",
    "normal_diff = np.sum(state-line)**2\n",
    "print(normal_diff)\n",
    "if sphere_dist < 0.2**2:\n",
    "    print(True)\n",
    "else:\n",
    "    print(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#optimal_steps = [80, 75, 80, 75, 62, 75, 83, 96, 51, 24, 62, 62, 62, 77, 65, 64, 67, 59, 56, 83, 82, 54, 56, 53, 56, 38, 56, 84, 66, 71, 71, 64, 97, 84, 71, 71, 38, 51, 30, 92, 97, 84, 43, 79, 27, 46, 89, 25, 81, 25, 48, 43, 86, 48, 57, 14, 89, 43, 43, 19, 92, 14, 27, 9, 78, 4, 16, 3, 29, 3, 47, 6, 42, 21, 39, 5, 72, 34, 98, 88, 90, 75, 77, 59, 49, 32, 82, 100]\n",
    "eps_reward = 0\n",
    "state = env.reset()\n",
    "for i in optimal_steps:\n",
    "    next_state, reward, terminal = env.step(i)\n",
    "    state = next_state\n",
    "    eps_reward += reward.item()\n",
    "    print(\"Action: \", i, \"Reward: \", reward.item())\n",
    "print(eps_reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(\"Init environment..\")\n",
    "env = RLTe.RLtractEnvironment(device = 'cpu')\n",
    "print(\"..done!\")\n",
    "n_actions = env.action_space.n\n",
    "#print(n_actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "referenceLine = env.referenceStreamline_ijk\n",
    "print(referenceLine.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(referenceLine[0])\n",
    "state = TractographyState(referenceLine[0], env.interpolateDWIatState)\n",
    "print(state.getCoordinate())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "possible_actions = []\n",
    "past_state = env.reset()\n",
    "all_next_states = []\n",
    "for i in range(len(referenceLine)):\n",
    "    best_actions = []\n",
    "    next_states = []\n",
    "    for z in range(n_actions):\n",
    "        env.state = TractographyState(referenceLine[i], env.interpolateDWIatState)\n",
    "        next_state, reward, _ = env.step(z)\n",
    "        env.stepCounter = i\n",
    "        #if reward == -1:\n",
    "        #    reward = 0\n",
    "        #elif reward < 0.2:\n",
    "        if reward > 1.0:\n",
    "            print(\"Step: \", i, \"Action: \", z, \"Distance: \", reward)\n",
    "        #    reward = 1\n",
    "        #elif reward < 1.:\n",
    "        #    reward = 0\n",
    "        #else:\n",
    "        #    reward = -1\n",
    "        #if reward == 1:\n",
    "        #    best_actions.append(z)\n",
    "            #print(i, z, referenceLine[i].numpy(), next_state.getCoordinate().numpy(), reward)\n",
    "    print(i, best_actions)\n",
    "    #print(i, reward)\n",
    "    #if reward > 0.9:\n",
    "    #    best_actions.append(i)\n",
    "    possible_actions.append(best_actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "step_distance = []\n",
    "optimal_steps = []#[100, 80, 75, 80, 75, 100, 62, 75, 83, 75, 83, 100, 83, 62, 67, 51, 67, 100, 59, 59, 59, 100, 59, 56, 51, 56, 66, 100, 66, 71, 71, 79, 58, 100, 71, 71, 84, 71, 100, 92, 84, 92, 97, 100, 97, 38, 97, 43, 38, 100, 43, 43, 89, 48]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "last_state = env.reset()\n",
    "print(len(env.referenceStreamline_ijk))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "steps = 0\n",
    "while len(optimal_steps) < 87:\n",
    "    step_distance = []\n",
    "    for i in range(n_actions):\n",
    "        env.reset()\n",
    "        if len(optimal_steps)>0:\n",
    "            for z in range(len(optimal_steps)):\n",
    "                _,_,_ = env.step(optimal_steps[z])\n",
    "        next_state, _, terminal = env.step(i)\n",
    "        #distance = lineseg_dist(next_state.getCoordinate().numpy(), referenceLine[np.min([len(optimal_steps), 85])].numpy(), referenceLine[np.min([len(optimal_steps)+1, len(referenceLine)-1])].numpy())\n",
    "        #distance = ((next_state.getCoordinate()[0] - env.referenceStreamline_ijk[np.min([env.stepCounter, 58])][0])**2 \\\n",
    "        #              + (next_state.getCoordinate()[1] - env.referenceStreamline_ijk[np.min([env.stepCounter, 58])][1])**2 \\\n",
    "        #              + (next_state.getCoordinate()[2] - env.referenceStreamline_ijk[np.min([env.stepCounter, 58])][2])**2)\n",
    "        current_index = np.min([env.stepCounter, len(env.referenceStreamline_ijk)-1])\n",
    "        qry_pt = next_state.getCoordinate().view(-1,3)\n",
    "        distance = torch.sum((env.referenceStreamline_ijk[current_index] - qry_pt)**2)\n",
    "        \n",
    "        step_distance.append(distance)\n",
    "    optimal_steps.append(np.argmin(step_distance))\n",
    "print(optimal_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Streamline index 0, stepWidth 0.81\n",
    "optimal_steps = [80, 88, 67, 75, 75, 62, 75, 83, 75, 83, 83, 83, 62, 67, 67, 54, 59, 59, 59, 59, 59, 59, 51, 56, 51, 56, 66, 66, 79, 71, 71, 66, 71, 71, 71, 71, 71, 84, 92, 84, 84, 92, 97, 97, 38, 97, 97, 38, 43, 43, 38, 35, 89, 48, 48, 73, 94, 35, 89, 43, 35, 22, 35, 35, 14, 14, 11, 3, 3, 16, 16, 21, 16, 21, 21, 34, 26, 47, 26, 39, 93, 39, 72, 72, 77, 77, 100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Streamline index 4, stepWidth 0.81\n",
    "optimal_steps = [17, 30, 17, 30, 17, 30, 17, 1, 6, 3, 3, 3, 11, 16, 16, 24, 16, 24, 37, 24, 91, 45, 78, 78, 86, 99, 86, 86, 86, 70, 70, 65, 70, 65, 86, 65, 86, 99, 99, 40, 45, 45, 100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Streamline index 4, stepWidth 0.79\n",
    "#optimal_steps =[17, 30, 17, 30, 17, 30, 17, 1, 6, 3, 3, 3, 11, 16, 16, 16, 24, 24, 24, 37, 24, 91, 78, 78, 99, 86, 86, 86, 86, 86, 70, 70, 70, 65, 65, 86, 86, 86, 99, 99, 99, 45, 99]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Streamline index 4, stepWidth 0.78\n",
    "#optimal_steps = [17, 30, 17, 30, 17, 30, 17, 1, 6, 1, 3, 11, 3, 16, 16, 24, 16, 24, 24, 37, 37, 45, 78, 78, 99, 86, 86, 78, 86, 86, 78, 70, 65, 65, 65, 86, 86, 86, 99, 99, 99, 45, 78]\n",
    "print(optimal_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Streamline index 4, stepWidth 0.8\n",
    "#optimal_steps = [17, 30, 17, 30, 17, 30, 17, 1, 6, 3, 3, 3, 11, 16, 16, 16, 24, 24, 37, 24, 37, 78, 45, 78, 86, 86, 86, 86, 86, 78, 70, 65, 70, 65, 86, 65, 86, 86, 99, 32, 99, 45]\n",
    "print(optimal_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Streamline index 2\n",
    "optimal_steps = [3, 3, 100, 6, 3, 11, 3, 6, 100, 11, 6, 11, 3, 100, 19, 3, 11, 6, 16, 100, 3, 3, 3, 2, 11, 7, 18, 100, 15, 7, 2, 2, 100, 0, 10, 0, 2, 0, 100, 0, 0, 3, 0, 11, 100, 3, 3, 3, 3, 100, 3, 3, 1, 11, 100, 3, 3, 3, 16, 11, 100, 16, 29, 16, 42, 100, 21, 29, 21, 42, 21, 100, 34, 21, 13, 26, 100, 23, 18, 23, 31, 100, 44, 31, 31, 44, 44, 100, 44, 31, 36, 98, 15, 100, 23, 23, 44, 15, 44, 100, 15, 28, 15, 20, 36, 100, 20, 28, 20, 20, 100, 28, 28, 36, 49, 28, 100, 36, 49, 49, 90, 100, 49, 95, 95, 49, 46, 49, 38, 36, 25, 100, 28, 28, 33, 36, 41, 100, 20, 28, 12, 20, 20, 100, 7, 15, 7, 20, 10, 25, 100]\n",
    "#print(optimal_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For Streamline with len 44 (index 4)\n",
    "#print(optimal_steps)\n",
    "optimal_steps = [17, 30, 100, 17, 30, 17, 17, 6, 0, 17, 37, 0, 0, 78, 24, 16, 24, 24, 100, 37, 45, 45, 70, 100, 99, 86, 86, 78, 94, 62, 100, 65, 70, 86, 65, 100, 86, 94, 99, 45, 99, 100, 100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with line distance\n",
    "#optimal_steps = [80, 88, 54, 96, 46, 75, 75, 75, 83, 75, 83, 83, 62, 54, 1, 59, 54, 59, 59, 67, 56, 59, 51, 59, 61, 11, 53, 61, 66, 71, 71, 79, 58, 71, 71, 71, 21, 71, 84, 92, 84, 92, 97, 84, 43, 84, 30, 97, 47, 97, 43, 30, 89, 35, 94, 73, 48, 89, 22, 72, 43, 35, 22, 35, 35, 6, 19, 3, 16, 16, 66, 16, 8, 21, 29, 21, 26, 26, 93, 26, 93, 85, 35, 85, 72, 77, 100]\n",
    "optimal_steps = [100, 80, 75, 80, 75, 100, 62, 75, 83, 75, 83, 100, 83, 62, 67, 51, 67, 100, 59, 59, 59, 100, 59, 56, 51, 56, 66, 100, 66, 71, 71, 79, 58, 100, 71, 71, 84, 71, 100, 92, 84, 92, 97, 100, 97, 38, 97, 43, 38, 100, 43, 43, 89, 48, 100, 94, 81, 48, 43, 100, 35, 43, 35, 22, 27, 100, 6, 11, 3, 16, 100, 8, 29, 21, 21, 100, 34, 26, 26, 93, 39, 100, 93, 72, 77, 77, 101]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#optimal_steps = [80, 75, 80, 75, 62, 75, 83, 96, 51, 24, 62, 62, 62, 77, 65, 64, 67, 59, 56, 83, 82, 54, 56, 53, 56, 38, 56, 84, 66, 71, 71, 64, 97, 84, 71, 71, 38, 51, 30, 92, 97, 84, 43, 79, 27, 46, 89, 25, 81, 25, 48, 43, 86, 48, 57, 14, 89, 43, 43, 19, 92, 14, 27, 9, 78, 4, 16, 3, 29, 3, 47, 6, 42, 21, 39, 5, 72, 34, 98, 88, 90, 75, 77, 59, 49, 32, 82]\n",
    "print(optimal_steps) # <-- min of max distance reward streamline 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimal_steps = [80, 75, 80, 75, 62, 75, 83, 96, 51, 24, 62, 62, 62, 77, 65, 64, 67, 59, 56, 83, 82, 54, 56, 53, 56, 38, 56, 84, 66, 71, 71, 64, 97, 84, 71, 71, 38, 51, 30, 92, 97, 84, 43, 79, 27, 46, 89, 25, 81, 25, 48, 43, 86, 48, 57, 14, 89, 43, 43, 19, 92, 14, 27, 9, 78, 4, 16, 3, 29, 3, 47, 6, 42, 21, 39, 5, 72, 34, 98, 88, 90, 75, 77, 59, 49, 32, 82]\n",
    "print(optimal_steps) # <-- min of max distance reward streamline 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimal_steps = [80, 88, 54, 96, 59, 37, 54, 37, 67, 91, 62, 78, 64, 70, 64, 62, 69, 83, 56, 59, 59, 53, 42, 53, 53, 56, 79, 58, 87, 60, 87, 58, 92, 52, 46, 58, 38, 58, 30, 58, 38, 84, 17, 55, 30, 76, 25, 76, 17, 76, 17, 81, 30, 86, 48, 65, 27, 97, 14, 84, 6, 97, 14, 89, 3, 27, 8, 19, 13, 19, 13, 29, 8, 96, 2, 88, 31, 47, 26, 59, 5, 72, 72, 85, 61, 39]\n",
    "print(optimal_steps) # <-- min reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change optimal steps\n",
    "optimal_steps = [17, 30, 100, 17, 30, 17, 17, 6, 0, 6, 0, 0, 0, 78, 24, 16, 24, 24, 100, 37, 45, 45, 70, 100, 99, 86, 86, 78, 94, 62, 100, 65, 70, 86, 65, 100, 86, 94, 99, 45, 99, 100, 100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Object(object):\n",
    "    pass\n",
    "\n",
    "state = env.reset()\n",
    "print(env.state.getCoordinate().numpy(), env.referenceStreamline_ijk[0])\n",
    "step = 1\n",
    "all_distances = []\n",
    "all_states = []\n",
    "len_line = len(env.referenceStreamline_ijk)-1\n",
    "all_states.append(state.getCoordinate())\n",
    "\n",
    "for i in optimal_steps:\n",
    "    next_state, reward, terminal = env.step(i)\n",
    "    #print(step, reward)\n",
    "    #current_index = np.min([env.points_visited+1,len(env.referenceStreamline_ijk)-1])\n",
    "    #print(\"Reference Line at current index: \", env.referenceStreamline_ijk[current_index])\n",
    "    #distance = lineseg_dist(next_state.getCoordinate().numpy(), referenceLine[step-1].numpy(), referenceLine[np.min([step, len(referenceLine)-1])].numpy())\n",
    "    #distance = 2 + (distance/10)\n",
    "    #distance = ((next_state.getCoordinate()[0] - env.referenceStreamline_ijk[np.min([env.stepCounter, len_line])][0])**2 \\\n",
    "    #                  + (next_state.getCoordinate()[1] - env.referenceStreamline_ijk[np.min([env.stepCounter, len_line])][1])**2 \\\n",
    "    #                  + (next_state.getCoordinate()[2] - env.referenceStreamline_ijk[np.min([env.stepCounter, len_line])][2])**2)\n",
    "    current_index = np.min([env.stepCounter, len(env.referenceStreamline_ijk)-1])\n",
    "    qry_pt = next_state.getCoordinate().view(-1,3)\n",
    "    distance = torch.sum((env.referenceStreamline_ijk[current_index] - qry_pt)**2)\n",
    "    \n",
    "    print(step, i, next_state.getCoordinate().numpy(), env.referenceStreamline_ijk[np.min([env.stepCounter,len_line])].numpy(), reward, -distance.item())\n",
    "    all_distances.append(distance)\n",
    "    all_states.append(next_state.getCoordinate())\n",
    "    states.x.append(next_state.getCoordinate()[0])\n",
    "    states.y.append(next_state.getCoordinate()[1])\n",
    "    states.z.append(next_state.getCoordinate()[2])\n",
    "    #if distance < 0.71:\n",
    "    #    reward = 1 - distance\n",
    "    #    #print(reward)\n",
    "    #    if reward < 0.3:\n",
    "    #        reward = 1\n",
    "    step += 1\n",
    "\n",
    "print(np.min(all_distances), np.max(all_distances), np.sum(all_distances))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(env.referenceStreamline_ijk[4])\n",
    "print(env.referenceStreamline_ijk.T[1][4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, x in enumerate(all_states):\n",
    "    try:\n",
    "        print(x, env.referenceStreamline_ijk[i])\n",
    "    except IndexError:\n",
    "        print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#for i in range(len(states)):\n",
    "#    print(states[i], env.referenceStreamline_ijk[i])\n",
    "#    distance = ((states.T[0][i] - env.referenceStreamline_ijk.T[0][i])**2 \\\n",
    "#                      + (states.T[1][i] - env.referenceStreamline_ijk.T[1][i] )**2 \\\n",
    "#                      + (states.T[2][i] - env.referenceStreamline_ijk.T[2][i])**2)\n",
    "#    print(distance)\n",
    "\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = plt.axes(projection='3d')\n",
    "ax.plot3D(env.referenceStreamline_ijk.T[0][:], env.referenceStreamline_ijk.T[1][:], env.referenceStreamline_ijk.T[2][:])\n",
    "ax.plot3D(states.T[0][:], states.T[1][:], states.T[2][:])\n",
    "#print(optimal_steps[:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "states = torch.stack(all_states)\n",
    "print(states.T[0][:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(all_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(referenceLine[86])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.reset()\n",
    "print(env.state.getCoordinate().numpy(), referenceLine[0])\n",
    "step = 0\n",
    "#all_rewards = []\n",
    "eps_reward = 0\n",
    "for i in optimal_steps:\n",
    "    next_state, distance, terminal = env.step(i)\n",
    "    if distance < 0.71:\n",
    "        reward = 1 - distance\n",
    "        #print(reward)\n",
    "        if reward < 0.3:\n",
    "            reward = 1\n",
    "    eps_reward += reward\n",
    "    #all_rewards.append(reward)\n",
    "    step += 1\n",
    "print(eps_reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(n_actions):\n",
    "    \n",
    "    next_state, distance, terminal = env.step(i)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.reset()\n",
    "terminal = False\n",
    "step = 0\n",
    "actions = []\n",
    "past_state = env.state\n",
    "step = 1\n",
    "while terminal != True:\n",
    "    for i in range(n_actions)\n",
    "    action = np.random.randint(n_actions)\n",
    "    next_state, reward, terminal = env.step(action)\n",
    "    if reward < 1:\n",
    "        actions.append(action)\n",
    "        past_state = next_state\n",
    "        print(\"Action: \", action, \"Step: \",step, \"Coordinates: \", next_state.getCoordinate().numpy(), referenceLine[step].numpy())\n",
    "        step += 1\n",
    "    else:\n",
    "        env.state = past_state\n",
    "        env.stepCounter = step\n",
    "    #action = np.random.choice(possible_actions[step])\n",
    "    #next_state, reward, terminal = env.step(action)\n",
    "    #step += 1\n",
    "\n",
    "print(actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(referenceLine[6])\n",
    "print(referenceLine[7])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = TractographyState(torch.Tensor([ 77.8994346, 108.7020324, 90.72022516]), env.interpolateDWIatState)\n",
    "for i in range(n_actions):\n",
    "    env.state = state\n",
    "    env.stepCounter -= 1\n",
    "    next_state, _, terminal = env.step(i)\n",
    "    qry_pt = next_state.getCoordinate().view(-1,3)\n",
    "    distance = torch.min(torch.sum((referenceLine[7] - qry_pt)**2, dim=1))\n",
    "    if distance < 0.3:\n",
    "        print(i, next_state.getCoordinate().numpy(), referenceLine[7].numpy(), distance.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.reset()\n",
    "next_state, reward, terminal = env.step(100)\n",
    "print(next_state.getCoordinate().numpy())\n",
    "print(reward)\n",
    "print(terminal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(possible_actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.state = TractographyState(referenceLine[85], env.interpolateDWIatState)\n",
    "print(env.state.getCoordinate())\n",
    "print(referenceLine[86])\n",
    "print(possible_actions[85])\n",
    "for i in possible_actions[85]:\n",
    "    env.state = TractographyState(referenceLine[85], env.interpolateDWIatState)\n",
    "    env.stepCounter = 84\n",
    "    next_state, reward, _ = env.step(z)\n",
    "    print(next_state.getCoordinate(), reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#env.state = TractographyState(referenceLine[85], env.interpolateDWIatState)\n",
    "for i in range(n_actions):\n",
    "    env.reset()\n",
    "    env.state = TractographyState(referenceLine[85], env.interpolateDWIatState)\n",
    "    next_state, reward, _ = env.step(i)\n",
    "    distance = env.rewardForTerminalState(next_state)\n",
    "    if distance < 0.3:\n",
    "        print(referenceLine[86].numpy(), next_state.getCoordinate().numpy(), distance.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distance = env.rewardForTerminalState(next_state)\n",
    "print(referenceLine[86])\n",
    "print(distance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(n_actions):\n",
    "    env.state = TractographyState(torch.FloatTensor([ 74.64776812, 107.9270337, 93.22325858]), env.interpolateDWIatState)\n",
    "    next_state, reward, _ = env.step(i)\n",
    "    env.stepCounter = 2\n",
    "    if reward < 0.1:\n",
    "        reward = 1\n",
    "    elif reward < 0.5:\n",
    "        reward = 0\n",
    "    else:\n",
    "        reward = -1\n",
    "    if reward == 1:\n",
    "        #best_actions.append(i)\n",
    "        print(\"[{}]\".format(i), referenceLine[2].numpy(), next_state.getCoordinate().numpy(), reward)\n",
    "#print(best_actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = TractographyState(torch.FloatTensor(referenceLine[0]), env.interpolateDWIatState)\n",
    "coordinates = state.getCoordinate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(referenceLine[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(referenceLine[0])\n",
    "print(referenceLine[70])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = TractographyState(referenceLine[69], env.interpolateDWIatState)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shape = env.reset().getValue().reshape(-1).shape[0]\n",
    "print(shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = torch.FloatTensor(state.getValue()).unsqueeze(0).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_vals = agent.main_dqn(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(state.getValue().shape)\n",
    "shape = state.getValue().shape\n",
    "shape = np.prod(np.array(shape))\n",
    "print(shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = TractographyState(referenceLine[70], env.interpolateDWIatState)\n",
    "qry_pt = torch.FloatTensor(state.getCoordinate()).view(-1,3)\n",
    "distance = torch.min(torch.sum( (referenceLine - qry_pt)**2, dim =1 ))\n",
    "qry_pt = torch.FloatTensor(state.getCoordinate()).view(3)\n",
    "distance_terminal = torch.sum( (referenceLine[-1,:] - qry_pt)**2 )\n",
    "\n",
    "#print(distance)\n",
    "#print(distance_terminal)\n",
    "reward = (torch.tanh(-distance+5.3) + 2*torch.tanh(-distance_terminal+5.3))/2\n",
    "print(reward)\n",
    "\n",
    "print(torch.tanh(-distance+5.3))\n",
    "print(torch.tanh(-distance_terminal+5.3))\n",
    "\n",
    "reward += 200/20 * reward.sign()\n",
    "print(reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(torch.tanh(-distance_terminal+5.3)+2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = TractographyState([32., 84., 94.], env.interpolateDWIatState)\n",
    "qry_pt = torch.FloatTensor(state.getCoordinate()).view(-1,3)\n",
    "distance = torch.min(torch.sum( (referenceLine - qry_pt)**2, dim =1 ))\n",
    "print(torch.tanh(-distance+5.3))\n",
    "qry_pt = torch.FloatTensor(state.getCoordinate()).view(3)\n",
    "distance = torch.sum( (referenceLine[-1,:] - qry_pt)**2 )\n",
    "print(-distance)\n",
    "print(torch.tanh(-distance)+2)\n",
    "#print(torch.where(distance < env.maxL2dist_to_terminalState, 1, 0 ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(-1.5 + 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qry_pt = torch.FloatTensor(state.getCoordinate()).view(3)\n",
    "distance = torch.sum( (referenceLine[-1,:] - qry_pt)**2 )\n",
    "print(round(-distance.item(),4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Init agent\")\n",
    "#memory = ReplayMemory(size=replay_memory_size)\n",
    "state = env.reset()\n",
    "agent = Agent(n_actions=n_actions, inp_size=state.getValue().shape, device=device, hidden=256, agent_history_length=agent_history_length, memory_size=replay_memory_size, learning_rate=learning_rate)\n",
    "\n",
    "print(\"Init epsilon-greedy action scheduler\")\n",
    "action_scheduler = Action_Scheduler(num_actions=n_actions, max_steps=max_steps, eps_annealing_steps=100000, replay_memory_start_size=replay_memory_size, model=agent.main_dqn)\n",
    "\n",
    "step_counter = 0\n",
    "    \n",
    "eps_rewards = []\n",
    "\n",
    "print(\"Start training...\")\n",
    "while step_counter < max_steps:\n",
    "    epoch_step = 0\n",
    "\n",
    "######## fill memory begins here\n",
    "    while epoch_step < evaluate_every:  # To Do implement evaluation\n",
    "        state = env.reset()\n",
    "        episode_reward_sum = 0\n",
    "        \n",
    "        #fill replay memory while interacting with env\n",
    "        for episode_counter in range(max_episode_length):\n",
    "            # get action with epsilon-greedy strategy       \n",
    "            action = action_scheduler.get_action(step_counter, torch.FloatTensor(state.getValue()).to(device).unsqueeze(0))\n",
    "                    \n",
    "            next_state, reward, terminal = env.step(action)\n",
    "\n",
    "            if reward >= 1:\n",
    "                reward = 10\n",
    "            elif reward > -0.05:\n",
    "                reward = 1\n",
    "            \n",
    "            if episode_counter == max_episode_length-1:\n",
    "                reward = -100\n",
    "                terminal = True\n",
    "            # increase counter\n",
    "            step_counter += 1\n",
    "            epoch_step += 1\n",
    "\n",
    "            # accumulate reward for current episode\n",
    "            episode_reward_sum += reward\n",
    "\n",
    "\n",
    "            agent.replay_memory.add_experience(action=action,\n",
    "                                state=state.getValue(),\n",
    "                                reward=reward,\n",
    "                                new_state=next_state.getValue(),\n",
    "                                terminal=terminal)\n",
    "\n",
    "\n",
    "            state = next_state\n",
    "\n",
    "        \n",
    "\n",
    "            ####### optimization is happening here\n",
    "            if step_counter > replay_memory_size:\n",
    "                loss = agent.optimize()\n",
    "\n",
    "\n",
    "            ####### target network update\n",
    "            if step_counter > replay_memory_size and step_counter % network_update_every == 0:\n",
    "                agent.target_dqn.load_state_dict(agent.main_dqn.state_dict())\n",
    "            \n",
    "            # if episode ended before maximum step\n",
    "            if terminal:\n",
    "                terminal = False\n",
    "                state = env.reset()\n",
    "                break\n",
    "                \n",
    "        eps_rewards.append(episode_reward_sum)\n",
    "        \n",
    "        if len(eps_rewards) % 10 == 0:\n",
    "            with open(path+'/logs/rewards.dat', 'a') as reward_file:\n",
    "                print(\"[{}] {}, {}\".format(len(eps_rewards), step_counter, np.mean(eps_rewards[-100:])), file=reward_file)\n",
    "            print(\"[{}] {}, {}\".format(len(eps_rewards), step_counter, np.mean(eps_rewards[-100:])) )\n",
    "    torch.save(agent.main_dqn.state_dict(), path+'/checkpoints/fibre_agent_{}_reward_{:.2f}.pth'.format(step_counter, np.mean(eps_rewards[-100:])))\n",
    "########## evaluation starting here\n",
    "    eval_rewards = []\n",
    "    for _ in range(eval_runs):\n",
    "        eval_steps = 0\n",
    "        state = env.reset()\n",
    "        eval_episode_reward = 0\n",
    "        while eval_steps < max_episode_length:\n",
    "            action = action_scheduler.get_action(step_counter, torch.FloatTensor(state.getValue()).to(device).unsqueeze(0), evaluation=True)\n",
    "\n",
    "            next_state, reward, terminal = env.step(action)\n",
    "\n",
    "            eval_steps += 1\n",
    "            eval_episode_reward += reward\n",
    "            state = next_state\n",
    "\n",
    "            if terminal:\n",
    "                terminal = False\n",
    "                break\n",
    "\n",
    "        eval_rewards.append(eval_episode_reward)\n",
    "    \n",
    "    print(\"Evaluation score:\", np.mean(eval_rewards))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!mkdir -p 'checkpoints/'\n",
    "#torch.save(agent.main_dqn.state_dict(), 'checkpoints/fiber_agent_{}_reward_{:.2f}.pth'.format(step_counter, np.mean(rewards[-100:])))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
