{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import gym\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "import os, sys\n",
    "sys.path.insert(0,'..')\n",
    "\n",
    "from collections import deque \n",
    "\n",
    "from dfibert.tracker.nn.rl import Agent, Action_Scheduler, DQN\n",
    "import dfibert.envs.RLtractEnvironment as RLTe\n",
    "from dfibert.envs._state import TractographyState\n",
    "\n",
    "class Object(object):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test on Lunar Lander to check functionality of agent\n",
    "#env = gym.make('LunarLander-v2')\n",
    "#n_actions= env.action_space.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_steps = 30000000\n",
    "replay_memory_size = 60000\n",
    "agent_history_length = 1\n",
    "evaluate_every = 200000\n",
    "eval_runs = 5#20\n",
    "network_update_every = 10000\n",
    "start_learning = 20000\n",
    "eps_annealing_steps = 10000000\n",
    "\n",
    "max_episode_length = 2000\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "batch_size = 64\n",
    "learning_rate = 0.0000000625\n",
    "#batch_size = 512\n",
    "#learning_rate = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading precomputed streamlines (data/HCP307200_DTI_smallSet.vtk) for ID 100307\n"
     ]
    }
   ],
   "source": [
    "env = RLTe.RLtractEnvironment(stepWidth=0.1, action_space=20, device = 'cpu')\n",
    "#env = RLTe.RLtractEnvironment(stepWidth=0.3, action_space=20, device = 'cpu', pReferenceStreamlines='data/HCP307200_DTI_min40.vtk')\n",
    "n_actions = env.action_space.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = env.reset()\n",
    "print(state)\n",
    "#print(state.getValue().flatten().shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = env.reset(streamline_index=0)\n",
    "#env.stepCounter += 1\n",
    "#state = env.state = TractographyState(env.referenceStreamline_ijk[1], env.interpolateDWIatState)\n",
    "best_actions = []\n",
    "    #path_vectors = []\n",
    "    #reference_vectors = []\n",
    "    #cosine_sims = []\n",
    "    #distances = []\n",
    "rewards = []\n",
    "all_states = []\n",
    "all_states.append(state.getCoordinate())\n",
    "for i in range(n_actions):\n",
    "    #print(state.getCoordinate(), env.state.getCoordinate())\n",
    "    #print(env.stepCounter)\n",
    "    next_state, reward,_,_ = env.step(i)\n",
    "    all_states.append(next_state.getCoordinate())\n",
    "    all_states.append(state.getCoordinate())\n",
    "    rewards.append(reward)\n",
    "    #print(reward)\n",
    "    best_actions.append(reward)\n",
    "    env.state = state\n",
    "    env.stepCounter -= 1\n",
    "best_action= torch.argmax(torch.tensor(best_actions))\n",
    "#return best_action, rewards[best_action]\n",
    "print(best_action, float(rewards[best_action]))\n",
    "#print(np.argmin(rewards))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading precomputed streamlines (data/HCP307200_DTI_smallSet.vtk) for ID 100307\n"
     ]
    }
   ],
   "source": [
    "env = RLTe.RLtractEnvironment(stepWidth=0.1, action_space=20, maxL2dist_to_State=0.1, device = 'cpu')\n",
    "\n",
    "def get_best_action(state, env):\n",
    "    distances = []\n",
    "    points_before = env.points_visited\n",
    "    for i in range(n_actions):\n",
    "        next_state, reward,_, _ = env.step(i)\n",
    "        env.stepCounter -= 1\n",
    "        if env.points_visited > points_before:\n",
    "            env.points_visited = points_before\n",
    "        del env.state_history[-1]\n",
    "        distances.append(env.l2_distance)\n",
    "        env.state = state\n",
    "        \n",
    "\n",
    "    best_action= np.argmin(distances)\n",
    "    return best_action, distances[best_action]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_actions = env.action_space.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reached point 2/87 at epsiode step 10\n",
      "Reached point 3/87 at epsiode step 20\n",
      "Reached point 4/87 at epsiode step 31\n",
      "Reached point 5/87 at epsiode step 41\n",
      "Reached point 6/87 at epsiode step 52\n",
      "Reached point 7/87 at epsiode step 62\n",
      "Reached point 8/87 at epsiode step 72\n",
      "Reached point 9/87 at epsiode step 82\n",
      "Reached point 10/87 at epsiode step 92\n",
      "Reached point 11/87 at epsiode step 102\n",
      "Reached point 12/87 at epsiode step 112\n",
      "Reached point 13/87 at epsiode step 120\n",
      "Reached point 14/87 at epsiode step 129\n",
      "Reached point 15/87 at epsiode step 139\n",
      "Reached point 16/87 at epsiode step 148\n",
      "Reached point 17/87 at epsiode step 157\n",
      "Reached point 18/87 at epsiode step 165\n",
      "Reached point 19/87 at epsiode step 174\n",
      "Reached point 20/87 at epsiode step 183\n",
      "Reached point 21/87 at epsiode step 191\n",
      "Reached point 22/87 at epsiode step 200\n",
      "Reached point 23/87 at epsiode step 208\n",
      "Reached point 24/87 at epsiode step 216\n",
      "Reached point 25/87 at epsiode step 224\n",
      "Reached point 26/87 at epsiode step 233\n",
      "Reached point 27/87 at epsiode step 242\n",
      "Reached point 28/87 at epsiode step 251\n",
      "Reached point 29/87 at epsiode step 259\n",
      "Reached point 30/87 at epsiode step 267\n",
      "Reached point 31/87 at epsiode step 275\n",
      "Reached point 32/87 at epsiode step 284\n",
      "Reached point 33/87 at epsiode step 292\n",
      "Reached point 34/87 at epsiode step 300\n",
      "Reached point 35/87 at epsiode step 309\n",
      "Reached point 36/87 at epsiode step 317\n",
      "Reached point 37/87 at epsiode step 326\n",
      "Reached point 38/87 at epsiode step 334\n",
      "Reached point 39/87 at epsiode step 342\n",
      "Reached point 40/87 at epsiode step 351\n",
      "Reached point 41/87 at epsiode step 359\n",
      "Reached point 42/87 at epsiode step 367\n",
      "Reached point 43/87 at epsiode step 377\n",
      "Reached point 44/87 at epsiode step 387\n",
      "Reached point 45/87 at epsiode step 397\n",
      "Reached point 46/87 at epsiode step 408\n",
      "Reached point 47/87 at epsiode step 418\n",
      "Reached point 48/87 at epsiode step 430\n",
      "Reached point 49/87 at epsiode step 440\n",
      "Reached point 50/87 at epsiode step 452\n",
      "Reached point 51/87 at epsiode step 463\n",
      "Reached point 52/87 at epsiode step 474\n",
      "Reached point 53/87 at epsiode step 486\n",
      "Reached point 54/87 at epsiode step 497\n",
      "Reached point 55/87 at epsiode step 506\n",
      "Reached point 56/87 at epsiode step 514\n",
      "Reached point 57/87 at epsiode step 522\n",
      "Reached point 58/87 at epsiode step 530\n",
      "Reached point 59/87 at epsiode step 541\n",
      "Reached point 60/87 at epsiode step 553\n",
      "Reached point 61/87 at epsiode step 564\n",
      "Reached point 62/87 at epsiode step 576\n",
      "Reached point 63/87 at epsiode step 588\n",
      "Reached point 64/87 at epsiode step 599\n",
      "Reached point 65/87 at epsiode step 610\n",
      "Reached point 66/87 at epsiode step 619\n",
      "Reached point 67/87 at epsiode step 628\n",
      "Reached point 68/87 at epsiode step 637\n",
      "Reached point 69/87 at epsiode step 647\n",
      "Reached point 70/87 at epsiode step 656\n",
      "Reached point 71/87 at epsiode step 665\n",
      "Reached point 72/87 at epsiode step 673\n",
      "Reached point 73/87 at epsiode step 681\n",
      "Reached point 74/87 at epsiode step 689\n",
      "Reached point 75/87 at epsiode step 698\n",
      "Reached point 76/87 at epsiode step 706\n",
      "Reached point 77/87 at epsiode step 715\n",
      "Reached point 78/87 at epsiode step 725\n",
      "Reached point 79/87 at epsiode step 736\n",
      "Reached point 80/87 at epsiode step 747\n",
      "Reached point 81/87 at epsiode step 758\n",
      "Reached point 82/87 at epsiode step 769\n",
      "Reached point 83/87 at epsiode step 781\n",
      "Reached point 84/87 at epsiode step 792\n",
      "Reached point 85/87 at epsiode step 803\n",
      "Reached point 86/87 at epsiode step 812\n",
      "Defi reached the terminal state!\n",
      "Defi reached the terminal state!\n",
      "Defi reached the terminal state!\n",
      "Defi reached the terminal state!\n",
      "Defi reached the terminal state!\n",
      "Defi reached the terminal state!\n",
      "Defi reached the terminal state!\n",
      "Defi reached the terminal state!\n",
      "Defi reached the terminal state!\n",
      "Defi reached the terminal state!\n",
      "Defi reached the terminal state!\n",
      "Defi reached the terminal state!\n",
      "Defi reached the terminal state!\n",
      "Defi reached the terminal state!\n",
      "Defi reached the terminal state!\n",
      "Defi reached the terminal state!\n",
      "Defi reached the terminal state!\n",
      "Defi reached the terminal state!\n",
      "Defi reached the terminal state!\n",
      "Defi reached the terminal state!\n",
      "Defi reached the terminal state!\n",
      "Reached point 86/87 at epsiode step 819\n"
     ]
    }
   ],
   "source": [
    "state = env.reset(streamline_index=0)\n",
    "all_top_actions = []\n",
    "all_top_rewards = []\n",
    "terminal = False\n",
    "\n",
    "states = Object()\n",
    "states.x = [state.getCoordinate()[0]]\n",
    "states.y = [state.getCoordinate()[1]]\n",
    "states.z = [state.getCoordinate()[2]]\n",
    "i = 0\n",
    "while (terminal != True) and (i < 1000):\n",
    "    best_actions, best_rewards = get_best_action(state, env)\n",
    "    all_top_actions.append(best_actions)\n",
    "    all_top_rewards.append(best_rewards)\n",
    "    next_state, reward, terminal, _ = env.step(best_actions)\n",
    "    state = next_state\n",
    "    if reward == 1:\n",
    "        print(\"Reached point {}/{} at epsiode step {}\".format(env.points_visited, len(env.referenceStreamline_ijk), env.stepCounter))\n",
    "    #print(best_actions, best_rewards, env.points_visited)\n",
    "    i += 1\n",
    "    #print(i)\n",
    "    states.x.append(state.getCoordinate()[0])\n",
    "    states.y.append(state.getCoordinate()[1])\n",
    "    states.z.append(state.getCoordinate()[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "no_timesteps = 3\n",
    "fig = plt.figure()\n",
    "ax = plt.axes(projection='3d')\n",
    "#ax.plot3D(env.referenceStreamline_ijk.T[0][0:no_timesteps], env.referenceStreamline_ijk.T[1][0:no_timesteps], env.referenceStreamline_ijk.T[2][0:no_timesteps])\n",
    "ax.plot3D(states.x, states.y, states.z)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transfer learning\n",
    "Fill replay memory with perfect actions for supervised approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "action, _ = get_best_action(state,env)\n",
    "next_state, reward, terminal, _ = env.step(action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dfibert.tracker import StreamlinesFromFileTracker\n",
    "\n",
    "file_sl = StreamlinesFromFileTracker(env.pReferenceStreamlines)\n",
    "file_sl.track()\n",
    "\n",
    "tracked_streamlines = file_sl.get_streamlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 1/15 [01:38<22:55, 98.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defi reached the terminal state!\n",
      "Defi reached the terminal state!\n",
      "Defi reached the terminal state!\n",
      "Defi reached the terminal state!\n",
      "Defi reached the terminal state!\n",
      "Defi reached the terminal state!\n",
      "Defi reached the terminal state!\n",
      "Defi reached the terminal state!\n",
      "Defi reached the terminal state!\n",
      "Defi reached the terminal state!\n",
      "Defi reached the terminal state!\n",
      "Defi reached the terminal state!\n",
      "Defi reached the terminal state!\n",
      "Defi reached the terminal state!\n",
      "Defi reached the terminal state!\n",
      "Defi reached the terminal state!\n",
      "Defi reached the terminal state!\n",
      "Defi reached the terminal state!\n",
      "Defi reached the terminal state!\n",
      "Defi reached the terminal state!\n",
      "Defi reached the terminal state!\n",
      "1 86.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 13%|█▎        | 2/15 [02:39<18:54, 87.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defi reached the terminal state!\n",
      "Defi reached the terminal state!\n",
      "Defi reached the terminal state!\n",
      "Defi reached the terminal state!\n",
      "Defi reached the terminal state!\n",
      "Defi reached the terminal state!\n",
      "Defi reached the terminal state!\n",
      "Defi reached the terminal state!\n",
      "Defi reached the terminal state!\n",
      "Defi reached the terminal state!\n",
      "Defi reached the terminal state!\n",
      "Defi reached the terminal state!\n",
      "Defi reached the terminal state!\n",
      "Defi reached the terminal state!\n",
      "Defi reached the terminal state!\n",
      "Defi reached the terminal state!\n",
      "Defi reached the terminal state!\n",
      "Defi reached the terminal state!\n",
      "Defi reached the terminal state!\n",
      "Defi reached the terminal state!\n",
      "Defi reached the terminal state!\n",
      "2 71.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 20%|██        | 3/15 [04:39<19:25, 97.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 85.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 27%|██▋       | 4/15 [05:57<16:44, 91.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defi reached the terminal state!\n",
      "Defi reached the terminal state!\n",
      "Defi reached the terminal state!\n",
      "Defi reached the terminal state!\n",
      "Defi reached the terminal state!\n",
      "Defi reached the terminal state!\n",
      "Defi reached the terminal state!\n",
      "Defi reached the terminal state!\n",
      "Defi reached the terminal state!\n",
      "Defi reached the terminal state!\n",
      "Defi reached the terminal state!\n",
      "Defi reached the terminal state!\n",
      "Defi reached the terminal state!\n",
      "Defi reached the terminal state!\n",
      "Defi reached the terminal state!\n",
      "Defi reached the terminal state!\n",
      "Defi reached the terminal state!\n",
      "Defi reached the terminal state!\n",
      "Defi reached the terminal state!\n",
      "Defi reached the terminal state!\n",
      "Defi reached the terminal state!\n",
      "4 81.25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 5/15 [06:44<13:29, 80.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defi reached the terminal state!\n",
      "Defi reached the terminal state!\n",
      "Defi reached the terminal state!\n",
      "Defi reached the terminal state!\n",
      "Defi reached the terminal state!\n",
      "Defi reached the terminal state!\n",
      "Defi reached the terminal state!\n",
      "Defi reached the terminal state!\n",
      "Defi reached the terminal state!\n",
      "Defi reached the terminal state!\n",
      "Defi reached the terminal state!\n",
      "Defi reached the terminal state!\n",
      "Defi reached the terminal state!\n",
      "Defi reached the terminal state!\n",
      "Defi reached the terminal state!\n",
      "Defi reached the terminal state!\n",
      "Defi reached the terminal state!\n",
      "Defi reached the terminal state!\n",
      "Defi reached the terminal state!\n",
      "Defi reached the terminal state!\n",
      "Defi reached the terminal state!\n",
      "5 73.4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-8b866b1177db>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0moverall_reward\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0moverall_runs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m15\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstreamline_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moverall_runs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0mepisode_reward\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mterminal\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/fibre_tracking/deepFibreTracking/dfibert/envs/RLtractEnvironment.py\u001b[0m in \u001b[0;36mreset\u001b[0;34m(self, streamline_index)\u001b[0m\n\u001b[1;32m    253\u001b[0m             \u001b[0mstreamline_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtracked_streamlines\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    254\u001b[0m         \u001b[0;31m#print(\"Reset to streamline %d/%d\" % (streamline_index+1, len(tracked_streamlines)))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 255\u001b[0;31m         \u001b[0mreferenceStreamline_ras\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtracked_streamlines\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstreamline_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    256\u001b[0m         \u001b[0mreferenceStreamline_ijk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_ijk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreferenceStreamline_ras\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    257\u001b[0m         \u001b[0mreferenceStreamline_ijk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreferenceStreamline_ijk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "from tqdm import trange\n",
    "state = env.reset().getValue()\n",
    "agent = Agent(n_actions=n_actions, inp_size=state.shape, device=device, hidden=10, gamma=0.99, agent_history_length=agent_history_length, memory_size=replay_memory_size, batch_size=512, learning_rate=learning_rate)\n",
    "\n",
    "overall_runs = 0\n",
    "overall_reward = []\n",
    "for overall_runs in trange(15):\n",
    "    state = env.reset(streamline_index=overall_runs)\n",
    "    episode_reward = 0\n",
    "    terminal = False\n",
    "    for i in range(1000):#while not terminal:\n",
    "        action, _ = get_best_action(state,env)\n",
    "        next_state, reward, terminal, _ = env.step(action)\n",
    "            \n",
    "        agent.replay_memory.add_experience(action=action,\n",
    "                                state = state.getValue(),\n",
    "                                reward=reward,\n",
    "                                new_state = next_state.getValue(),\n",
    "                                terminal=terminal)\n",
    "        \n",
    "        episode_reward += reward\n",
    "        \n",
    "        state = next_state\n",
    "        \n",
    "        if terminal == True:\n",
    "            break\n",
    "            \n",
    "    overall_runs += 1\n",
    "    overall_reward.append(episode_reward)\n",
    "    print(overall_runs, np.mean(overall_reward[-100:]))\n",
    "print(\"Replay memory ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save memory\n",
    "import h5py\n",
    "hf = h5py.File('memory_dti.hdf5', 'w')\n",
    "hf.create_dataset('states', data=agent.replay_memory.states[:agent.replay_memory.count])\n",
    "hf.create_dataset('new_states', data=agent.replay_memory.new_states[:agent.replay_memory.count])\n",
    "hf.create_dataset('actions', data=agent.replay_memory.actions[:agent.replay_memory.count])\n",
    "hf.create_dataset('rewards', data=agent.replay_memory.rewards[:agent.replay_memory.count])\n",
    "hf.create_dataset('terminals', data=agent.replay_memory.terminal_flags[:agent.replay_memory.count])\n",
    "hf.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load memory\n",
    "#state = #env.reset().getValue()\n",
    "agent_learn = Agent(n_actions=20, inp_size=(642, 3, 3, 3), device=device, hidden=10, gamma=0.95, agent_history_length=agent_history_length, memory_size=replay_memory_size, batch_size=512, learning_rate=learning_rate)\n",
    "hf = h5py.File('memory_dti.hdf5', 'r')\n",
    "agent_learn.replay_memory.states = np.array(hf[\"states\"][:60000])\n",
    "agent_learn.replay_memory.new_states = np.array(hf[\"new_states\"][:60000])\n",
    "agent_learn.replay_memory.actions = np.array(hf[\"actions\"][:60000])\n",
    "agent_learn.replay_memory.rewards = np.array(hf[\"rewards\"][:60000])\n",
    "agent_learn.replay_memory.terminal_flags = np.array(hf[\"terminals\"][:60000])\n",
    "hf.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(agent_learn.replay_memory.states.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_learn.replay_memory.count = 60000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_learn = Agent(n_actions=101, inp_size=(100, 3, 3, 3), device=device, hidden=10, gamma=0.95, agent_history_length=agent_history_length, memory_size=replay_memory_size, batch_size=512, learning_rate=learning_rate)\n",
    "hf = h5py.File('memory_dti.hdf5', 'r')\n",
    "agent_learn.replay_memory.states = np.array(hf[\"states\"][:])\n",
    "agent_learn.replay_memory.new_states = np.array(hf[\"new_states\"][:])\n",
    "agent_learn.replay_memory.actions = np.array(hf[\"actions\"][:])\n",
    "agent_learn.replay_memory.rewards = np.array(hf[\"rewards\"][:])\n",
    "agent_learn.replay_memory.terminal_flags = np.array(hf[\"terminals\"][:])\n",
    "print(agent_learn.replay_memory.states.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_learn.replay_memory.rewards[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_learn.replay_memory.count = 30000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.optimizer = torch.optim.Adam(agent.main_dqn.parameters(), 0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50000/50000 [06:54<00:00, 120.54epochs/s, loss=0.225]\n"
     ]
    }
   ],
   "source": [
    "#### Supervised learning on perfect memory --> worked\n",
    "\n",
    "#state = env.reset().getValue()\n",
    "#agent_learn = Agent(n_actions=n_actions, inp_size=state.shape, device=device, hidden=10, gamma=0.99, agent_history_length=agent_history_length, memory_size=replay_memory_size, batch_size=64, learning_rate=0.00001)\n",
    "losses = []\n",
    "\n",
    "with trange(50000, unit=\"epochs\") as pbar:\n",
    "    for i in pbar:\n",
    "    \n",
    "        states, actions, _, _, _ = agent.replay_memory.get_minibatch()\n",
    "\n",
    "        states = torch.FloatTensor(states).to(agent.device)\n",
    "        actions = torch.LongTensor(actions).to(agent.device)\n",
    "\n",
    "        predicted_q = agent.main_dqn(states)\n",
    "        loss = torch.nn.functional.cross_entropy(predicted_q, actions)\n",
    "        #print(loss.item())\n",
    "        agent.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        agent.optimizer.step()\n",
    "        losses.append(loss.item())\n",
    "\n",
    "        pbar.set_postfix(loss=loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.2688422180712223, 0.2677806200165498, 0.2674466148018837, 0.2663012734230827, 0.26788164488971233, 0.2698832035064697, 0.27360816087041584, 0.2732531795134911, 0.27654164160291356, 0.27369105138561944, 0.275684954226017, 0.27095557749271393, 0.2722139451652765, 0.2646679473774774, 0.26812829325596493, 0.25447799265384674, 0.26754165440797806, 0.2501937250296275, 0.28506603837013245, 0.22472885251045227]\n"
     ]
    }
   ],
   "source": [
    "mean_losses = []\n",
    "for i in range(len(losses)):\n",
    "    mean_losses.append(np.mean(losses[i:i+99]))\n",
    "print(mean_losses[-20:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "fig, ax = plt.subplots()\n",
    "plt.plot(range(len(losses[:])), losses[:])\n",
    "plt.plot(range(len(losses[:])), mean_losses[:])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.target_dqn.load_state_dict(agent.main_dqn.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 0.828125\n"
     ]
    }
   ],
   "source": [
    "states, actions, _, _, _ = agent.replay_memory.get_minibatch()\n",
    "states = torch.FloatTensor(states).to(agent.device)\n",
    "predicted_q = torch.argmax(agent.main_dqn(states), dim=1)\n",
    "\n",
    "false = 0\n",
    "for i in range(len(actions)):\n",
    "    if predicted_q[i] != actions[i]:\n",
    "        false += 1 \n",
    "    \n",
    "print(\"Accuracy =\", 1 - false / len(actions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true,
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reward 0.0 at step 1, points reached 1, l2 distance 0.7191250262328941\n",
      "Episode finished with 42.0 reward\n",
      "Reward 0.0 at step 2, points reached 1, l2 distance 0.6436396269863729\n",
      "Episode finished with 42.0 reward\n",
      "Reward 0.0 at step 3, points reached 1, l2 distance 0.5756762419102027\n",
      "Episode finished with 42.0 reward\n",
      "Reward 0.0 at step 4, points reached 1, l2 distance 0.4927653645843112\n",
      "Episode finished with 42.0 reward\n",
      "Reward 0.0 at step 5, points reached 1, l2 distance 0.42753878680189744\n",
      "Episode finished with 42.0 reward\n",
      "Reward 0.0 at step 6, points reached 1, l2 distance 0.37783743052557384\n",
      "Episode finished with 42.0 reward\n",
      "Reward 0.0 at step 7, points reached 1, l2 distance 0.28258941977201696\n",
      "Episode finished with 42.0 reward\n",
      "Reward 0.0 at step 8, points reached 1, l2 distance 0.19223016523948988\n",
      "Episode finished with 42.0 reward\n",
      "Reward 0.0 at step 9, points reached 1, l2 distance 0.15035036860944173\n",
      "Episode finished with 42.0 reward\n",
      "Reward 1.0 at step 10, points reached 2, l2 distance 0.050412629522195516\n",
      "Episode finished with 43.0 reward\n",
      "Reward 0.0 at step 11, points reached 2, l2 distance 0.7692304525044089\n",
      "Episode finished with 43.0 reward\n",
      "Reward 0.0 at step 12, points reached 2, l2 distance 0.6880480111075452\n",
      "Episode finished with 43.0 reward\n",
      "Reward 0.0 at step 13, points reached 2, l2 distance 0.6199550262067053\n",
      "Episode finished with 43.0 reward\n",
      "Reward 0.0 at step 14, points reached 2, l2 distance 0.5374749804979747\n",
      "Episode finished with 43.0 reward\n",
      "Reward 0.0 at step 15, points reached 2, l2 distance 0.471862339252285\n",
      "Episode finished with 43.0 reward\n",
      "Reward 0.0 at step 16, points reached 2, l2 distance 0.38733671522386176\n",
      "Episode finished with 43.0 reward\n",
      "Reward 0.0 at step 17, points reached 2, l2 distance 0.32656544078818756\n",
      "Episode finished with 43.0 reward\n",
      "Reward 0.0 at step 18, points reached 2, l2 distance 0.23845585437641592\n",
      "Episode finished with 43.0 reward\n",
      "Reward 0.0 at step 19, points reached 2, l2 distance 0.19057018197134323\n",
      "Episode finished with 43.0 reward\n",
      "Reward 1.0 at step 20, points reached 3, l2 distance 0.09681810388158234\n",
      "Episode finished with 44.0 reward\n",
      "Reward 0.0 at step 21, points reached 3, l2 distance 0.8059382327253496\n",
      "Episode finished with 44.0 reward\n",
      "Reward 0.0 at step 22, points reached 3, l2 distance 0.7218807773335061\n",
      "Episode finished with 44.0 reward\n",
      "Reward 0.0 at step 23, points reached 3, l2 distance 0.6424074084245863\n",
      "Episode finished with 44.0 reward\n",
      "Reward 0.0 at step 24, points reached 3, l2 distance 0.5606876672222175\n",
      "Episode finished with 44.0 reward\n",
      "Reward 0.0 at step 25, points reached 3, l2 distance 0.4915128481155641\n",
      "Episode finished with 44.0 reward\n",
      "Reward 0.0 at step 26, points reached 3, l2 distance 0.4130442881695587\n",
      "Episode finished with 44.0 reward\n",
      "Reward 0.0 at step 27, points reached 3, l2 distance 0.3473619830817861\n",
      "Episode finished with 44.0 reward\n",
      "Reward 0.0 at step 28, points reached 3, l2 distance 0.2692983383364361\n",
      "Episode finished with 44.0 reward\n",
      "Reward 0.0 at step 29, points reached 3, l2 distance 0.21311241621550675\n",
      "Episode finished with 44.0 reward\n",
      "Reward 0.0 at step 30, points reached 3, l2 distance 0.11766103325111427\n",
      "Episode finished with 44.0 reward\n",
      "Reward 1.0 at step 31, points reached 4, l2 distance 0.047658530676349356\n",
      "Episode finished with 45.0 reward\n",
      "Reward 0.0 at step 32, points reached 4, l2 distance 0.7509928581663767\n",
      "Episode finished with 45.0 reward\n",
      "Reward 0.0 at step 33, points reached 4, l2 distance 0.6708894307444637\n",
      "Episode finished with 45.0 reward\n",
      "Reward 0.0 at step 34, points reached 4, l2 distance 0.5968207298279424\n",
      "Episode finished with 45.0 reward\n",
      "Reward 0.0 at step 35, points reached 4, l2 distance 0.5225677294492207\n",
      "Episode finished with 45.0 reward\n",
      "Reward 0.0 at step 36, points reached 4, l2 distance 0.4462692881286943\n",
      "Episode finished with 45.0 reward\n",
      "Reward 0.0 at step 37, points reached 4, l2 distance 0.3726645535192204\n",
      "Episode finished with 45.0 reward\n",
      "Reward 0.0 at step 38, points reached 4, l2 distance 0.29630838450554897\n",
      "Episode finished with 45.0 reward\n",
      "Reward 0.0 at step 39, points reached 4, l2 distance 0.2244143361385557\n",
      "Episode finished with 45.0 reward\n",
      "Reward 0.0 at step 40, points reached 4, l2 distance 0.14873509053531317\n",
      "Episode finished with 45.0 reward\n",
      "Reward 1.0 at step 41, points reached 5, l2 distance 0.08539095098321983\n",
      "Episode finished with 46.0 reward\n",
      "Reward 0.0 at step 42, points reached 5, l2 distance 0.7955400548105237\n",
      "Episode finished with 46.0 reward\n",
      "Reward 0.0 at step 43, points reached 5, l2 distance 0.7188947205111986\n",
      "Episode finished with 46.0 reward\n",
      "Reward 0.0 at step 44, points reached 5, l2 distance 0.6486410868498979\n",
      "Episode finished with 46.0 reward\n",
      "Reward 0.0 at step 45, points reached 5, l2 distance 0.574303899706551\n",
      "Episode finished with 46.0 reward\n",
      "Reward 0.0 at step 46, points reached 5, l2 distance 0.506548273319064\n",
      "Episode finished with 46.0 reward\n",
      "Reward 0.0 at step 47, points reached 5, l2 distance 0.43419051864854546\n",
      "Episode finished with 46.0 reward\n",
      "Reward 0.0 at step 48, points reached 5, l2 distance 0.3716559350825663\n",
      "Episode finished with 46.0 reward\n",
      "Reward 0.0 at step 49, points reached 5, l2 distance 0.3047932594026588\n",
      "Episode finished with 46.0 reward\n",
      "Reward 0.0 at step 50, points reached 5, l2 distance 0.2556290323663005\n",
      "Episode finished with 46.0 reward\n",
      "Reward 0.0 at step 51, points reached 5, l2 distance 0.17967532215469229\n",
      "Episode finished with 46.0 reward\n",
      "Reward 0.0 at step 52, points reached 5, l2 distance 0.1386370823201862\n",
      "Episode finished with 46.0 reward\n",
      "Reward 0.0 at step 53, points reached 5, l2 distance 0.16173206174755822\n",
      "Episode finished with 46.0 reward\n",
      "Reward 0.0 at step 54, points reached 5, l2 distance 0.24983744195582475\n",
      "Episode finished with 46.0 reward\n",
      "Reward 0.0 at step 55, points reached 5, l2 distance 0.24577426421993276\n",
      "Episode finished with 46.0 reward\n",
      "Reward 0.0 at step 56, points reached 5, l2 distance 0.34569787412160086\n",
      "Episode finished with 46.0 reward\n",
      "Reward 0.0 at step 57, points reached 5, l2 distance 0.3746115392449164\n",
      "Episode finished with 46.0 reward\n",
      "Reward 0.0 at step 58, points reached 5, l2 distance 0.4562563684522826\n",
      "Episode finished with 46.0 reward\n",
      "Reward 0.0 at step 59, points reached 5, l2 distance 0.5048182293913014\n",
      "Episode finished with 46.0 reward\n",
      "Reward 0.0 at step 60, points reached 5, l2 distance 0.5903577719932552\n",
      "Episode finished with 46.0 reward\n",
      "Reward 0.0 at step 61, points reached 5, l2 distance 0.6798552442835991\n",
      "Episode finished with 46.0 reward\n",
      "Reward 0.0 at step 62, points reached 5, l2 distance 0.7352307772195884\n",
      "Episode finished with 46.0 reward\n",
      "Reward 0.0 at step 63, points reached 5, l2 distance 0.8246756549136418\n",
      "Episode finished with 46.0 reward\n",
      "Reward 0.0 at step 64, points reached 5, l2 distance 0.9163053944202055\n",
      "Episode finished with 46.0 reward\n",
      "Reward 0.0 at step 65, points reached 5, l2 distance 1.009525242814608\n",
      "Episode finished with 46.0 reward\n",
      "Reward 0.0 at step 66, points reached 5, l2 distance 1.0928830270110763\n",
      "Episode finished with 46.0 reward\n",
      "Reward 0.0 at step 67, points reached 5, l2 distance 1.1459185635816747\n",
      "Episode finished with 46.0 reward\n",
      "Reward 0.0 at step 68, points reached 5, l2 distance 1.2293176940170998\n",
      "Episode finished with 46.0 reward\n",
      "Reward 0.0 at step 69, points reached 5, l2 distance 1.28561214215997\n",
      "Episode finished with 46.0 reward\n",
      "Reward 0.0 at step 70, points reached 5, l2 distance 1.3759445008140516\n",
      "Episode finished with 46.0 reward\n",
      "Reward 0.0 at step 71, points reached 5, l2 distance 1.435494563687132\n",
      "Episode finished with 46.0 reward\n",
      "Reward 0.0 at step 72, points reached 5, l2 distance 1.525422602899224\n",
      "Episode finished with 46.0 reward\n",
      "Reward 0.0 at step 73, points reached 5, l2 distance 1.587507567320539\n",
      "Episode finished with 46.0 reward\n",
      "Reward 0.0 at step 74, points reached 5, l2 distance 1.6769897248253378\n",
      "Episode finished with 46.0 reward\n",
      "Reward 0.0 at step 75, points reached 5, l2 distance 1.7675997278907771\n",
      "Episode finished with 46.0 reward\n",
      "Reward 0.0 at step 76, points reached 5, l2 distance 1.8591726813046001\n",
      "Episode finished with 46.0 reward\n",
      "Reward 0.0 at step 77, points reached 5, l2 distance 1.9515730372648428\n",
      "Episode finished with 46.0 reward\n",
      "Reward 0.0 at step 78, points reached 5, l2 distance 2.022009190248912\n",
      "Episode finished with 46.0 reward\n",
      "Reward 0.0 at step 79, points reached 5, l2 distance 2.1146839433252773\n",
      "Episode finished with 46.0 reward\n",
      "Reward 0.0 at step 80, points reached 5, l2 distance 2.20799800608253\n",
      "Episode finished with 46.0 reward\n",
      "Reward 0.0 at step 81, points reached 5, l2 distance 2.26737485387629\n",
      "Episode finished with 46.0 reward\n",
      "Reward 0.0 at step 82, points reached 5, l2 distance 2.342931857743246\n",
      "Episode finished with 46.0 reward\n",
      "Reward 0.0 at step 83, points reached 5, l2 distance 2.4358050474827375\n",
      "Episode finished with 46.0 reward\n",
      "Reward -1.0 at step 84, points reached 5, l2 distance 2.5292217713471756\n",
      "Episode finished with 45.0 reward\n",
      "Reward -1.0 at step 85, points reached 5, l2 distance 2.6002508586323434\n",
      "Episode finished with 44.0 reward\n",
      "Reward -1.0 at step 86, points reached 5, l2 distance 2.693765732384555\n",
      "Episode finished with 43.0 reward\n",
      "Reward -1.0 at step 87, points reached 5, l2 distance 2.766219140626406\n",
      "Episode finished with 42.0 reward\n",
      "Reward -1.0 at step 88, points reached 5, l2 distance 2.8597710364565634\n",
      "Episode finished with 41.0 reward\n",
      "Reward -1.0 at step 89, points reached 5, l2 distance 2.9197672715038943\n",
      "Episode finished with 40.0 reward\n",
      "Reward -1.0 at step 90, points reached 5, l2 distance 2.993115075980335\n",
      "Episode finished with 39.0 reward\n",
      "Reward -1.0 at step 91, points reached 5, l2 distance 3.0542313174452986\n",
      "Episode finished with 38.0 reward\n",
      "Reward -1.0 at step 92, points reached 5, l2 distance 3.146576758616947\n",
      "Episode finished with 37.0 reward\n",
      "Reward -1.0 at step 93, points reached 5, l2 distance 3.23937673870654\n",
      "Episode finished with 36.0 reward\n",
      "Reward -1.0 at step 94, points reached 5, l2 distance 3.332593286417907\n",
      "Episode finished with 35.0 reward\n",
      "Reward -1.0 at step 95, points reached 5, l2 distance 3.406410001537177\n",
      "Episode finished with 34.0 reward\n",
      "Reward -1.0 at step 96, points reached 5, l2 distance 3.49964258315247\n",
      "Episode finished with 33.0 reward\n",
      "Reward -1.0 at step 97, points reached 5, l2 distance 3.560599260638641\n",
      "Episode finished with 32.0 reward\n",
      "Reward -1.0 at step 98, points reached 5, l2 distance 3.6232907666797574\n",
      "Episode finished with 31.0 reward\n",
      "Reward -1.0 at step 99, points reached 5, l2 distance 3.6947153805118975\n",
      "Episode finished with 30.0 reward\n",
      "Reward -1.0 at step 100, points reached 5, l2 distance 3.786962524854119\n",
      "Episode finished with 29.0 reward\n",
      "Reward -1.0 at step 101, points reached 5, l2 distance 3.84969341365238\n",
      "Episode finished with 28.0 reward\n",
      "Reward -1.0 at step 102, points reached 5, l2 distance 3.9217687877556684\n",
      "Episode finished with 27.0 reward\n",
      "Reward -1.0 at step 103, points reached 5, l2 distance 4.01383649030206\n",
      "Episode finished with 26.0 reward\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reward -1.0 at step 104, points reached 5, l2 distance 4.1062752364090445\n",
      "Episode finished with 25.0 reward\n",
      "Reward -1.0 at step 105, points reached 5, l2 distance 4.199060521522873\n",
      "Episode finished with 24.0 reward\n",
      "Reward -1.0 at step 106, points reached 5, l2 distance 4.273419775096057\n",
      "Episode finished with 23.0 reward\n",
      "Reward -1.0 at step 107, points reached 5, l2 distance 4.366232045400785\n",
      "Episode finished with 22.0 reward\n",
      "Reward -1.0 at step 108, points reached 5, l2 distance 4.441310880685084\n",
      "Episode finished with 21.0 reward\n",
      "Reward -1.0 at step 109, points reached 5, l2 distance 4.534132860077139\n",
      "Episode finished with 20.0 reward\n",
      "Reward -1.0 at step 110, points reached 5, l2 distance 4.609866984119519\n",
      "Episode finished with 19.0 reward\n",
      "Reward -1.0 at step 111, points reached 5, l2 distance 4.680830382204586\n",
      "Episode finished with 18.0 reward\n",
      "Reward -1.0 at step 112, points reached 5, l2 distance 4.743029440761574\n",
      "Episode finished with 17.0 reward\n",
      "Reward -1.0 at step 113, points reached 5, l2 distance 4.835405774939162\n",
      "Episode finished with 16.0 reward\n",
      "Reward -1.0 at step 114, points reached 5, l2 distance 4.928079721422835\n",
      "Episode finished with 15.0 reward\n",
      "Reward -1.0 at step 115, points reached 5, l2 distance 5.021034801022885\n",
      "Episode finished with 14.0 reward\n",
      "Reward -1.0 at step 116, points reached 5, l2 distance 5.1142556844048075\n",
      "Episode finished with 13.0 reward\n",
      "Reward -1.0 at step 117, points reached 5, l2 distance 5.2077280975345515\n",
      "Episode finished with 12.0 reward\n",
      "Reward -1.0 at step 118, points reached 5, l2 distance 5.3014387358748944\n",
      "Episode finished with 11.0 reward\n",
      "Reward -1.0 at step 119, points reached 5, l2 distance 5.376311831529328\n",
      "Episode finished with 10.0 reward\n",
      "Reward -1.0 at step 120, points reached 5, l2 distance 5.469991078243464\n",
      "Episode finished with 9.0 reward\n",
      "Reward -1.0 at step 121, points reached 5, l2 distance 5.5638903549839975\n",
      "Episode finished with 8.0 reward\n",
      "Reward -1.0 at step 122, points reached 5, l2 distance 5.657998707008054\n",
      "Episode finished with 7.0 reward\n",
      "Reward -1.0 at step 123, points reached 5, l2 distance 5.752305872841401\n",
      "Episode finished with 6.0 reward\n",
      "Reward -1.0 at step 124, points reached 5, l2 distance 5.846802232070666\n",
      "Episode finished with 5.0 reward\n",
      "Reward -1.0 at step 125, points reached 5, l2 distance 5.941478757613073\n",
      "Episode finished with 4.0 reward\n",
      "Reward -1.0 at step 126, points reached 5, l2 distance 6.013369681866365\n",
      "Episode finished with 3.0 reward\n",
      "Reward -1.0 at step 127, points reached 5, l2 distance 6.108297074947213\n",
      "Episode finished with 2.0 reward\n",
      "Reward -1.0 at step 128, points reached 5, l2 distance 6.165522669110087\n",
      "Episode finished with 1.0 reward\n",
      "Reward -1.0 at step 129, points reached 5, l2 distance 6.237832970619222\n",
      "Episode finished with 0.0 reward\n",
      "Reward -1.0 at step 130, points reached 5, l2 distance 6.310899346002763\n",
      "Episode finished with -1.0 reward\n",
      "Reward -1.0 at step 131, points reached 5, l2 distance 6.384695837814983\n",
      "Episode finished with -2.0 reward\n",
      "Reward -1.0 at step 132, points reached 5, l2 distance 6.479558485625003\n",
      "Episode finished with -3.0 reward\n",
      "Reward -1.0 at step 133, points reached 5, l2 distance 6.57457340029435\n",
      "Episode finished with -4.0 reward\n",
      "Reward -1.0 at step 134, points reached 5, l2 distance 6.649143949373357\n",
      "Episode finished with -5.0 reward\n",
      "Reward -1.0 at step 135, points reached 5, l2 distance 6.7443351952219555\n",
      "Episode finished with -6.0 reward\n",
      "Reward -1.0 at step 136, points reached 5, l2 distance 6.81958663174962\n",
      "Episode finished with -7.0 reward\n",
      "Reward -1.0 at step 137, points reached 5, l2 distance 6.914933733070592\n",
      "Episode finished with -8.0 reward\n",
      "Reward -1.0 at step 138, points reached 5, l2 distance 6.969667640681544\n",
      "Episode finished with -9.0 reward\n",
      "Reward -1.0 at step 139, points reached 5, l2 distance 7.045169757103348\n",
      "Episode finished with -10.0 reward\n",
      "Reward -1.0 at step 140, points reached 5, l2 distance 7.12127564353774\n",
      "Episode finished with -11.0 reward\n",
      "Reward -1.0 at step 141, points reached 5, l2 distance 7.197966148578642\n",
      "Episode finished with -12.0 reward\n",
      "Reward -1.0 at step 142, points reached 5, l2 distance 7.275222784281198\n",
      "Episode finished with -13.0 reward\n",
      "Reward -1.0 at step 143, points reached 5, l2 distance 7.353027706038136\n",
      "Episode finished with -14.0 reward\n",
      "Reward -1.0 at step 144, points reached 5, l2 distance 7.431363692526739\n",
      "Episode finished with -15.0 reward\n",
      "Reward -1.0 at step 145, points reached 5, l2 distance 7.510214125805259\n",
      "Episode finished with -16.0 reward\n",
      "Reward -1.0 at step 146, points reached 5, l2 distance 7.5632769689827395\n",
      "Episode finished with -17.0 reward\n",
      "Reward -1.0 at step 147, points reached 5, l2 distance 7.642260721671197\n",
      "Episode finished with -18.0 reward\n",
      "Reward -1.0 at step 148, points reached 5, l2 distance 7.695897074137079\n",
      "Episode finished with -19.0 reward\n",
      "Reward -1.0 at step 149, points reached 5, l2 distance 7.775002427509507\n",
      "Episode finished with -20.0 reward\n",
      "Reward -1.0 at step 150, points reached 5, l2 distance 7.854584248694884\n",
      "Episode finished with -21.0 reward\n",
      "Reward -1.0 at step 151, points reached 5, l2 distance 7.934628201239982\n",
      "Episode finished with -22.0 reward\n",
      "Reward -1.0 at step 152, points reached 5, l2 distance 8.015120439767571\n",
      "Episode finished with -23.0 reward\n",
      "Reward -1.0 at step 153, points reached 5, l2 distance 8.096047593489489\n",
      "Episode finished with -24.0 reward\n",
      "Reward -1.0 at step 154, points reached 5, l2 distance 8.177396750073584\n",
      "Episode finished with -25.0 reward\n",
      "Reward -1.0 at step 155, points reached 5, l2 distance 8.229796091490233\n",
      "Episode finished with -26.0 reward\n",
      "Reward -1.0 at step 156, points reached 5, l2 distance 8.31121021411373\n",
      "Episode finished with -27.0 reward\n",
      "Reward -1.0 at step 157, points reached 5, l2 distance 8.393026077575886\n",
      "Episode finished with -28.0 reward\n",
      "Reward -1.0 at step 158, points reached 5, l2 distance 8.47523204723914\n",
      "Episode finished with -29.0 reward\n",
      "Reward -1.0 at step 159, points reached 5, l2 distance 8.557816881087714\n",
      "Episode finished with -30.0 reward\n",
      "Reward -1.0 at step 160, points reached 5, l2 distance 8.64076971605599\n",
      "Episode finished with -31.0 reward\n",
      "Reward -1.0 at step 161, points reached 5, l2 distance 8.724080054744501\n",
      "Episode finished with -32.0 reward\n",
      "Reward -1.0 at step 162, points reached 5, l2 distance 8.807737752525973\n",
      "Episode finished with -33.0 reward\n",
      "Reward -1.0 at step 163, points reached 5, l2 distance 8.891733005041917\n",
      "Episode finished with -34.0 reward\n",
      "Reward -1.0 at step 164, points reached 5, l2 distance 8.976056336088373\n",
      "Episode finished with -35.0 reward\n",
      "Reward -1.0 at step 165, points reached 5, l2 distance 9.060698585887994\n",
      "Episode finished with -36.0 reward\n",
      "Reward -1.0 at step 166, points reached 5, l2 distance 9.145650899744272\n",
      "Episode finished with -37.0 reward\n",
      "Reward -1.0 at step 167, points reached 5, l2 distance 9.230904717072626\n",
      "Episode finished with -38.0 reward\n",
      "Reward -1.0 at step 168, points reached 5, l2 distance 9.316451760802186\n",
      "Episode finished with -39.0 reward\n",
      "Reward -1.0 at step 169, points reached 5, l2 distance 9.402284027141205\n",
      "Episode finished with -40.0 reward\n",
      "Reward -1.0 at step 170, points reached 5, l2 distance 9.488393775698556\n",
      "Episode finished with -41.0 reward\n",
      "Reward -1.0 at step 171, points reached 5, l2 distance 9.574773519953125\n",
      "Episode finished with -42.0 reward\n",
      "Reward -1.0 at step 172, points reached 5, l2 distance 9.661416018062571\n",
      "Episode finished with -43.0 reward\n",
      "Reward -1.0 at step 173, points reached 5, l2 distance 9.67997653804826\n",
      "Episode finished with -44.0 reward\n",
      "Reward -1.0 at step 174, points reached 5, l2 distance 9.727822129711027\n",
      "Episode finished with -45.0 reward\n",
      "Reward -1.0 at step 175, points reached 5, l2 distance 9.81484813316389\n",
      "Episode finished with -46.0 reward\n",
      "Reward -1.0 at step 176, points reached 5, l2 distance 9.902119185651415\n",
      "Episode finished with -47.0 reward\n",
      "Reward -1.0 at step 177, points reached 5, l2 distance 9.98962886480879\n",
      "Episode finished with -48.0 reward\n",
      "Reward -1.0 at step 178, points reached 5, l2 distance 10.077370954092942\n",
      "Episode finished with -49.0 reward\n",
      "Reward -1.0 at step 179, points reached 5, l2 distance 10.165339435364231\n",
      "Episode finished with -50.0 reward\n",
      "Reward -1.0 at step 180, points reached 5, l2 distance 10.253528481744532\n",
      "Episode finished with -51.0 reward\n",
      "Reward -1.0 at step 181, points reached 5, l2 distance 10.341932450742533\n",
      "Episode finished with -52.0 reward\n",
      "Reward -1.0 at step 182, points reached 5, l2 distance 10.430545877637304\n",
      "Episode finished with -53.0 reward\n",
      "Reward -1.0 at step 183, points reached 5, l2 distance 10.519363469111221\n",
      "Episode finished with -54.0 reward\n",
      "Reward -1.0 at step 184, points reached 5, l2 distance 10.608380097123538\n",
      "Episode finished with -55.0 reward\n",
      "Reward -1.0 at step 185, points reached 5, l2 distance 10.697590793016056\n",
      "Episode finished with -56.0 reward\n",
      "Reward -1.0 at step 186, points reached 5, l2 distance 10.786990741842557\n",
      "Episode finished with -57.0 reward\n",
      "Reward -1.0 at step 187, points reached 5, l2 distance 10.876575276913783\n",
      "Episode finished with -58.0 reward\n",
      "Reward -1.0 at step 188, points reached 5, l2 distance 10.966339874550089\n",
      "Episode finished with -59.0 reward\n",
      "Reward -1.0 at step 189, points reached 5, l2 distance 11.056280149033958\n",
      "Episode finished with -60.0 reward\n",
      "Reward -1.0 at step 190, points reached 5, l2 distance 11.146391847754925\n",
      "Episode finished with -61.0 reward\n",
      "Reward -1.0 at step 191, points reached 5, l2 distance 11.236670846539582\n",
      "Episode finished with -62.0 reward\n",
      "Reward -1.0 at step 192, points reached 5, l2 distance 11.327113145159613\n",
      "Episode finished with -63.0 reward\n",
      "Reward -1.0 at step 193, points reached 5, l2 distance 11.417714863011023\n",
      "Episode finished with -64.0 reward\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reward -1.0 at step 194, points reached 5, l2 distance 11.508472234957944\n",
      "Episode finished with -65.0 reward\n",
      "Reward -1.0 at step 195, points reached 5, l2 distance 11.599381607334633\n",
      "Episode finished with -66.0 reward\n",
      "Reward -1.0 at step 196, points reached 5, l2 distance 11.690439434099478\n",
      "Episode finished with -67.0 reward\n",
      "Reward -1.0 at step 197, points reached 5, l2 distance 11.781642273135066\n",
      "Episode finished with -68.0 reward\n",
      "Reward -1.0 at step 198, points reached 5, l2 distance 11.872986782688605\n",
      "Episode finished with -69.0 reward\n",
      "Reward -1.0 at step 199, points reached 5, l2 distance 11.964469717947111\n",
      "Episode finished with -70.0 reward\n",
      "Reward -1.0 at step 200, points reached 5, l2 distance 12.056087927742087\n",
      "Episode finished with -71.0 reward\n",
      "Reward -1.0 at step 201, points reached 5, l2 distance 12.056268904271334\n",
      "Episode finished with -72.0 reward\n",
      "Reward -1.0 at step 202, points reached 5, l2 distance 12.057279281112772\n",
      "Episode finished with -73.0 reward\n",
      "Reward -1.0 at step 203, points reached 5, l2 distance 12.136468385292783\n",
      "Episode finished with -74.0 reward\n",
      "Reward -1.0 at step 204, points reached 5, l2 distance 12.215962756642272\n",
      "Episode finished with -75.0 reward\n",
      "Reward -1.0 at step 205, points reached 5, l2 distance 12.29575647433418\n",
      "Episode finished with -76.0 reward\n",
      "Reward -1.0 at step 206, points reached 5, l2 distance 12.375843748229482\n",
      "Episode finished with -77.0 reward\n",
      "Reward -1.0 at step 207, points reached 5, l2 distance 12.45621891606962\n",
      "Episode finished with -78.0 reward\n",
      "Reward -1.0 at step 208, points reached 5, l2 distance 12.536876440705257\n",
      "Episode finished with -79.0 reward\n",
      "Reward -1.0 at step 209, points reached 5, l2 distance 12.617810907363065\n",
      "Episode finished with -80.0 reward\n",
      "Reward -1.0 at step 210, points reached 5, l2 distance 12.699017020952063\n",
      "Episode finished with -81.0 reward\n",
      "Reward -1.0 at step 211, points reached 5, l2 distance 12.780489603410741\n",
      "Episode finished with -82.0 reward\n",
      "Reward -1.0 at step 212, points reached 5, l2 distance 12.862223591096134\n",
      "Episode finished with -83.0 reward\n",
      "Reward -1.0 at step 213, points reached 5, l2 distance 12.944214032215697\n",
      "Episode finished with -84.0 reward\n",
      "Reward -1.0 at step 214, points reached 5, l2 distance 13.026456084302808\n",
      "Episode finished with -85.0 reward\n",
      "Reward -1.0 at step 215, points reached 5, l2 distance 13.108945011736433\n",
      "Episode finished with -86.0 reward\n",
      "Reward -1.0 at step 216, points reached 5, l2 distance 13.191676183305493\n",
      "Episode finished with -87.0 reward\n",
      "Reward -1.0 at step 217, points reached 5, l2 distance 13.274645069818222\n",
      "Episode finished with -88.0 reward\n",
      "Reward -1.0 at step 218, points reached 5, l2 distance 13.357847241756774\n",
      "Episode finished with -89.0 reward\n",
      "Reward -1.0 at step 219, points reached 5, l2 distance 13.441278366977189\n",
      "Episode finished with -90.0 reward\n",
      "Reward -1.0 at step 220, points reached 5, l2 distance 13.524934208454724\n",
      "Episode finished with -91.0 reward\n",
      "Reward -1.0 at step 221, points reached 5, l2 distance 13.608810622074532\n",
      "Episode finished with -92.0 reward\n",
      "Reward -1.0 at step 222, points reached 5, l2 distance 13.692903554467494\n",
      "Episode finished with -93.0 reward\n",
      "Reward -1.0 at step 223, points reached 5, l2 distance 13.777209040890988\n",
      "Episode finished with -94.0 reward\n",
      "Reward -1.0 at step 224, points reached 5, l2 distance 13.86172320315437\n",
      "Episode finished with -95.0 reward\n",
      "Reward -1.0 at step 225, points reached 5, l2 distance 13.946442247588742\n",
      "Episode finished with -96.0 reward\n",
      "Reward -1.0 at step 226, points reached 5, l2 distance 14.031362463060669\n",
      "Episode finished with -97.0 reward\n",
      "Reward -1.0 at step 227, points reached 5, l2 distance 14.099522738395692\n",
      "Episode finished with -98.0 reward\n",
      "Reward -1.0 at step 228, points reached 5, l2 distance 14.16806092347454\n",
      "Episode finished with -99.0 reward\n",
      "Reward -1.0 at step 229, points reached 5, l2 distance 14.236971560413942\n",
      "Episode finished with -100.0 reward\n",
      "Reward -1.0 at step 230, points reached 5, l2 distance 14.306249267113465\n",
      "Episode finished with -101.0 reward\n",
      "Reward -1.0 at step 231, points reached 5, l2 distance 14.375888736823672\n",
      "Episode finished with -102.0 reward\n",
      "Reward -1.0 at step 232, points reached 5, l2 distance 14.445884737678162\n",
      "Episode finished with -103.0 reward\n",
      "Reward -1.0 at step 233, points reached 5, l2 distance 14.532165198838582\n",
      "Episode finished with -104.0 reward\n",
      "Reward -1.0 at step 234, points reached 5, l2 distance 14.53722082527652\n",
      "Episode finished with -105.0 reward\n",
      "Reward -1.0 at step 235, points reached 5, l2 distance 14.57608962868304\n",
      "Episode finished with -106.0 reward\n",
      "Reward -1.0 at step 236, points reached 5, l2 distance 14.6471993544592\n",
      "Episode finished with -107.0 reward\n",
      "Reward -1.0 at step 237, points reached 5, l2 distance 14.718644944258243\n",
      "Episode finished with -108.0 reward\n",
      "Reward -1.0 at step 238, points reached 5, l2 distance 14.790421530876241\n",
      "Episode finished with -109.0 reward\n",
      "Reward -1.0 at step 239, points reached 5, l2 distance 14.877940588858108\n",
      "Episode finished with -110.0 reward\n",
      "Reward -1.0 at step 240, points reached 5, l2 distance 14.885641920540264\n",
      "Episode finished with -111.0 reward\n",
      "Reward -1.0 at step 241, points reached 5, l2 distance 14.925956958942152\n",
      "Episode finished with -112.0 reward\n",
      "Reward -1.0 at step 242, points reached 5, l2 distance 14.934614647849697\n",
      "Episode finished with -113.0 reward\n",
      "Reward -1.0 at step 243, points reached 5, l2 distance 15.02308224516265\n",
      "Episode finished with -114.0 reward\n",
      "Reward -1.0 at step 244, points reached 5, l2 distance 15.032668878357203\n",
      "Episode finished with -115.0 reward\n",
      "Reward -1.0 at step 245, points reached 5, l2 distance 15.042914181359182\n",
      "Episode finished with -116.0 reward\n",
      "Reward -1.0 at step 246, points reached 5, l2 distance 15.074537718957025\n",
      "Episode finished with -117.0 reward\n",
      "Reward -1.0 at step 247, points reached 5, l2 distance 15.106757025012977\n",
      "Episode finished with -118.0 reward\n",
      "Reward -1.0 at step 248, points reached 5, l2 distance 15.118885798581745\n",
      "Episode finished with -119.0 reward\n",
      "Reward -1.0 at step 249, points reached 5, l2 distance 15.208677033866895\n",
      "Episode finished with -120.0 reward\n",
      "Reward -1.0 at step 250, points reached 5, l2 distance 15.221697245053951\n",
      "Episode finished with -121.0 reward\n",
      "Reward -1.0 at step 251, points reached 5, l2 distance 15.235362710473806\n",
      "Episode finished with -122.0 reward\n",
      "Reward -1.0 at step 252, points reached 5, l2 distance 15.278693365444095\n",
      "Episode finished with -123.0 reward\n",
      "Reward -1.0 at step 253, points reached 5, l2 distance 15.293266007413104\n",
      "Episode finished with -124.0 reward\n",
      "Reward -1.0 at step 254, points reached 5, l2 distance 15.308478023361072\n",
      "Episode finished with -125.0 reward\n",
      "Reward -1.0 at step 255, points reached 5, l2 distance 15.344635569898841\n",
      "Episode finished with -126.0 reward\n",
      "Reward -1.0 at step 256, points reached 5, l2 distance 15.361073798710212\n",
      "Episode finished with -127.0 reward\n",
      "Reward -1.0 at step 257, points reached 5, l2 distance 15.378144742648187\n",
      "Episode finished with -128.0 reward\n",
      "Reward -1.0 at step 258, points reached 5, l2 distance 15.41603527576023\n",
      "Episode finished with -129.0 reward\n",
      "Reward -1.0 at step 259, points reached 5, l2 distance 15.434316452570402\n",
      "Episode finished with -130.0 reward\n",
      "Reward -1.0 at step 260, points reached 5, l2 distance 15.453223129583257\n",
      "Episode finished with -131.0 reward\n",
      "Reward -1.0 at step 261, points reached 5, l2 distance 15.472753013839798\n",
      "Episode finished with -132.0 reward\n",
      "Reward -1.0 at step 262, points reached 5, l2 distance 15.492903748551791\n",
      "Episode finished with -133.0 reward\n",
      "Reward -1.0 at step 263, points reached 5, l2 distance 15.513672914448458\n",
      "Episode finished with -134.0 reward\n",
      "Reward -1.0 at step 264, points reached 5, l2 distance 15.535058031146626\n",
      "Episode finished with -135.0 reward\n",
      "Reward -1.0 at step 265, points reached 5, l2 distance 15.55705655854265\n",
      "Episode finished with -136.0 reward\n",
      "Reward -1.0 at step 266, points reached 5, l2 distance 15.599469525295886\n",
      "Episode finished with -137.0 reward\n",
      "Reward -1.0 at step 267, points reached 5, l2 distance 15.642406789735885\n",
      "Episode finished with -138.0 reward\n",
      "Reward -1.0 at step 268, points reached 5, l2 distance 15.685864046348087\n",
      "Episode finished with -139.0 reward\n",
      "Reward -1.0 at step 269, points reached 5, l2 distance 15.729836985342722\n",
      "Episode finished with -140.0 reward\n",
      "Reward -1.0 at step 270, points reached 5, l2 distance 15.774321294128505\n",
      "Episode finished with -141.0 reward\n",
      "Reward -1.0 at step 271, points reached 5, l2 distance 15.819312658751524\n",
      "Episode finished with -142.0 reward\n",
      "Reward -1.0 at step 272, points reached 5, l2 distance 15.864806765298987\n",
      "Episode finished with -143.0 reward\n",
      "Reward -1.0 at step 273, points reached 5, l2 distance 15.910799301267572\n",
      "Episode finished with -144.0 reward\n",
      "Reward -1.0 at step 274, points reached 5, l2 distance 15.897740716416497\n",
      "Episode finished with -145.0 reward\n",
      "Reward -1.0 at step 275, points reached 5, l2 distance 15.944769333363347\n",
      "Episode finished with -146.0 reward\n",
      "Reward -1.0 at step 276, points reached 5, l2 distance 15.932870522230811\n",
      "Episode finished with -147.0 reward\n",
      "Reward -1.0 at step 277, points reached 5, l2 distance 15.98092427829961\n",
      "Episode finished with -148.0 reward\n",
      "Reward -1.0 at step 278, points reached 5, l2 distance 16.029457835476542\n",
      "Episode finished with -149.0 reward\n",
      "Reward -1.0 at step 279, points reached 5, l2 distance 16.07846684886015\n",
      "Episode finished with -150.0 reward\n",
      "Reward -1.0 at step 280, points reached 5, l2 distance 16.10857139057513\n",
      "Episode finished with -151.0 reward\n",
      "Reward -1.0 at step 281, points reached 5, l2 distance 16.099443153071604\n",
      "Episode finished with -152.0 reward\n",
      "Reward -1.0 at step 282, points reached 5, l2 distance 16.14995199096767\n",
      "Episode finished with -153.0 reward\n",
      "Reward -1.0 at step 283, points reached 5, l2 distance 16.14196442218562\n",
      "Episode finished with -154.0 reward\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reward -1.0 at step 284, points reached 5, l2 distance 16.19345433752477\n",
      "Episode finished with -155.0 reward\n",
      "Reward -1.0 at step 285, points reached 5, l2 distance 16.18660242240053\n",
      "Episode finished with -156.0 reward\n",
      "Reward -1.0 at step 286, points reached 5, l2 distance 16.23906137860416\n",
      "Episode finished with -157.0 reward\n",
      "Reward -1.0 at step 287, points reached 5, l2 distance 16.29196522630968\n",
      "Episode finished with -158.0 reward\n",
      "Reward -1.0 at step 288, points reached 5, l2 distance 16.34530964565965\n",
      "Episode finished with -159.0 reward\n",
      "Reward -1.0 at step 289, points reached 5, l2 distance 16.399090337262766\n",
      "Episode finished with -160.0 reward\n",
      "Reward -1.0 at step 290, points reached 5, l2 distance 16.394894708424825\n",
      "Episode finished with -161.0 reward\n",
      "Reward -1.0 at step 291, points reached 5, l2 distance 16.44960964522757\n",
      "Episode finished with -162.0 reward\n",
      "Reward -1.0 at step 292, points reached 5, l2 distance 16.504749088077396\n",
      "Episode finished with -163.0 reward\n",
      "Reward -1.0 at step 293, points reached 5, l2 distance 16.502160103351834\n",
      "Episode finished with -164.0 reward\n",
      "Reward -1.0 at step 294, points reached 5, l2 distance 16.55821371886176\n",
      "Episode finished with -165.0 reward\n",
      "Reward -1.0 at step 295, points reached 5, l2 distance 16.614680106530688\n",
      "Episode finished with -166.0 reward\n",
      "Reward -1.0 at step 296, points reached 5, l2 distance 16.61367742747662\n",
      "Episode finished with -167.0 reward\n",
      "Reward -1.0 at step 297, points reached 5, l2 distance 16.671037749639584\n",
      "Episode finished with -168.0 reward\n",
      "Reward -1.0 at step 298, points reached 5, l2 distance 16.728799168965175\n",
      "Episode finished with -169.0 reward\n",
      "Reward -1.0 at step 299, points reached 5, l2 distance 16.72936165139315\n",
      "Episode finished with -170.0 reward\n",
      "Reward -1.0 at step 300, points reached 5, l2 distance 16.78799665688502\n",
      "Episode finished with -171.0 reward\n",
      "Reward -1.0 at step 301, points reached 5, l2 distance 16.847021168149034\n",
      "Episode finished with -172.0 reward\n",
      "Reward -1.0 at step 302, points reached 5, l2 distance 16.90643110560418\n",
      "Episode finished with -173.0 reward\n",
      "Reward -1.0 at step 303, points reached 5, l2 distance 16.90900463849146\n",
      "Episode finished with -174.0 reward\n",
      "Reward -1.0 at step 304, points reached 5, l2 distance 16.973046063962162\n",
      "Episode finished with -175.0 reward\n",
      "Reward -1.0 at step 305, points reached 5, l2 distance 16.976643620512714\n",
      "Episode finished with -176.0 reward\n",
      "Reward -1.0 at step 306, points reached 5, l2 distance 17.041461035817306\n",
      "Episode finished with -177.0 reward\n",
      "Reward -1.0 at step 307, points reached 5, l2 distance 17.046074062854522\n",
      "Episode finished with -178.0 reward\n",
      "Reward -1.0 at step 308, points reached 5, l2 distance 17.11165443100797\n",
      "Episode finished with -179.0 reward\n",
      "Reward -1.0 at step 309, points reached 5, l2 distance 17.173763311228395\n",
      "Episode finished with -180.0 reward\n",
      "Reward -1.0 at step 310, points reached 5, l2 distance 17.179830391668087\n",
      "Episode finished with -181.0 reward\n",
      "Reward -1.0 at step 311, points reached 5, l2 distance 17.246460616347747\n",
      "Episode finished with -182.0 reward\n",
      "Reward -1.0 at step 312, points reached 5, l2 distance 17.253519658323352\n",
      "Episode finished with -183.0 reward\n",
      "Reward -1.0 at step 313, points reached 5, l2 distance 17.320879968911232\n",
      "Episode finished with -184.0 reward\n",
      "Reward -1.0 at step 314, points reached 5, l2 distance 17.32892179273931\n",
      "Episode finished with -185.0 reward\n",
      "Reward -1.0 at step 315, points reached 5, l2 distance 17.337536679134107\n",
      "Episode finished with -186.0 reward\n",
      "Reward -1.0 at step 316, points reached 5, l2 distance 17.40233230403524\n",
      "Episode finished with -187.0 reward\n",
      "Reward -1.0 at step 317, points reached 5, l2 distance 17.41194663799343\n",
      "Episode finished with -188.0 reward\n",
      "Reward -1.0 at step 318, points reached 5, l2 distance 17.477498269114367\n",
      "Episode finished with -189.0 reward\n",
      "Reward -1.0 at step 319, points reached 5, l2 distance 17.488102537267302\n",
      "Episode finished with -190.0 reward\n",
      "Reward -1.0 at step 320, points reached 5, l2 distance 17.554397138020924\n",
      "Episode finished with -191.0 reward\n",
      "Reward -1.0 at step 321, points reached 5, l2 distance 17.5659816688181\n",
      "Episode finished with -192.0 reward\n",
      "Reward -1.0 at step 322, points reached 5, l2 distance 17.633006238796966\n",
      "Episode finished with -193.0 reward\n",
      "Reward -1.0 at step 323, points reached 5, l2 distance 17.645561216072622\n",
      "Episode finished with -194.0 reward\n",
      "Reward -1.0 at step 324, points reached 5, l2 distance 17.71330280216489\n",
      "Episode finished with -195.0 reward\n",
      "Reward -1.0 at step 325, points reached 5, l2 distance 17.72681827843658\n",
      "Episode finished with -196.0 reward\n",
      "Reward -1.0 at step 326, points reached 5, l2 distance 17.740887136562357\n",
      "Episode finished with -197.0 reward\n",
      "Reward -1.0 at step 327, points reached 5, l2 distance 17.81297635268463\n",
      "Episode finished with -198.0 reward\n",
      "Reward -1.0 at step 328, points reached 5, l2 distance 17.82796195445124\n",
      "Episode finished with -199.0 reward\n",
      "Reward -1.0 at step 329, points reached 5, l2 distance 17.8434954074985\n",
      "Episode finished with -200.0 reward\n",
      "Reward -1.0 at step 330, points reached 5, l2 distance 17.859575282337463\n",
      "Episode finished with -201.0 reward\n",
      "Reward -1.0 at step 331, points reached 5, l2 distance 17.933008999945407\n",
      "Episode finished with -202.0 reward\n",
      "Reward -1.0 at step 332, points reached 5, l2 distance 17.94998672681155\n",
      "Episode finished with -203.0 reward\n",
      "Reward -1.0 at step 333, points reached 5, l2 distance 17.967504979637503\n",
      "Episode finished with -204.0 reward\n",
      "Reward -1.0 at step 334, points reached 5, l2 distance 17.985562178978068\n",
      "Episode finished with -205.0 reward\n",
      "Reward -1.0 at step 335, points reached 5, l2 distance 18.057174448181506\n",
      "Episode finished with -206.0 reward\n",
      "Reward -1.0 at step 336, points reached 5, l2 distance 18.07613977749298\n",
      "Episode finished with -207.0 reward\n",
      "Reward -1.0 at step 337, points reached 5, l2 distance 18.095637857198312\n",
      "Episode finished with -208.0 reward\n",
      "Reward -1.0 at step 338, points reached 5, l2 distance 18.115666967083314\n",
      "Episode finished with -209.0 reward\n",
      "Reward -1.0 at step 339, points reached 5, l2 distance 18.18864198693508\n",
      "Episode finished with -210.0 reward\n",
      "Reward -1.0 at step 340, points reached 5, l2 distance 18.209559193871762\n",
      "Episode finished with -211.0 reward\n",
      "Reward -1.0 at step 341, points reached 5, l2 distance 18.231000925383707\n",
      "Episode finished with -212.0 reward\n",
      "Reward -1.0 at step 342, points reached 5, l2 distance 18.307811234902815\n",
      "Episode finished with -213.0 reward\n",
      "Reward -1.0 at step 343, points reached 5, l2 distance 18.330095791118612\n",
      "Episode finished with -214.0 reward\n",
      "Reward -1.0 at step 344, points reached 5, l2 distance 18.35289816923569\n",
      "Episode finished with -215.0 reward\n",
      "Reward -1.0 at step 345, points reached 5, l2 distance 18.376216441615107\n",
      "Episode finished with -216.0 reward\n",
      "Reward -1.0 at step 346, points reached 5, l2 distance 18.400048646887754\n",
      "Episode finished with -217.0 reward\n",
      "Reward -1.0 at step 347, points reached 5, l2 distance 18.424392790715963\n",
      "Episode finished with -218.0 reward\n",
      "Reward -1.0 at step 348, points reached 5, l2 distance 18.44924684656089\n",
      "Episode finished with -219.0 reward\n",
      "Reward -1.0 at step 349, points reached 5, l2 distance 18.4746087564551\n",
      "Episode finished with -220.0 reward\n",
      "Reward -1.0 at step 350, points reached 5, l2 distance 18.534352118953777\n",
      "Episode finished with -221.0 reward\n",
      "Reward -1.0 at step 351, points reached 5, l2 distance 18.56059713665988\n",
      "Episode finished with -222.0 reward\n",
      "Reward -1.0 at step 352, points reached 5, l2 distance 18.5873431040966\n",
      "Episode finished with -223.0 reward\n",
      "Reward -1.0 at step 353, points reached 5, l2 distance 18.614587861927752\n",
      "Episode finished with -224.0 reward\n",
      "Reward -1.0 at step 354, points reached 5, l2 distance 18.642329223287753\n",
      "Episode finished with -225.0 reward\n",
      "Reward -1.0 at step 355, points reached 5, l2 distance 18.670564974565423\n",
      "Episode finished with -226.0 reward\n",
      "Reward -1.0 at step 356, points reached 5, l2 distance 18.699292876188917\n",
      "Episode finished with -227.0 reward\n",
      "Reward -1.0 at step 357, points reached 5, l2 distance 18.728510663411285\n",
      "Episode finished with -228.0 reward\n",
      "Reward -1.0 at step 358, points reached 5, l2 distance 18.75821604709602\n",
      "Episode finished with -229.0 reward\n",
      "Reward -1.0 at step 359, points reached 5, l2 distance 18.839720374343432\n",
      "Episode finished with -230.0 reward\n",
      "Reward -1.0 at step 360, points reached 5, l2 distance 18.921402122923435\n",
      "Episode finished with -231.0 reward\n",
      "Reward -1.0 at step 361, points reached 5, l2 distance 18.9521070858984\n",
      "Episode finished with -232.0 reward\n",
      "Reward -1.0 at step 362, points reached 5, l2 distance 18.98328916951299\n",
      "Episode finished with -233.0 reward\n",
      "Reward -1.0 at step 363, points reached 5, l2 distance 19.014946026516565\n",
      "Episode finished with -234.0 reward\n",
      "Reward -1.0 at step 364, points reached 5, l2 distance 19.047075289642947\n",
      "Episode finished with -235.0 reward\n",
      "Reward -1.0 at step 365, points reached 5, l2 distance 19.079674572364063\n",
      "Episode finished with -236.0 reward\n",
      "Reward -1.0 at step 366, points reached 5, l2 distance 19.112741469640213\n",
      "Episode finished with -237.0 reward\n",
      "Reward -1.0 at step 367, points reached 5, l2 distance 19.146273558666543\n",
      "Episode finished with -238.0 reward\n",
      "Reward -1.0 at step 368, points reached 5, l2 distance 19.180268399615333\n",
      "Episode finished with -239.0 reward\n",
      "Reward -1.0 at step 369, points reached 5, l2 distance 19.214723536373647\n",
      "Episode finished with -240.0 reward\n",
      "Reward -1.0 at step 370, points reached 5, l2 distance 19.249636497276\n",
      "Episode finished with -241.0 reward\n",
      "Reward -1.0 at step 371, points reached 5, l2 distance 19.285004795831636\n",
      "Episode finished with -242.0 reward\n",
      "Reward -1.0 at step 372, points reached 5, l2 distance 19.32082593144605\n",
      "Episode finished with -243.0 reward\n",
      "Reward -1.0 at step 373, points reached 5, l2 distance 19.35709739013644\n",
      "Episode finished with -244.0 reward\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reward -1.0 at step 374, points reached 5, l2 distance 19.270530472956562\n",
      "Episode finished with -245.0 reward\n",
      "Reward -1.0 at step 375, points reached 5, l2 distance 19.30748792439736\n",
      "Episode finished with -246.0 reward\n",
      "Reward -1.0 at step 376, points reached 5, l2 distance 19.34489170795102\n",
      "Episode finished with -247.0 reward\n",
      "Reward -1.0 at step 377, points reached 5, l2 distance 19.300915369895378\n",
      "Episode finished with -248.0 reward\n",
      "Reward -1.0 at step 378, points reached 5, l2 distance 19.21505526849955\n",
      "Episode finished with -249.0 reward\n",
      "Reward -1.0 at step 379, points reached 5, l2 distance 19.253246402256856\n",
      "Episode finished with -250.0 reward\n",
      "Reward -1.0 at step 380, points reached 5, l2 distance 19.291880288881664\n",
      "Episode finished with -251.0 reward\n",
      "Reward -1.0 at step 381, points reached 5, l2 distance 19.33095427378238\n",
      "Episode finished with -252.0 reward\n",
      "Reward -1.0 at step 382, points reached 5, l2 distance 19.37046569366853\n",
      "Episode finished with -253.0 reward\n",
      "Reward -1.0 at step 383, points reached 5, l2 distance 19.410411877240364\n",
      "Episode finished with -254.0 reward\n",
      "Reward -1.0 at step 384, points reached 5, l2 distance 19.450790145869092\n",
      "Episode finished with -255.0 reward\n",
      "Reward -1.0 at step 385, points reached 5, l2 distance 19.454366766169034\n",
      "Episode finished with -256.0 reward\n",
      "Reward -1.0 at step 386, points reached 5, l2 distance 19.49530133317737\n",
      "Episode finished with -257.0 reward\n",
      "Reward -1.0 at step 387, points reached 5, l2 distance 19.411433449248236\n",
      "Episode finished with -258.0 reward\n",
      "Reward -1.0 at step 388, points reached 5, l2 distance 19.415772276556538\n",
      "Episode finished with -259.0 reward\n",
      "Reward -1.0 at step 389, points reached 5, l2 distance 19.457509634630032\n",
      "Episode finished with -260.0 reward\n",
      "Reward -1.0 at step 390, points reached 5, l2 distance 19.462486620842398\n",
      "Episode finished with -261.0 reward\n",
      "Reward -1.0 at step 391, points reached 5, l2 distance 19.504771039346483\n",
      "Episode finished with -262.0 reward\n",
      "Reward -1.0 at step 392, points reached 5, l2 distance 19.547475569192795\n",
      "Episode finished with -263.0 reward\n",
      "Reward -1.0 at step 393, points reached 5, l2 distance 19.59059746304459\n",
      "Episode finished with -264.0 reward\n",
      "Reward -1.0 at step 394, points reached 5, l2 distance 19.634133970967724\n",
      "Episode finished with -265.0 reward\n",
      "Reward -1.0 at step 395, points reached 5, l2 distance 19.551944804436943\n",
      "Episode finished with -266.0 reward\n",
      "Reward -1.0 at step 396, points reached 5, l2 distance 19.59614997927455\n",
      "Episode finished with -267.0 reward\n",
      "Reward -1.0 at step 397, points reached 5, l2 distance 19.640764811602814\n",
      "Episode finished with -268.0 reward\n",
      "Reward -1.0 at step 398, points reached 5, l2 distance 19.68578651614328\n",
      "Episode finished with -269.0 reward\n",
      "Reward -1.0 at step 399, points reached 5, l2 distance 19.604541342144394\n",
      "Episode finished with -270.0 reward\n",
      "Reward -1.0 at step 400, points reached 5, l2 distance 19.650227229066218\n",
      "Episode finished with -271.0 reward\n",
      "Reward -1.0 at step 401, points reached 5, l2 distance 19.696314860197127\n",
      "Episode finished with -272.0 reward\n",
      "Reward -1.0 at step 402, points reached 5, l2 distance 19.74280142204011\n",
      "Episode finished with -273.0 reward\n",
      "Reward -1.0 at step 403, points reached 5, l2 distance 19.789684103294718\n",
      "Episode finished with -274.0 reward\n",
      "Reward -1.0 at step 404, points reached 5, l2 distance 19.836960095394716\n",
      "Episode finished with -275.0 reward\n",
      "Reward -1.0 at step 405, points reached 5, l2 distance 19.797343521970436\n",
      "Episode finished with -276.0 reward\n",
      "Reward -1.0 at step 406, points reached 5, l2 distance 19.845119342405077\n",
      "Episode finished with -277.0 reward\n",
      "Reward -1.0 at step 407, points reached 5, l2 distance 19.806038187487957\n",
      "Episode finished with -278.0 reward\n",
      "Reward -1.0 at step 408, points reached 5, l2 distance 19.85431092082863\n",
      "Episode finished with -279.0 reward\n",
      "Reward -1.0 at step 409, points reached 5, l2 distance 19.815766737125635\n",
      "Episode finished with -280.0 reward\n",
      "Reward -1.0 at step 410, points reached 5, l2 distance 19.736918983306197\n",
      "Episode finished with -281.0 reward\n",
      "Reward -1.0 at step 411, points reached 5, l2 distance 19.785951764401386\n",
      "Episode finished with -282.0 reward\n",
      "Reward -1.0 at step 412, points reached 5, l2 distance 19.748078568869623\n",
      "Episode finished with -283.0 reward\n",
      "Reward -1.0 at step 413, points reached 5, l2 distance 19.79760302918467\n",
      "Episode finished with -284.0 reward\n",
      "Reward -1.0 at step 414, points reached 5, l2 distance 19.847507758913117\n",
      "Episode finished with -285.0 reward\n",
      "Reward -1.0 at step 415, points reached 5, l2 distance 19.897789896846607\n",
      "Episode finished with -286.0 reward\n",
      "Reward -1.0 at step 416, points reached 5, l2 distance 19.948446589095234\n",
      "Episode finished with -287.0 reward\n",
      "Reward -1.0 at step 417, points reached 5, l2 distance 19.99947498953209\n",
      "Episode finished with -288.0 reward\n",
      "Reward -1.0 at step 418, points reached 5, l2 distance 19.962578268913358\n",
      "Episode finished with -289.0 reward\n",
      "Reward -1.0 at step 419, points reached 5, l2 distance 20.014084339552852\n",
      "Episode finished with -290.0 reward\n",
      "Reward -1.0 at step 420, points reached 5, l2 distance 19.93758340898939\n",
      "Episode finished with -291.0 reward\n",
      "Reward -1.0 at step 421, points reached 5, l2 distance 19.861291308222718\n",
      "Episode finished with -292.0 reward\n",
      "Reward -1.0 at step 422, points reached 5, l2 distance 19.9137041839421\n",
      "Episode finished with -293.0 reward\n",
      "Reward -1.0 at step 423, points reached 5, l2 distance 19.966480316295684\n",
      "Episode finished with -294.0 reward\n",
      "Reward -1.0 at step 424, points reached 5, l2 distance 19.890944611755472\n",
      "Episode finished with -295.0 reward\n",
      "Reward -1.0 at step 425, points reached 5, l2 distance 19.944353586592054\n",
      "Episode finished with -296.0 reward\n",
      "Reward -1.0 at step 426, points reached 5, l2 distance 19.998119972289658\n",
      "Episode finished with -297.0 reward\n",
      "Reward -1.0 at step 427, points reached 5, l2 distance 20.052240893854577\n",
      "Episode finished with -298.0 reward\n",
      "Reward -1.0 at step 428, points reached 5, l2 distance 20.10671348839131\n",
      "Episode finished with -299.0 reward\n",
      "Reward -1.0 at step 429, points reached 5, l2 distance 20.032489381143122\n",
      "Episode finished with -300.0 reward\n",
      "Reward -1.0 at step 430, points reached 5, l2 distance 20.0875840704727\n",
      "Episode finished with -301.0 reward\n",
      "Reward -1.0 at step 431, points reached 5, l2 distance 20.052578311785098\n",
      "Episode finished with -302.0 reward\n",
      "Reward -1.0 at step 432, points reached 5, l2 distance 20.10812925665337\n",
      "Episode finished with -303.0 reward\n",
      "Reward -1.0 at step 433, points reached 5, l2 distance 20.07367150357785\n",
      "Episode finished with -304.0 reward\n",
      "Reward -1.0 at step 434, points reached 5, l2 distance 20.129674988803647\n",
      "Episode finished with -305.0 reward\n",
      "Reward -1.0 at step 435, points reached 5, l2 distance 20.18601849489346\n",
      "Episode finished with -306.0 reward\n",
      "Reward -1.0 at step 436, points reached 5, l2 distance 20.11342400519991\n",
      "Episode finished with -307.0 reward\n",
      "Reward -1.0 at step 437, points reached 5, l2 distance 20.170379105953344\n",
      "Episode finished with -308.0 reward\n",
      "Reward -1.0 at step 438, points reached 5, l2 distance 20.136844686980137\n",
      "Episode finished with -309.0 reward\n",
      "Reward -1.0 at step 439, points reached 5, l2 distance 20.19424284992621\n",
      "Episode finished with -310.0 reward\n",
      "Reward -1.0 at step 440, points reached 5, l2 distance 20.1225946720664\n",
      "Episode finished with -311.0 reward\n",
      "Reward -1.0 at step 441, points reached 5, l2 distance 20.180599277873892\n",
      "Episode finished with -312.0 reward\n",
      "Reward -1.0 at step 442, points reached 5, l2 distance 20.238931742855733\n",
      "Episode finished with -313.0 reward\n",
      "Reward -1.0 at step 443, points reached 5, l2 distance 20.29758924034632\n",
      "Episode finished with -314.0 reward\n",
      "Reward -1.0 at step 444, points reached 5, l2 distance 20.227012202872228\n",
      "Episode finished with -315.0 reward\n",
      "Reward -1.0 at step 445, points reached 5, l2 distance 20.286267130169726\n",
      "Episode finished with -316.0 reward\n",
      "Reward -1.0 at step 446, points reached 5, l2 distance 20.3458409877318\n",
      "Episode finished with -317.0 reward\n",
      "Reward -1.0 at step 447, points reached 5, l2 distance 20.276065206805132\n",
      "Episode finished with -318.0 reward\n",
      "Reward -1.0 at step 448, points reached 5, l2 distance 20.3362305316322\n",
      "Episode finished with -319.0 reward\n",
      "Reward -1.0 at step 449, points reached 5, l2 distance 20.26698516187964\n",
      "Episode finished with -320.0 reward\n",
      "Reward -1.0 at step 450, points reached 5, l2 distance 20.32773917730496\n",
      "Episode finished with -321.0 reward\n",
      "Reward -1.0 at step 451, points reached 5, l2 distance 20.25902850386247\n",
      "Episode finished with -322.0 reward\n",
      "Reward -1.0 at step 452, points reached 5, l2 distance 20.32036832768085\n",
      "Episode finished with -323.0 reward\n",
      "Reward -1.0 at step 453, points reached 5, l2 distance 20.252196556818266\n",
      "Episode finished with -324.0 reward\n",
      "Reward -1.0 at step 454, points reached 5, l2 distance 20.314119202464035\n",
      "Episode finished with -325.0 reward\n",
      "Reward -1.0 at step 455, points reached 5, l2 distance 20.246490459309683\n",
      "Episode finished with -326.0 reward\n",
      "Reward -1.0 at step 456, points reached 5, l2 distance 20.30899283712631\n",
      "Episode finished with -327.0 reward\n",
      "Reward -1.0 at step 457, points reached 5, l2 distance 20.24191116345147\n",
      "Episode finished with -328.0 reward\n",
      "Reward -1.0 at step 458, points reached 5, l2 distance 20.25786723991613\n",
      "Episode finished with -329.0 reward\n",
      "Reward -1.0 at step 459, points reached 5, l2 distance 20.321025553289655\n",
      "Episode finished with -330.0 reward\n",
      "Reward -1.0 at step 460, points reached 5, l2 distance 20.254648192302955\n",
      "Episode finished with -331.0 reward\n",
      "Reward -1.0 at step 461, points reached 5, l2 distance 20.318378586023016\n",
      "Episode finished with -332.0 reward\n",
      "Reward -1.0 at step 462, points reached 5, l2 distance 20.382400333031338\n",
      "Episode finished with -333.0 reward\n",
      "Reward -1.0 at step 463, points reached 5, l2 distance 20.31685552145541\n",
      "Episode finished with -334.0 reward\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reward -1.0 at step 464, points reached 5, l2 distance 20.381442387483375\n",
      "Episode finished with -335.0 reward\n",
      "Reward -1.0 at step 465, points reached 5, l2 distance 20.3164566123538\n",
      "Episode finished with -336.0 reward\n",
      "Reward -1.0 at step 466, points reached 5, l2 distance 20.381605072709036\n",
      "Episode finished with -337.0 reward\n",
      "Reward -1.0 at step 467, points reached 5, l2 distance 20.317181924933454\n",
      "Episode finished with -338.0 reward\n",
      "Reward -1.0 at step 468, points reached 5, l2 distance 20.3346158297172\n",
      "Episode finished with -339.0 reward\n",
      "Reward -1.0 at step 469, points reached 5, l2 distance 20.40039453756382\n",
      "Episode finished with -340.0 reward\n",
      "Reward -1.0 at step 470, points reached 5, l2 distance 20.33669269289194\n",
      "Episode finished with -341.0 reward\n",
      "Reward -1.0 at step 471, points reached 5, l2 distance 20.403024446753385\n",
      "Episode finished with -342.0 reward\n",
      "Reward -1.0 at step 472, points reached 5, l2 distance 20.33989231818326\n",
      "Episode finished with -343.0 reward\n",
      "Reward -1.0 at step 473, points reached 5, l2 distance 20.406773310560286\n",
      "Episode finished with -344.0 reward\n",
      "Reward -1.0 at step 474, points reached 5, l2 distance 20.379102446899363\n",
      "Episode finished with -345.0 reward\n",
      "Reward -1.0 at step 475, points reached 5, l2 distance 20.397993918018575\n",
      "Episode finished with -346.0 reward\n",
      "Reward -1.0 at step 476, points reached 5, l2 distance 20.417357694320437\n",
      "Episode finished with -347.0 reward\n",
      "Reward -1.0 at step 477, points reached 5, l2 distance 20.48474345479877\n",
      "Episode finished with -348.0 reward\n",
      "Reward -1.0 at step 478, points reached 5, l2 distance 20.458548190186836\n",
      "Episode finished with -349.0 reward\n",
      "Reward -1.0 at step 479, points reached 5, l2 distance 20.52629960588612\n",
      "Episode finished with -350.0 reward\n",
      "Reward -1.0 at step 480, points reached 5, l2 distance 20.464991536461586\n",
      "Episode finished with -351.0 reward\n",
      "Reward -1.0 at step 481, points reached 5, l2 distance 20.43954815861911\n",
      "Episode finished with -352.0 reward\n",
      "Reward -1.0 at step 482, points reached 5, l2 distance 20.507932933424463\n",
      "Episode finished with -353.0 reward\n",
      "Reward -1.0 at step 483, points reached 5, l2 distance 20.447402408442347\n",
      "Episode finished with -354.0 reward\n",
      "Reward -1.0 at step 484, points reached 5, l2 distance 20.51631765375339\n",
      "Episode finished with -355.0 reward\n",
      "Reward -1.0 at step 485, points reached 5, l2 distance 20.53762980017142\n",
      "Episode finished with -356.0 reward\n",
      "Reward -1.0 at step 486, points reached 5, l2 distance 20.47784427682084\n",
      "Episode finished with -357.0 reward\n",
      "Reward -1.0 at step 487, points reached 5, l2 distance 20.499783652204318\n",
      "Episode finished with -358.0 reward\n",
      "Reward -1.0 at step 488, points reached 5, l2 distance 20.522186855904092\n",
      "Episode finished with -359.0 reward\n",
      "Reward -1.0 at step 489, points reached 5, l2 distance 20.545052370585527\n",
      "Episode finished with -360.0 reward\n",
      "Reward -1.0 at step 490, points reached 5, l2 distance 20.614703520730476\n",
      "Episode finished with -361.0 reward\n",
      "Reward -1.0 at step 491, points reached 5, l2 distance 20.59224006309799\n",
      "Episode finished with -362.0 reward\n",
      "Reward -1.0 at step 492, points reached 5, l2 distance 20.662229730188415\n",
      "Episode finished with -363.0 reward\n",
      "Reward -1.0 at step 493, points reached 5, l2 distance 20.603997738897053\n",
      "Episode finished with -364.0 reward\n",
      "Reward -1.0 at step 494, points reached 5, l2 distance 20.674499993347176\n",
      "Episode finished with -365.0 reward\n",
      "Reward -1.0 at step 495, points reached 5, l2 distance 20.65288436302908\n",
      "Episode finished with -366.0 reward\n",
      "Reward -1.0 at step 496, points reached 5, l2 distance 20.723716412178515\n",
      "Episode finished with -367.0 reward\n",
      "Reward -1.0 at step 497, points reached 5, l2 distance 20.702648785588877\n",
      "Episode finished with -368.0 reward\n",
      "Reward -1.0 at step 498, points reached 5, l2 distance 20.7276861171035\n",
      "Episode finished with -369.0 reward\n",
      "Reward -1.0 at step 499, points reached 5, l2 distance 20.753175101629285\n",
      "Episode finished with -370.0 reward\n",
      "Reward -1.0 at step 500, points reached 5, l2 distance 20.779114077087268\n",
      "Episode finished with -371.0 reward\n",
      "Reward -1.0 at step 501, points reached 5, l2 distance 20.805501360417313\n",
      "Episode finished with -372.0 reward\n",
      "Reward -1.0 at step 502, points reached 5, l2 distance 20.83233524807162\n",
      "Episode finished with -373.0 reward\n",
      "Reward -1.0 at step 503, points reached 5, l2 distance 20.859614016509695\n",
      "Episode finished with -374.0 reward\n",
      "Reward -1.0 at step 504, points reached 5, l2 distance 20.8873359226946\n",
      "Episode finished with -375.0 reward\n",
      "Reward -1.0 at step 505, points reached 5, l2 distance 20.958741179904575\n",
      "Episode finished with -376.0 reward\n",
      "Reward -1.0 at step 506, points reached 5, l2 distance 20.986933473477944\n",
      "Episode finished with -377.0 reward\n",
      "Reward -1.0 at step 507, points reached 5, l2 distance 21.01556378958581\n",
      "Episode finished with -378.0 reward\n",
      "Reward -1.0 at step 508, points reached 5, l2 distance 21.044630340495683\n",
      "Episode finished with -379.0 reward\n",
      "Reward -1.0 at step 509, points reached 5, l2 distance 21.089848703826206\n",
      "Episode finished with -380.0 reward\n",
      "Reward -1.0 at step 510, points reached 5, l2 distance 21.135443466661595\n",
      "Episode finished with -381.0 reward\n",
      "Reward -1.0 at step 511, points reached 5, l2 distance 21.165734704517412\n",
      "Episode finished with -382.0 reward\n",
      "Reward -1.0 at step 512, points reached 5, l2 distance 21.21209520301997\n",
      "Episode finished with -383.0 reward\n",
      "Reward -1.0 at step 513, points reached 5, l2 distance 21.283289514211166\n",
      "Episode finished with -384.0 reward\n",
      "Reward -1.0 at step 514, points reached 5, l2 distance 21.354869307718893\n",
      "Episode finished with -385.0 reward\n",
      "Reward -1.0 at step 515, points reached 5, l2 distance 21.40184941216462\n",
      "Episode finished with -386.0 reward\n",
      "Reward -1.0 at step 516, points reached 5, l2 distance 21.4491928373118\n",
      "Episode finished with -387.0 reward\n",
      "Reward -1.0 at step 517, points reached 5, l2 distance 21.496897182696028\n",
      "Episode finished with -388.0 reward\n",
      "Reward -1.0 at step 518, points reached 5, l2 distance 21.544960050894066\n",
      "Episode finished with -389.0 reward\n",
      "Reward -1.0 at step 519, points reached 5, l2 distance 21.593379047889606\n",
      "Episode finished with -390.0 reward\n",
      "Reward -1.0 at step 520, points reached 5, l2 distance 21.642151783430744\n",
      "Episode finished with -391.0 reward\n",
      "Reward -1.0 at step 521, points reached 5, l2 distance 21.691275871379055\n",
      "Episode finished with -392.0 reward\n",
      "Reward -1.0 at step 522, points reached 5, l2 distance 21.725341492175154\n",
      "Episode finished with -393.0 reward\n",
      "Reward -1.0 at step 523, points reached 5, l2 distance 21.759813348808475\n",
      "Episode finished with -394.0 reward\n",
      "Reward -1.0 at step 524, points reached 5, l2 distance 21.794689513694376\n",
      "Episode finished with -395.0 reward\n",
      "Reward -1.0 at step 525, points reached 5, l2 distance 21.829968049030576\n",
      "Episode finished with -396.0 reward\n",
      "Reward -1.0 at step 526, points reached 5, l2 distance 21.865647007236074\n",
      "Episode finished with -397.0 reward\n",
      "Reward -1.0 at step 527, points reached 5, l2 distance 21.901724431386857\n",
      "Episode finished with -398.0 reward\n",
      "Reward -1.0 at step 528, points reached 5, l2 distance 21.938198355648172\n",
      "Episode finished with -399.0 reward\n",
      "Reward -1.0 at step 529, points reached 5, l2 distance 21.975066805703236\n",
      "Episode finished with -400.0 reward\n",
      "Reward -1.0 at step 530, points reached 5, l2 distance 22.01232779917827\n",
      "Episode finished with -401.0 reward\n",
      "Reward -1.0 at step 531, points reached 5, l2 distance 22.049979346063626\n",
      "Episode finished with -402.0 reward\n",
      "Reward -1.0 at step 532, points reached 5, l2 distance 22.088019449130982\n",
      "Episode finished with -403.0 reward\n",
      "Reward -1.0 at step 533, points reached 5, l2 distance 22.15955112849053\n",
      "Episode finished with -404.0 reward\n",
      "Reward -1.0 at step 534, points reached 5, l2 distance 22.152835372824367\n",
      "Episode finished with -405.0 reward\n",
      "Reward -1.0 at step 535, points reached 5, l2 distance 22.224621039643495\n",
      "Episode finished with -406.0 reward\n",
      "Reward -1.0 at step 536, points reached 5, l2 distance 22.173810380721097\n",
      "Episode finished with -407.0 reward\n",
      "Reward -1.0 at step 537, points reached 5, l2 distance 22.246041729802048\n",
      "Episode finished with -408.0 reward\n",
      "Reward -1.0 at step 538, points reached 5, l2 distance 22.24007901183053\n",
      "Episode finished with -409.0 reward\n",
      "Reward -1.0 at step 539, points reached 5, l2 distance 22.312556612044546\n",
      "Episode finished with -410.0 reward\n",
      "Reward -1.0 at step 540, points reached 5, l2 distance 22.30707257000282\n",
      "Episode finished with -411.0 reward\n",
      "Reward -1.0 at step 541, points reached 5, l2 distance 22.30203557348637\n",
      "Episode finished with -412.0 reward\n",
      "Reward -1.0 at step 542, points reached 5, l2 distance 22.37478454360838\n",
      "Episode finished with -413.0 reward\n",
      "Reward -1.0 at step 543, points reached 5, l2 distance 22.370222388699773\n",
      "Episode finished with -414.0 reward\n",
      "Reward -1.0 at step 544, points reached 5, l2 distance 22.44320843019193\n",
      "Episode finished with -415.0 reward\n",
      "Reward -1.0 at step 545, points reached 5, l2 distance 22.439118370767925\n",
      "Episode finished with -416.0 reward\n",
      "Reward -1.0 at step 546, points reached 5, l2 distance 22.512337738400173\n",
      "Episode finished with -417.0 reward\n",
      "Reward -1.0 at step 547, points reached 5, l2 distance 22.46418664814465\n",
      "Episode finished with -418.0 reward\n",
      "Reward -1.0 at step 548, points reached 5, l2 distance 22.53783129532999\n",
      "Episode finished with -419.0 reward\n",
      "Reward -1.0 at step 549, points reached 5, l2 distance 22.534475833312626\n",
      "Episode finished with -420.0 reward\n",
      "Reward -1.0 at step 550, points reached 5, l2 distance 22.487129238102607\n",
      "Episode finished with -421.0 reward\n",
      "Reward -1.0 at step 551, points reached 5, l2 distance 22.561217651494935\n",
      "Episode finished with -422.0 reward\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reward -1.0 at step 552, points reached 5, l2 distance 22.558569861658146\n",
      "Episode finished with -423.0 reward\n",
      "Reward -1.0 at step 553, points reached 5, l2 distance 22.632878672842686\n",
      "Episode finished with -424.0 reward\n",
      "Reward -1.0 at step 554, points reached 5, l2 distance 22.63069357253795\n",
      "Episode finished with -425.0 reward\n",
      "Reward -1.0 at step 555, points reached 5, l2 distance 22.705219150224583\n",
      "Episode finished with -426.0 reward\n",
      "Reward -1.0 at step 556, points reached 5, l2 distance 22.703493860715067\n",
      "Episode finished with -427.0 reward\n",
      "Reward -1.0 at step 557, points reached 5, l2 distance 22.657871936587274\n",
      "Episode finished with -428.0 reward\n",
      "Reward -1.0 at step 558, points reached 5, l2 distance 22.732823154930095\n",
      "Episode finished with -429.0 reward\n",
      "Reward -1.0 at step 559, points reached 5, l2 distance 22.73179878753367\n",
      "Episode finished with -430.0 reward\n",
      "Reward -1.0 at step 560, points reached 5, l2 distance 22.80695784809457\n",
      "Episode finished with -431.0 reward\n",
      "Reward -1.0 at step 561, points reached 5, l2 distance 22.806387615702373\n",
      "Episode finished with -432.0 reward\n",
      "Reward -1.0 at step 562, points reached 5, l2 distance 22.881750993200153\n",
      "Episode finished with -433.0 reward\n",
      "Reward -1.0 at step 563, points reached 5, l2 distance 22.837600447899103\n",
      "Episode finished with -434.0 reward\n",
      "Reward -1.0 at step 564, points reached 5, l2 distance 22.837726566157393\n",
      "Episode finished with -435.0 reward\n",
      "Reward -1.0 at step 565, points reached 5, l2 distance 22.794175327133846\n",
      "Episode finished with -436.0 reward\n",
      "Reward -1.0 at step 566, points reached 5, l2 distance 22.87015270254675\n",
      "Episode finished with -437.0 reward\n",
      "Reward -1.0 at step 567, points reached 5, l2 distance 22.87097321563484\n",
      "Episode finished with -438.0 reward\n",
      "Reward -1.0 at step 568, points reached 5, l2 distance 22.947144351602528\n",
      "Episode finished with -439.0 reward\n",
      "Reward -1.0 at step 569, points reached 5, l2 distance 22.948410126958077\n",
      "Episode finished with -440.0 reward\n",
      "Reward -1.0 at step 570, points reached 5, l2 distance 22.90611925745624\n",
      "Episode finished with -441.0 reward\n",
      "Reward -1.0 at step 571, points reached 5, l2 distance 22.982683052958098\n",
      "Episode finished with -442.0 reward\n",
      "Reward -1.0 at step 572, points reached 5, l2 distance 22.940953182857868\n",
      "Episode finished with -443.0 reward\n",
      "Reward -1.0 at step 573, points reached 5, l2 distance 23.017897261861464\n",
      "Episode finished with -444.0 reward\n",
      "Reward -1.0 at step 574, points reached 5, l2 distance 22.9767283904078\n",
      "Episode finished with -445.0 reward\n",
      "Reward -1.0 at step 575, points reached 5, l2 distance 22.935921622981148\n",
      "Episode finished with -446.0 reward\n",
      "Reward -1.0 at step 576, points reached 5, l2 distance 23.013440490337253\n",
      "Episode finished with -447.0 reward\n",
      "Reward -1.0 at step 577, points reached 5, l2 distance 23.01614774878569\n",
      "Episode finished with -448.0 reward\n",
      "Reward -1.0 at step 578, points reached 5, l2 distance 22.976151969748756\n",
      "Episode finished with -449.0 reward\n",
      "Reward -1.0 at step 579, points reached 5, l2 distance 23.05404313036374\n",
      "Episode finished with -450.0 reward\n",
      "Reward -1.0 at step 580, points reached 5, l2 distance 23.05743457863645\n",
      "Episode finished with -451.0 reward\n",
      "Reward -1.0 at step 581, points reached 5, l2 distance 23.135496470512372\n",
      "Episode finished with -452.0 reward\n",
      "Reward -1.0 at step 582, points reached 5, l2 distance 23.13932029895924\n",
      "Episode finished with -453.0 reward\n",
      "Reward -1.0 at step 583, points reached 5, l2 distance 23.217549693198166\n",
      "Episode finished with -454.0 reward\n",
      "Reward -1.0 at step 584, points reached 5, l2 distance 23.179000288860305\n",
      "Episode finished with -455.0 reward\n",
      "Reward -1.0 at step 585, points reached 5, l2 distance 23.183502152425387\n",
      "Episode finished with -456.0 reward\n",
      "Reward -1.0 at step 586, points reached 5, l2 distance 23.26208599250578\n",
      "Episode finished with -457.0 reward\n",
      "Reward -1.0 at step 587, points reached 5, l2 distance 23.26701365995578\n",
      "Episode finished with -458.0 reward\n",
      "Reward -1.0 at step 588, points reached 5, l2 distance 23.226318975528454\n",
      "Episode finished with -459.0 reward\n",
      "Reward -1.0 at step 589, points reached 5, l2 distance 23.231997150476083\n",
      "Episode finished with -460.0 reward\n",
      "Reward -1.0 at step 590, points reached 5, l2 distance 23.19510535745424\n",
      "Episode finished with -461.0 reward\n",
      "Reward -1.0 at step 591, points reached 5, l2 distance 23.1585866038296\n",
      "Episode finished with -462.0 reward\n",
      "Reward -1.0 at step 592, points reached 5, l2 distance 23.23782807788787\n",
      "Episode finished with -463.0 reward\n",
      "Reward -1.0 at step 593, points reached 5, l2 distance 23.20186878819809\n",
      "Episode finished with -464.0 reward\n",
      "Reward -1.0 at step 594, points reached 5, l2 distance 23.281453479084423\n",
      "Episode finished with -465.0 reward\n",
      "Reward -1.0 at step 595, points reached 5, l2 distance 23.288293719711817\n",
      "Episode finished with -466.0 reward\n",
      "Reward -1.0 at step 596, points reached 5, l2 distance 23.253144622544074\n",
      "Episode finished with -467.0 reward\n",
      "Reward -1.0 at step 597, points reached 5, l2 distance 23.333055918243623\n",
      "Episode finished with -468.0 reward\n",
      "Reward -1.0 at step 598, points reached 5, l2 distance 23.340561635497767\n",
      "Episode finished with -469.0 reward\n",
      "Reward -1.0 at step 599, points reached 5, l2 distance 23.34849323697477\n",
      "Episode finished with -470.0 reward\n",
      "Reward -1.0 at step 600, points reached 5, l2 distance 23.356850288804843\n",
      "Episode finished with -471.0 reward\n",
      "Reward -1.0 at step 601, points reached 5, l2 distance 23.319944261802945\n",
      "Episode finished with -472.0 reward\n",
      "Reward -1.0 at step 602, points reached 5, l2 distance 23.32905137092126\n",
      "Episode finished with -473.0 reward\n",
      "Reward -1.0 at step 603, points reached 5, l2 distance 23.40917539309285\n",
      "Episode finished with -474.0 reward\n",
      "Reward -1.0 at step 604, points reached 5, l2 distance 23.376129541240864\n",
      "Episode finished with -475.0 reward\n",
      "Reward -1.0 at step 605, points reached 5, l2 distance 23.385894050092475\n",
      "Episode finished with -476.0 reward\n",
      "Reward -1.0 at step 606, points reached 5, l2 distance 23.46632263590896\n",
      "Episode finished with -477.0 reward\n",
      "Reward -1.0 at step 607, points reached 5, l2 distance 23.476487567674138\n",
      "Episode finished with -478.0 reward\n",
      "Reward -1.0 at step 608, points reached 5, l2 distance 23.55704328465034\n",
      "Episode finished with -479.0 reward\n",
      "Reward -1.0 at step 609, points reached 5, l2 distance 23.525227437951987\n",
      "Episode finished with -480.0 reward\n",
      "Reward -1.0 at step 610, points reached 5, l2 distance 23.53604187559463\n",
      "Episode finished with -481.0 reward\n",
      "Reward -1.0 at step 611, points reached 5, l2 distance 23.54727602785764\n",
      "Episode finished with -482.0 reward\n",
      "Reward -1.0 at step 612, points reached 5, l2 distance 23.513449359974558\n",
      "Episode finished with -483.0 reward\n",
      "Reward -1.0 at step 613, points reached 5, l2 distance 23.52542795537126\n",
      "Episode finished with -484.0 reward\n",
      "Reward -1.0 at step 614, points reached 5, l2 distance 23.53782530657068\n",
      "Episode finished with -485.0 reward\n",
      "Reward -1.0 at step 615, points reached 5, l2 distance 23.55064075225799\n",
      "Episode finished with -486.0 reward\n",
      "Reward -1.0 at step 616, points reached 5, l2 distance 23.51817016403669\n",
      "Episode finished with -487.0 reward\n",
      "Reward -1.0 at step 617, points reached 5, l2 distance 23.531729771964528\n",
      "Episode finished with -488.0 reward\n",
      "Reward -1.0 at step 618, points reached 5, l2 distance 23.612645425843837\n",
      "Episode finished with -489.0 reward\n",
      "Reward -1.0 at step 619, points reached 5, l2 distance 23.626585967340027\n",
      "Episode finished with -490.0 reward\n",
      "Reward -1.0 at step 620, points reached 5, l2 distance 23.64094128704679\n",
      "Episode finished with -491.0 reward\n",
      "Reward -1.0 at step 621, points reached 5, l2 distance 23.609937537068983\n",
      "Episode finished with -492.0 reward\n",
      "Reward -1.0 at step 622, points reached 5, l2 distance 23.625033534085922\n",
      "Episode finished with -493.0 reward\n",
      "Reward -1.0 at step 623, points reached 5, l2 distance 23.640542897088253\n",
      "Episode finished with -494.0 reward\n",
      "Reward -1.0 at step 624, points reached 5, l2 distance 23.610577101578365\n",
      "Episode finished with -495.0 reward\n",
      "Reward -1.0 at step 625, points reached 5, l2 distance 23.626826459955694\n",
      "Episode finished with -496.0 reward\n",
      "Reward -1.0 at step 626, points reached 5, l2 distance 23.597574656863998\n",
      "Episode finished with -497.0 reward\n",
      "Reward -1.0 at step 627, points reached 5, l2 distance 23.614563847389952\n",
      "Episode finished with -498.0 reward\n",
      "Reward -1.0 at step 628, points reached 5, l2 distance 23.586028604594812\n",
      "Episode finished with -499.0 reward\n",
      "Reward -1.0 at step 629, points reached 5, l2 distance 23.557883287151938\n",
      "Episode finished with -500.0 reward\n",
      "Reward -1.0 at step 630, points reached 5, l2 distance 23.575941084526413\n",
      "Episode finished with -501.0 reward\n",
      "Reward -1.0 at step 631, points reached 5, l2 distance 23.504808795692345\n",
      "Episode finished with -502.0 reward\n",
      "Reward -1.0 at step 632, points reached 5, l2 distance 23.477505927911075\n",
      "Episode finished with -503.0 reward\n",
      "Reward -1.0 at step 633, points reached 5, l2 distance 23.49655805603466\n",
      "Episode finished with -504.0 reward\n",
      "Reward -1.0 at step 634, points reached 5, l2 distance 23.426015308243457\n",
      "Episode finished with -505.0 reward\n",
      "Reward -1.0 at step 635, points reached 5, l2 distance 23.399563451779095\n",
      "Episode finished with -506.0 reward\n",
      "Reward -1.0 at step 636, points reached 5, l2 distance 23.329362022934117\n",
      "Episode finished with -507.0 reward\n",
      "Reward -1.0 at step 637, points reached 5, l2 distance 23.349672705131578\n",
      "Episode finished with -508.0 reward\n",
      "Reward -1.0 at step 638, points reached 5, l2 distance 23.268080861972248\n",
      "Episode finished with -509.0 reward\n",
      "Reward -1.0 at step 639, points reached 5, l2 distance 23.198220691088174\n",
      "Episode finished with -510.0 reward\n",
      "Reward -1.0 at step 640, points reached 5, l2 distance 23.116902166698456\n",
      "Episode finished with -511.0 reward\n",
      "Reward -1.0 at step 641, points reached 5, l2 distance 23.04739406109992\n",
      "Episode finished with -512.0 reward\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reward -1.0 at step 642, points reached 5, l2 distance 23.02274306332596\n",
      "Episode finished with -513.0 reward\n",
      "Reward -1.0 at step 643, points reached 5, l2 distance 22.953704180284713\n",
      "Episode finished with -514.0 reward\n",
      "Reward -1.0 at step 644, points reached 5, l2 distance 22.976020957931603\n",
      "Episode finished with -515.0 reward\n",
      "Reward -1.0 at step 645, points reached 5, l2 distance 22.90748089192759\n",
      "Episode finished with -516.0 reward\n",
      "Reward -1.0 at step 646, points reached 5, l2 distance 22.93048164770943\n",
      "Episode finished with -517.0 reward\n",
      "Reward -1.0 at step 647, points reached 5, l2 distance 22.850470915068303\n",
      "Episode finished with -518.0 reward\n",
      "Reward -1.0 at step 648, points reached 5, l2 distance 22.770618206030903\n",
      "Episode finished with -519.0 reward\n",
      "Reward -1.0 at step 649, points reached 5, l2 distance 22.69092518892232\n",
      "Episode finished with -520.0 reward\n",
      "Reward -1.0 at step 650, points reached 5, l2 distance 22.611393552227444\n",
      "Episode finished with -521.0 reward\n",
      "Reward -1.0 at step 651, points reached 5, l2 distance 22.532025004827158\n",
      "Episode finished with -522.0 reward\n",
      "Reward -1.0 at step 652, points reached 5, l2 distance 22.50966055044062\n",
      "Episode finished with -523.0 reward\n",
      "Reward -1.0 at step 653, points reached 5, l2 distance 22.44270572774497\n",
      "Episode finished with -524.0 reward\n",
      "Reward -1.0 at step 654, points reached 5, l2 distance 22.46801858955532\n",
      "Episode finished with -525.0 reward\n",
      "Reward -1.0 at step 655, points reached 5, l2 distance 22.401213851406183\n",
      "Episode finished with -526.0 reward\n",
      "Reward -1.0 at step 656, points reached 5, l2 distance 22.334657030996937\n",
      "Episode finished with -527.0 reward\n",
      "Reward -1.0 at step 657, points reached 5, l2 distance 22.25640642808823\n",
      "Episode finished with -528.0 reward\n",
      "Reward -1.0 at step 658, points reached 5, l2 distance 22.178330629067496\n",
      "Episode finished with -529.0 reward\n",
      "Reward -1.0 at step 659, points reached 5, l2 distance 22.100431486561973\n",
      "Episode finished with -530.0 reward\n",
      "Reward -1.0 at step 660, points reached 5, l2 distance 22.02271087518987\n",
      "Episode finished with -531.0 reward\n",
      "Reward -1.0 at step 661, points reached 5, l2 distance 21.94517069179897\n",
      "Episode finished with -532.0 reward\n",
      "Reward -1.0 at step 662, points reached 5, l2 distance 21.86781285570596\n",
      "Episode finished with -533.0 reward\n",
      "Reward -1.0 at step 663, points reached 5, l2 distance 21.895709769531674\n",
      "Episode finished with -534.0 reward\n",
      "Reward -1.0 at step 664, points reached 5, l2 distance 21.818812212853256\n",
      "Episode finished with -535.0 reward\n",
      "Reward -1.0 at step 665, points reached 5, l2 distance 21.742102622514313\n",
      "Episode finished with -536.0 reward\n",
      "Reward -1.0 at step 666, points reached 5, l2 distance 21.665582995066927\n",
      "Episode finished with -537.0 reward\n",
      "Reward -1.0 at step 667, points reached 5, l2 distance 21.589255350389525\n",
      "Episode finished with -538.0 reward\n",
      "Reward -1.0 at step 668, points reached 5, l2 distance 21.513121731922407\n",
      "Episode finished with -539.0 reward\n",
      "Reward -1.0 at step 669, points reached 5, l2 distance 21.43718420690324\n",
      "Episode finished with -540.0 reward\n",
      "Reward -1.0 at step 670, points reached 5, l2 distance 21.36144486660233\n",
      "Episode finished with -541.0 reward\n",
      "Reward -1.0 at step 671, points reached 5, l2 distance 21.28590582655764\n",
      "Episode finished with -542.0 reward\n",
      "Reward -1.0 at step 672, points reached 5, l2 distance 21.222253285536464\n",
      "Episode finished with -543.0 reward\n",
      "Reward -1.0 at step 673, points reached 5, l2 distance 21.158881874377148\n",
      "Episode finished with -544.0 reward\n",
      "Reward -1.0 at step 674, points reached 5, l2 distance 21.083914347709793\n",
      "Episode finished with -545.0 reward\n",
      "Reward -1.0 at step 675, points reached 5, l2 distance 21.009155296424463\n",
      "Episode finished with -546.0 reward\n",
      "Reward -1.0 at step 676, points reached 5, l2 distance 20.947051280682608\n",
      "Episode finished with -547.0 reward\n",
      "Reward -1.0 at step 677, points reached 5, l2 distance 20.88524140252666\n",
      "Episode finished with -548.0 reward\n",
      "Reward -1.0 at step 678, points reached 5, l2 distance 20.823728281171952\n",
      "Episode finished with -549.0 reward\n",
      "Reward -1.0 at step 679, points reached 5, l2 distance 20.762514554223618\n",
      "Episode finished with -550.0 reward\n",
      "Reward -1.0 at step 680, points reached 5, l2 distance 20.701602877569997\n",
      "Episode finished with -551.0 reward\n",
      "Reward -1.0 at step 681, points reached 5, l2 distance 20.64099592526681\n",
      "Episode finished with -552.0 reward\n",
      "Reward -1.0 at step 682, points reached 5, l2 distance 20.580696389411806\n",
      "Episode finished with -553.0 reward\n",
      "Reward -1.0 at step 683, points reached 5, l2 distance 20.520706980009603\n",
      "Episode finished with -554.0 reward\n",
      "Reward -1.0 at step 684, points reached 5, l2 distance 20.461030424826646\n",
      "Episode finished with -555.0 reward\n",
      "Reward -1.0 at step 685, points reached 5, l2 distance 20.384077527832822\n",
      "Episode finished with -556.0 reward\n",
      "Reward -1.0 at step 686, points reached 5, l2 distance 20.311891751752167\n",
      "Episode finished with -557.0 reward\n",
      "Reward -1.0 at step 687, points reached 5, l2 distance 20.239942598957544\n",
      "Episode finished with -558.0 reward\n",
      "Reward -1.0 at step 688, points reached 5, l2 distance 20.168232601874042\n",
      "Episode finished with -559.0 reward\n",
      "Reward -1.0 at step 689, points reached 5, l2 distance 20.096764320593838\n",
      "Episode finished with -560.0 reward\n",
      "Reward -1.0 at step 690, points reached 5, l2 distance 20.025540343064208\n",
      "Episode finished with -561.0 reward\n",
      "Reward -1.0 at step 691, points reached 5, l2 distance 19.967720989390692\n",
      "Episode finished with -562.0 reward\n",
      "Reward -1.0 at step 692, points reached 5, l2 distance 19.89702099919828\n",
      "Episode finished with -563.0 reward\n",
      "Reward -1.0 at step 693, points reached 5, l2 distance 19.821246287146707\n",
      "Episode finished with -564.0 reward\n",
      "Reward -1.0 at step 694, points reached 5, l2 distance 19.745687228073805\n",
      "Episode finished with -565.0 reward\n",
      "Reward -1.0 at step 695, points reached 5, l2 distance 19.67034630712197\n",
      "Episode finished with -566.0 reward\n",
      "Reward -1.0 at step 696, points reached 5, l2 distance 19.600313581240016\n",
      "Episode finished with -567.0 reward\n",
      "Reward -1.0 at step 697, points reached 5, l2 distance 19.525335354839587\n",
      "Episode finished with -568.0 reward\n",
      "Reward -1.0 at step 698, points reached 5, l2 distance 19.450582226631564\n",
      "Episode finished with -569.0 reward\n",
      "Reward -1.0 at step 699, points reached 5, l2 distance 19.376056801913094\n",
      "Episode finished with -570.0 reward\n",
      "Reward -1.0 at step 700, points reached 5, l2 distance 19.32046189389411\n",
      "Episode finished with -571.0 reward\n",
      "Reward -1.0 at step 701, points reached 5, l2 distance 19.265225625355807\n",
      "Episode finished with -572.0 reward\n",
      "Reward -1.0 at step 702, points reached 5, l2 distance 19.210351089928515\n",
      "Episode finished with -573.0 reward\n",
      "Reward -1.0 at step 703, points reached 5, l2 distance 19.136752899542145\n",
      "Episode finished with -574.0 reward\n",
      "Reward -1.0 at step 704, points reached 5, l2 distance 19.082480977112393\n",
      "Episode finished with -575.0 reward\n",
      "Reward -1.0 at step 705, points reached 5, l2 distance 19.00936350546602\n",
      "Episode finished with -576.0 reward\n",
      "Reward -1.0 at step 706, points reached 5, l2 distance 18.93649179557899\n",
      "Episode finished with -577.0 reward\n",
      "Reward -1.0 at step 707, points reached 5, l2 distance 18.86386869560742\n",
      "Episode finished with -578.0 reward\n",
      "Reward -1.0 at step 708, points reached 5, l2 distance 18.791497087943913\n",
      "Episode finished with -579.0 reward\n",
      "Reward -1.0 at step 709, points reached 5, l2 distance 18.70216133119229\n",
      "Episode finished with -580.0 reward\n",
      "Reward -1.0 at step 710, points reached 5, l2 distance 18.612934054299007\n",
      "Episode finished with -581.0 reward\n",
      "Reward -1.0 at step 711, points reached 5, l2 distance 18.540913727449873\n",
      "Episode finished with -582.0 reward\n",
      "Reward -1.0 at step 712, points reached 5, l2 distance 18.46915400303421\n",
      "Episode finished with -583.0 reward\n",
      "Reward -1.0 at step 713, points reached 5, l2 distance 18.39765793047718\n",
      "Episode finished with -584.0 reward\n",
      "Reward -1.0 at step 714, points reached 5, l2 distance 18.326428595498506\n",
      "Episode finished with -585.0 reward\n",
      "Reward -1.0 at step 715, points reached 5, l2 distance 18.255469120374876\n",
      "Episode finished with -586.0 reward\n",
      "Reward -1.0 at step 716, points reached 5, l2 distance 18.16276920599547\n",
      "Episode finished with -587.0 reward\n",
      "Reward -1.0 at step 717, points reached 5, l2 distance 18.09193765648294\n",
      "Episode finished with -588.0 reward\n",
      "Reward -1.0 at step 718, points reached 5, l2 distance 18.021382607943085\n",
      "Episode finished with -589.0 reward\n",
      "Reward -1.0 at step 719, points reached 5, l2 distance 17.945012389236453\n",
      "Episode finished with -590.0 reward\n",
      "Reward -1.0 at step 720, points reached 5, l2 distance 17.868875403836302\n",
      "Episode finished with -591.0 reward\n",
      "Reward -1.0 at step 721, points reached 5, l2 distance 17.79829235129358\n",
      "Episode finished with -592.0 reward\n",
      "Reward -1.0 at step 722, points reached 5, l2 distance 17.727992358030985\n",
      "Episode finished with -593.0 reward\n",
      "Reward -1.0 at step 723, points reached 5, l2 distance 17.651782660197885\n",
      "Episode finished with -594.0 reward\n",
      "Reward -1.0 at step 724, points reached 5, l2 distance 17.581614637211022\n",
      "Episode finished with -595.0 reward\n",
      "Reward -1.0 at step 725, points reached 5, l2 distance 17.50548859735401\n",
      "Episode finished with -596.0 reward\n",
      "Reward -1.0 at step 726, points reached 5, l2 distance 17.435455627681833\n",
      "Episode finished with -597.0 reward\n",
      "Reward -1.0 at step 727, points reached 5, l2 distance 17.359415614294196\n",
      "Episode finished with -598.0 reward\n",
      "Reward -1.0 at step 728, points reached 5, l2 distance 17.2895208761496\n",
      "Episode finished with -599.0 reward\n",
      "Reward -1.0 at step 729, points reached 5, l2 distance 17.21992316428341\n",
      "Episode finished with -600.0 reward\n",
      "Reward -1.0 at step 730, points reached 5, l2 distance 17.14381610952605\n",
      "Episode finished with -601.0 reward\n",
      "Reward -1.0 at step 731, points reached 5, l2 distance 17.074363595709716\n",
      "Episode finished with -602.0 reward\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reward -1.0 at step 732, points reached 5, l2 distance 17.093267216307275\n",
      "Episode finished with -603.0 reward\n",
      "Reward -1.0 at step 733, points reached 5, l2 distance 17.024279311411416\n",
      "Episode finished with -604.0 reward\n",
      "Reward -1.0 at step 734, points reached 5, l2 distance 16.95560049121872\n",
      "Episode finished with -605.0 reward\n",
      "Reward -1.0 at step 735, points reached 5, l2 distance 16.87948044479576\n",
      "Episode finished with -606.0 reward\n",
      "Reward -1.0 at step 736, points reached 5, l2 distance 16.803610688023824\n",
      "Episode finished with -607.0 reward\n",
      "Reward -1.0 at step 737, points reached 5, l2 distance 16.734936916164806\n",
      "Episode finished with -608.0 reward\n",
      "Reward -1.0 at step 738, points reached 5, l2 distance 16.65916640773728\n",
      "Episode finished with -609.0 reward\n",
      "Reward -1.0 at step 739, points reached 5, l2 distance 16.59065539929843\n",
      "Episode finished with -610.0 reward\n",
      "Reward -1.0 at step 740, points reached 5, l2 distance 16.514987076445973\n",
      "Episode finished with -611.0 reward\n",
      "Reward -1.0 at step 741, points reached 5, l2 distance 16.446642789852465\n",
      "Episode finished with -612.0 reward\n",
      "Reward -1.0 at step 742, points reached 5, l2 distance 16.37107969433153\n",
      "Episode finished with -613.0 reward\n",
      "Reward -1.0 at step 743, points reached 5, l2 distance 16.302906214053476\n",
      "Episode finished with -614.0 reward\n",
      "Reward -1.0 at step 744, points reached 5, l2 distance 16.324462175732112\n",
      "Episode finished with -615.0 reward\n",
      "Reward -1.0 at step 745, points reached 5, l2 distance 16.249232972499982\n",
      "Episode finished with -616.0 reward\n",
      "Reward -1.0 at step 746, points reached 5, l2 distance 16.181413817973507\n",
      "Episode finished with -617.0 reward\n",
      "Reward -1.0 at step 747, points reached 5, l2 distance 16.106300002186\n",
      "Episode finished with -618.0 reward\n",
      "Reward -1.0 at step 748, points reached 5, l2 distance 16.0386633002436\n",
      "Episode finished with -619.0 reward\n",
      "Reward -1.0 at step 749, points reached 5, l2 distance 15.971366289605442\n",
      "Episode finished with -620.0 reward\n",
      "Reward -1.0 at step 750, points reached 5, l2 distance 15.933162630353454\n",
      "Episode finished with -621.0 reward\n",
      "Reward -1.0 at step 751, points reached 5, l2 distance 15.866066127832838\n",
      "Episode finished with -622.0 reward\n",
      "Reward -1.0 at step 752, points reached 5, l2 distance 15.889810915330335\n",
      "Episode finished with -623.0 reward\n",
      "Reward -1.0 at step 753, points reached 5, l2 distance 15.823252334334974\n",
      "Episode finished with -624.0 reward\n",
      "Reward -1.0 at step 754, points reached 5, l2 distance 15.78571435360726\n",
      "Episode finished with -625.0 reward\n",
      "Reward -1.0 at step 755, points reached 5, l2 distance 15.719369053307354\n",
      "Episode finished with -626.0 reward\n",
      "Reward -1.0 at step 756, points reached 5, l2 distance 15.682238007409227\n",
      "Episode finished with -627.0 reward\n",
      "Reward -1.0 at step 757, points reached 5, l2 distance 15.616111454863972\n",
      "Episode finished with -628.0 reward\n",
      "Reward -1.0 at step 758, points reached 5, l2 distance 15.541764524640989\n",
      "Episode finished with -629.0 reward\n",
      "Reward -1.0 at step 759, points reached 5, l2 distance 15.475853202112416\n",
      "Episode finished with -630.0 reward\n",
      "Reward -1.0 at step 760, points reached 5, l2 distance 15.40164860156955\n",
      "Episode finished with -631.0 reward\n",
      "Reward -1.0 at step 761, points reached 5, l2 distance 15.335957980654939\n",
      "Episode finished with -632.0 reward\n",
      "Reward -1.0 at step 762, points reached 5, l2 distance 15.261900086213076\n",
      "Episode finished with -633.0 reward\n",
      "Reward -1.0 at step 763, points reached 5, l2 distance 15.18813949431225\n",
      "Episode finished with -634.0 reward\n",
      "Reward -1.0 at step 764, points reached 5, l2 distance 15.122529164308252\n",
      "Episode finished with -635.0 reward\n",
      "Reward -1.0 at step 765, points reached 5, l2 distance 15.149940207450213\n",
      "Episode finished with -636.0 reward\n",
      "Reward -1.0 at step 766, points reached 5, l2 distance 15.084920810517902\n",
      "Episode finished with -637.0 reward\n",
      "Reward -1.0 at step 767, points reached 5, l2 distance 15.011441577260248\n",
      "Episode finished with -638.0 reward\n",
      "Reward -1.0 at step 768, points reached 5, l2 distance 14.938270334794428\n",
      "Episode finished with -639.0 reward\n",
      "Reward -1.0 at step 769, points reached 5, l2 distance 14.873350249831413\n",
      "Episode finished with -640.0 reward\n",
      "Reward -1.0 at step 770, points reached 5, l2 distance 14.838856038302831\n",
      "Episode finished with -641.0 reward\n",
      "Reward -1.0 at step 771, points reached 5, l2 distance 14.774194998494702\n",
      "Episode finished with -642.0 reward\n",
      "Reward -1.0 at step 772, points reached 5, l2 distance 14.740166247168673\n",
      "Episode finished with -643.0 reward\n",
      "Reward -1.0 at step 773, points reached 5, l2 distance 14.770287441160479\n",
      "Episode finished with -644.0 reward\n",
      "Reward -1.0 at step 774, points reached 5, l2 distance 14.706120604547428\n",
      "Episode finished with -645.0 reward\n",
      "Reward -1.0 at step 775, points reached 5, l2 distance 14.737086342945819\n",
      "Episode finished with -646.0 reward\n",
      "Reward -1.0 at step 776, points reached 5, l2 distance 14.768664276886584\n",
      "Episode finished with -647.0 reward\n",
      "Reward -1.0 at step 777, points reached 5, l2 distance 14.697123234504192\n",
      "Episode finished with -648.0 reward\n",
      "Reward -1.0 at step 778, points reached 5, l2 distance 14.633692095389392\n",
      "Episode finished with -649.0 reward\n",
      "Reward -1.0 at step 779, points reached 5, l2 distance 14.562354580689199\n",
      "Episode finished with -650.0 reward\n",
      "Reward -1.0 at step 780, points reached 5, l2 distance 14.595372340774508\n",
      "Episode finished with -651.0 reward\n",
      "Reward -1.0 at step 781, points reached 5, l2 distance 14.628999165970697\n",
      "Episode finished with -652.0 reward\n",
      "Reward -1.0 at step 782, points reached 5, l2 distance 14.663230866016438\n",
      "Episode finished with -653.0 reward\n",
      "Reward -1.0 at step 783, points reached 5, l2 distance 14.698063214662774\n",
      "Episode finished with -654.0 reward\n",
      "Reward -1.0 at step 784, points reached 5, l2 distance 14.73349195182014\n",
      "Episode finished with -655.0 reward\n",
      "Reward -1.0 at step 785, points reached 5, l2 distance 14.769512785683713\n",
      "Episode finished with -656.0 reward\n",
      "Reward -1.0 at step 786, points reached 5, l2 distance 14.707831380079627\n",
      "Episode finished with -657.0 reward\n",
      "Reward -1.0 at step 787, points reached 5, l2 distance 14.679033602627927\n",
      "Episode finished with -658.0 reward\n",
      "Reward -1.0 at step 788, points reached 5, l2 distance 14.617673746351322\n",
      "Episode finished with -659.0 reward\n",
      "Reward -1.0 at step 789, points reached 5, l2 distance 14.589402716571438\n",
      "Episode finished with -660.0 reward\n",
      "Reward -1.0 at step 790, points reached 5, l2 distance 14.528371971535542\n",
      "Episode finished with -661.0 reward\n",
      "Reward -1.0 at step 791, points reached 5, l2 distance 14.566648669725925\n",
      "Episode finished with -662.0 reward\n",
      "Reward -1.0 at step 792, points reached 5, l2 distance 14.539371033113888\n",
      "Episode finished with -663.0 reward\n",
      "Reward -1.0 at step 793, points reached 5, l2 distance 14.4789375773387\n",
      "Episode finished with -664.0 reward\n",
      "Reward -1.0 at step 794, points reached 5, l2 distance 14.452205790435672\n",
      "Episode finished with -665.0 reward\n",
      "Reward -1.0 at step 795, points reached 5, l2 distance 14.426117670729504\n",
      "Episode finished with -666.0 reward\n",
      "Reward -1.0 at step 796, points reached 5, l2 distance 14.400676716403494\n",
      "Episode finished with -667.0 reward\n",
      "Reward -1.0 at step 797, points reached 5, l2 distance 14.375886363316782\n",
      "Episode finished with -668.0 reward\n",
      "Reward -1.0 at step 798, points reached 5, l2 distance 14.417303934888295\n",
      "Episode finished with -669.0 reward\n",
      "Reward -1.0 at step 799, points reached 5, l2 distance 14.357230432841588\n",
      "Episode finished with -670.0 reward\n",
      "Reward -1.0 at step 800, points reached 5, l2 distance 14.333473244197796\n",
      "Episode finished with -671.0 reward\n",
      "Reward -1.0 at step 801, points reached 5, l2 distance 14.310375424235227\n",
      "Episode finished with -672.0 reward\n",
      "Reward -1.0 at step 802, points reached 5, l2 distance 14.287940170751593\n",
      "Episode finished with -673.0 reward\n",
      "Reward -1.0 at step 803, points reached 5, l2 distance 14.326986214364085\n",
      "Episode finished with -674.0 reward\n",
      "Reward -1.0 at step 804, points reached 5, l2 distance 14.305783517301688\n",
      "Episode finished with -675.0 reward\n",
      "Reward -1.0 at step 805, points reached 5, l2 distance 14.28524938883542\n",
      "Episode finished with -676.0 reward\n",
      "Reward -1.0 at step 806, points reached 5, l2 distance 14.32601433751291\n",
      "Episode finished with -677.0 reward\n",
      "Reward -1.0 at step 807, points reached 5, l2 distance 14.306715673866178\n",
      "Episode finished with -678.0 reward\n",
      "Reward -1.0 at step 808, points reached 5, l2 distance 14.352817753556634\n",
      "Episode finished with -679.0 reward\n",
      "Reward -1.0 at step 809, points reached 5, l2 distance 14.293379186312396\n",
      "Episode finished with -680.0 reward\n",
      "Reward -1.0 at step 810, points reached 5, l2 distance 14.275149240471038\n",
      "Episode finished with -681.0 reward\n",
      "Reward -1.0 at step 811, points reached 5, l2 distance 14.257597382158513\n",
      "Episode finished with -682.0 reward\n",
      "Reward -1.0 at step 812, points reached 5, l2 distance 14.240726118627022\n",
      "Episode finished with -683.0 reward\n",
      "Reward -1.0 at step 813, points reached 5, l2 distance 14.224537871571021\n",
      "Episode finished with -684.0 reward\n",
      "Reward -1.0 at step 814, points reached 5, l2 distance 14.209034975457367\n",
      "Episode finished with -685.0 reward\n",
      "Reward -1.0 at step 815, points reached 5, l2 distance 14.194219675902694\n",
      "Episode finished with -686.0 reward\n",
      "Reward -1.0 at step 816, points reached 5, l2 distance 14.18009412810069\n",
      "Episode finished with -687.0 reward\n",
      "Defi reached the terminal state!\n",
      "Reward 1.0 at step 817, points reached 5, l2 distance 14.18009412810069\n",
      "Episode finished with -686.0 reward\n"
     ]
    }
   ],
   "source": [
    "#### test performance of pretrained agent\n",
    "\n",
    "state = env.reset(streamline_index=0)\n",
    "terminal = False\n",
    "all_states = []\n",
    "all_states.append(state.getCoordinate())\n",
    "\n",
    "\n",
    "states = Object()\n",
    "states.x = [state.getCoordinate()[0]]\n",
    "states.y = [state.getCoordinate()[1]]\n",
    "states.z = [state.getCoordinate()[2]]\n",
    "\n",
    "while not terminal:\n",
    "    action = torch.argmax(agent.main_dqn(torch.FloatTensor(state.getValue()).unsqueeze(0).to(device)))\n",
    "    next_state, reward, terminal, _ = env.step(action)\n",
    "    #if reward < 0.:\n",
    "    #\n",
    "    #if reward == 0. and env.l2_distance < 0.5:\n",
    "    #    reward = 1.\n",
    "    #    env.points_visited += 1\n",
    "    print(\"Reward {} at step {}, points reached {}, l2 distance {}\".format(reward, env.stepCounter, env.points_visited, env.l2_distance))\n",
    "    \n",
    "    episode_reward += reward\n",
    "    all_states.append(next_state.getCoordinate())\n",
    "    state = next_state\n",
    "    \n",
    "    \n",
    "    states.x.append(state.getCoordinate()[0])\n",
    "    states.y.append(state.getCoordinate()[1])\n",
    "    states.z.append(state.getCoordinate()[2])\n",
    "    \n",
    "    print(\"Episode finished with {} reward\".format(episode_reward))\n",
    "    \n",
    "    #print(env.stepCounter, action, reward)#cosine_sim.item(), dist.item(), 1-(optimal_reward-(cosine_sim-dist)))\n",
    "    #if action == 100 and 1-(optimal_reward-(cosine_sim-dist)) == 1:\n",
    "    #    terminal = True\n",
    "    #else:\n",
    "    #    terminal = False\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validation of our agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#for i in range(len(states)):\n",
    "#    print(states[i], env.referenceStreamline_ijk[i])\n",
    "#    distance = ((states.T[0][i] - env.referenceStreamline_ijk.T[0][i])**2 \\\n",
    "#                      + (states.T[1][i] - env.referenceStreamline_ijk.T[1][i] )**2 \\\n",
    "#                      + (states.T[2][i] - env.referenceStreamline_ijk.T[2][i])**2)\n",
    "#    print(distance)\n",
    "no_timesteps = 5\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = plt.axes(projection='3d')\n",
    "ax.plot3D(env.referenceStreamline_ijk.T[0][0:no_timesteps], env.referenceStreamline_ijk.T[1][0:no_timesteps], env.referenceStreamline_ijk.T[2][0:no_timesteps])\n",
    "ax.plot3D(states.x, states.y, states.z)\n",
    "#plt.legend('gt','agent')\n",
    "#print(optimal_steps[:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(all_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = env.reset().getValue()\n",
    "agent = Agent(n_actions=n_actions, inp_size=state.shape, device=device, hidden=10, gamma=0.99, agent_history_length=agent_history_length, memory_size=replay_memory_size, batch_size=512, learning_rate=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.main_dqn.load_state_dict(torch.load(\"defi_pretrained_95pacc.pth\"))\n",
    "agent.target_dqn.load_state_dict(agent.main_dqn.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(state.shape)\n",
    "print(agent.main_dqn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "#### DQN training\n",
    "\n",
    "#uncomment next 2 lines for regular DQN trainig without pretrained network\n",
    "#state = env.reset().getValue()\n",
    "#agent = Agent(n_actions=n_actions, inp_size=state.shape, device=device, hidden=10, gamma=0.95, agent_history_length=agent_history_length, memory_size=replay_memory_size, batch_size=batch_size, learning_rate=learning_rate)\n",
    "\n",
    "#transition = init_transition()\n",
    "#agent = Agent(n_actions=n_actions, inp_size=np.array(transition).shape, device=device, hidden=10, agent_history_length=agent_history_length, memory_size=replay_memory_size, batch_size=batch_size, learning_rate=learning_rate)\n",
    "action_scheduler = Action_Scheduler(num_actions=n_actions, max_steps=max_steps, eps_initial=0.5, eps_annealing_steps=eps_annealing_steps, eps_final=0.1, eps_final_step=0.02, replay_memory_start_size=start_learning, model=agent.main_dqn)\n",
    "\n",
    "step_counter = 0\n",
    "\n",
    "eps_rewards = []\n",
    "\n",
    "episode_lengths = []\n",
    "\n",
    "cos = torch.nn.CosineSimilarity(dim=0)\n",
    "\n",
    "print(\"Start training...\")\n",
    "while step_counter < max_steps:\n",
    "    epoch_step = 0\n",
    "    #agent.main_dqn.train()\n",
    "######## fill memory begins here\n",
    "    while (epoch_step < evaluate_every) or (step_counter < start_learning):\n",
    "        state = env.reset()\n",
    "        env.stepCounter = np.random.randint(len(env.referenceStreamline_ijk)-10)\n",
    "        env.state = TractographyState(env.referenceStreamline_ijk[env.stepCounter], env.interpolateDWIatState)\n",
    "        #transition = init_transition()\n",
    "        #referenceLine = env.referenceStreamline_ijk\n",
    "        episode_reward_sum = 0\n",
    "        terminal = False\n",
    "        #fill replay memory while interacting with env\n",
    "        #for episode_counter in range(max_episode_length):\n",
    "        episode_step_counter = 0\n",
    "        positive_run = 0\n",
    "        points_visited = 0\n",
    "        \n",
    "        dist = 0\n",
    "        #influential_action = None\n",
    "        while not terminal:\n",
    "            # get action with epsilon-greedy strategy\n",
    "            #if dist < 0.1:\n",
    "            #_, optimal_reward = get_best_action(state, env)\n",
    "               #print(influential_action)\n",
    "            #else:\n",
    "            #    influential_action = None\n",
    "            action = action_scheduler.get_action(step_counter, torch.FloatTensor(state.getValue()).unsqueeze(0).to(device)) #influential_action=influential_action)\n",
    "            #action = action_scheduler.get_action(step_counter, torch.FloatTensor([np.array(transition)]).to(device))\n",
    "            #print(\"Before step: \", env.stepCounter)\n",
    "            \n",
    "            next_state, reward, terminal = env.step(action)\n",
    "            episode_step_counter += 1\n",
    "            #print(episode_step_counter, action, reward, optimal_reward, torch.tanh(1-(optimal_reward - reward)))\n",
    "            #print(\"After step: \", env.stepCounter)\n",
    "            \n",
    "            #if reward < -1.:\n",
    "            #    reward = -1.\n",
    "            \n",
    "            #terminal = False\n",
    "            \n",
    "            #current_index = np.min([env.stepCounter,len(env.referenceStreamline_ijk)-1])\n",
    "            #path_vector = (next_state.getCoordinate() - state.getCoordinate()).squeeze(0)\n",
    "            #reference_vector = env.referenceStreamline_ijk[current_index]-env.referenceStreamline_ijk[current_index-1]\n",
    "            ##    #print(path_vector, reference_vector)\n",
    "            #cosine_sim = cos(path_vector, reference_vector)\n",
    "            #dist = torch.sum((env.referenceStreamline_ijk[current_index] - next_state.getCoordinate())**2)\n",
    "            #reward = -torch.dist(env.referenceStreamline_ijk[current_index], next_state.getCoordinate(), p=2)\n",
    "            #if reward == 0.:\n",
    "            #    reward = 1.\n",
    "            \n",
    "            #if reward < -0.05:\n",
    "            #    env.stepCounter -= 1\n",
    "            \n",
    "            #reward = torch.tanh(1- (optimal_reward - reward))\n",
    "            #if reward >= 0.76:\n",
    "            #    reward = 1.\n",
    "            #elif reward < 0.:\n",
    "            #    reward = -1.\n",
    "            #else:\n",
    "            #    reward = 0.\n",
    "            current_index = np.min([env.stepCounter,len(env.referenceStreamline_ijk)-1])\n",
    "            dist = torch.dist(env.referenceStreamline_ijk[current_index], next_state.getCoordinate(), p=2)\n",
    "            dist_past = torch.dist(env.referenceStreamline_ijk[current_index], state.getCoordinate(), p=2)\n",
    "            #if dist <= 0.09:\n",
    "            #    reward = 1.\n",
    "            #elif dist < 0.25:\n",
    "            #    reward = 0.5\n",
    "            #elif dist < 1.:\n",
    "            #    reward = 0.25\n",
    "            #elif dist_past < dist:\n",
    "            #    reward = -1\n",
    "            #    #env.stepCounter -= 1\n",
    "            #else:\n",
    "            #    reward = 0.\n",
    "                #env.stepCounter -= 1\n",
    "            \n",
    "            #if action == 19:\n",
    "            #    if dist <= 0.09:\n",
    "            #        reward = 1.\n",
    "            #    else:\n",
    "            #        reward = -1.\n",
    "                    \n",
    "            #if reward > 0.:\n",
    "            #    positive_run += 1\n",
    "            #print(\"Before test for dist: \", env.stepCounter, \"Dist: \", dist)\n",
    "            #print(\"After test for dist: \", env.stepCounter)\n",
    "            #if dist < dist_past:\n",
    "            reward = 0.\n",
    "                #positive_run += 1 \n",
    "            if dist > 2.:\n",
    "                reward = -1.\n",
    "                \n",
    "            if dist > 0.25:\n",
    "                env.stepCounter -= 1\n",
    "                #if dist < dist_past:\n",
    "                    #reward = 0.5\n",
    "                    #positive_run += 1\n",
    "            else:\n",
    "                points_visited += 1\n",
    "                reward = 1.\n",
    "                positive_run += 1\n",
    "                print(\"Reached referencePoint {} at step {}\".format(points_visited, episode_step_counter))\n",
    "                #env.stepCounter -= 1\n",
    "            #if reward < -5.0:\n",
    "            #    reward = -5.0\n",
    "            #if reward < -100:\n",
    "            #    reward = -100\n",
    "            #if dist < 0.1:\n",
    "            #    dist = 0\n",
    "            #else:\n",
    "            #    dist = dist - 0.1\n",
    "            #if dist > 3*0.81:\n",
    "            #    env.stepCounter -= 1\n",
    "            #reward = cosine_sim - dist\n",
    "            #reward = 1 - (optimal_reward - reward)\n",
    "            #reward = 1- (optimal_reward - dist)\n",
    "            #if reward == optimal_reward:\n",
    "            #    reward = 1\n",
    "            #if action == 100 and dist < 0.1:\n",
    "            #    terminal = True\n",
    "            #print(\"From function: \", influential_action, optimal_reward)\n",
    "            #print(\"From scheduler: \", action, reward,  terminal)\n",
    "            #print(\"Cosine sim: \", cosine_sim)\n",
    "            #print(\"Dist: \", dist)\n",
    "            \n",
    "            #if episode_step_counter >= 200:\n",
    "            #    terminal = True\n",
    "            \n",
    "            #print(episode_step_counter, action, reward, terminal)\n",
    "            #print(reward)\n",
    "            #if dist > 0.7: # cosine_sim < 0.4 or\n",
    "            #    terminal = True\n",
    "            #next_state = next_state[:2]\n",
    "            #next_transition = add_to_transition(next_state, transition)\n",
    "            \n",
    "            step_counter += 1\n",
    "            epoch_step += 1\n",
    "\n",
    "            # accumulate reward for current episode\n",
    "            episode_reward_sum += reward\n",
    "\n",
    "\n",
    "            agent.replay_memory.add_experience(action=action,\n",
    "                                #state=np.array(transition),\n",
    "                                state = state.getValue(),\n",
    "                                reward=reward,\n",
    "                                #new_state=np.array(next_transition),\n",
    "                                new_state = next_state.getValue(),\n",
    "                                terminal=terminal)\n",
    "\n",
    "\n",
    "            state = next_state\n",
    "            #transition = next_transition\n",
    "\n",
    "\n",
    "\n",
    "            ####### optimization is happening here\n",
    "            if step_counter > start_learning and step_counter % 4 == 0:\n",
    "                #if reward > 0.:\n",
    "                #    print(\"reward was positive: \", reward)\n",
    "                loss = agent.optimize()\n",
    "\n",
    "\n",
    "            ####### target network update\n",
    "            if step_counter > start_learning and step_counter % network_update_every == 0:\n",
    "                #print(\"Update net\")\n",
    "                #print(agent.main_dqn(torch.tensor(state).to(device).unsqueeze(0)))\n",
    "                #print(agent.target_dqn(torch.tensor(state).to(device).unsqueeze(0)))\n",
    "                agent.target_dqn.load_state_dict(agent.main_dqn.state_dict())\n",
    "\n",
    "            # if episode ended before maximum step\n",
    "            if episode_step_counter >= 1000:\n",
    "                terminal = True\n",
    "            if terminal:\n",
    "                terminal = False\n",
    "                episode_lengths.append(episode_step_counter)\n",
    "                #state = env.reset()[:2]\n",
    "                #transition = init_transition()\n",
    "                break\n",
    "\n",
    "        eps_rewards.append(episode_reward_sum)\n",
    "\n",
    "        if len(eps_rewards) % 1 == 0:\n",
    "            #with open(path+'/logs/rewards.dat', 'a') as reward_file:\n",
    "                #print(\"[{}] {}, {}\".format(len(eps_rewards), step_counter, np.mean(eps_rewards[-100:])), file=reward_file)\n",
    "            print(\"{}, done {} episodes, {}, current eps {}\".format(step_counter, len(eps_rewards), np.mean(eps_rewards[-100:]), action_scheduler.eps_current), np.mean(episode_lengths[-100:]), positive_run, points_visited)\n",
    "    #torch.save(agent.main_dqn.state_dict(), path+'/checkpoints/fibre_agent_{}_reward_{:.2f}.pth'.format(step_counter, np.mean(eps_rewards[-100:])))\n",
    "\n",
    "########## evaluation starting here\n",
    "    eval_rewards = []\n",
    "    episode_final = 0\n",
    "    #agent.main_dqn.eval()\n",
    "    for _ in range(eval_runs):\n",
    "        eval_steps = 0\n",
    "        state = env.reset()\n",
    "        #transition = init_transition()\n",
    "        #env.state = TractographyState(env.referenceStreamline_ijk[0], env.interpolateDWIatState)\n",
    "        #env.stepCounter = 0\n",
    "        \n",
    "        eval_episode_reward = 0\n",
    "        while eval_steps < 1000:\n",
    "            #_, optimal_reward = get_best_action(state, env)\n",
    "            action = action_scheduler.get_action(step_counter, torch.FloatTensor(state.getValue()).unsqueeze(0).to(device), evaluation=True)\n",
    "            #action = action_scheduler.get_action(step_counter, torch.FloatTensor([np.array(transition)]).to(device), evaluation=True)\n",
    "            next_state, reward, terminal = env.step(action)\n",
    "            \n",
    "            eval_steps += 1\n",
    "            \n",
    "            #if reward < -0.05:\n",
    "            #    env.stepCounter -= 1\n",
    "            #reward = 1 - (optimal_reward-reward)\n",
    "            #if reward >= 0.76:\n",
    "            #    reward = 1\n",
    "            #elif reward < 0.:\n",
    "            #    reward = -1.\n",
    "            #else:\n",
    "            #    reward = 0.\n",
    "            current_index = np.min([env.stepCounter,len(env.referenceStreamline_ijk)-1])\n",
    "            dist = torch.dist(env.referenceStreamline_ijk[current_index], next_state.getCoordinate(), p=2)\n",
    "            dist_past = torch.dist(env.referenceStreamline_ijk[current_index], state.getCoordinate(), p=2)\n",
    "            \n",
    "            reward = 0.\n",
    "                #positive_run += 1 \n",
    "            if dist > 2.:\n",
    "                reward = -1.\n",
    "                \n",
    "            if dist > 0.25:\n",
    "                env.stepCounter -= 1\n",
    "                #if dist < dist_past:\n",
    "                #    reward = 0.5\n",
    "            else:\n",
    "                points_visited += 1\n",
    "                reward = 1.\n",
    "                positive_run += 1\n",
    "        \n",
    "            #if dist < dist_past:\n",
    "            #    reward = 1.\n",
    "            #else:\n",
    "            #    reward = -1.\n",
    "            #    \n",
    "            #if dist > 0.1:\n",
    "            #    env.stepCounter -= 1\n",
    "            #else:\n",
    "            #    reward += 4    \n",
    "            \n",
    "            #if dist <= 0.09:\n",
    "            #    reward = 1.\n",
    "            #elif dist < 0.25:\n",
    "            #    reward = 0.5\n",
    "            #elif dist < 1.:\n",
    "            #    reward = 0.25\n",
    "            #elif dist_past < dist:\n",
    "            #    reward = -1\n",
    "                #env.stepCounter -= 1\n",
    "            #else:\n",
    "            #    reward = 0.\n",
    "                #env.stepCounter -= 1\n",
    "            \n",
    "            #if action == 19:\n",
    "            #    if dist <= 0.09:\n",
    "            #        reward = 1.\n",
    "            #    else:\n",
    "            #        reward = -1.\n",
    "                    \n",
    "            #if reward != 1.:\n",
    "            #    env.stepCounter -= 1\n",
    "    \n",
    "\n",
    "            \n",
    "            #if reward < -5.0:\n",
    "            #    reward = -5.0\n",
    "            #if reward < -100:\n",
    "            #    reward = -100\n",
    "\n",
    "            #if reward < -1.:\n",
    "            #    reward = -1.\n",
    "            #terminal = False\n",
    "            #current_index = np.min([env.stepCounter,len(env.referenceStreamline_ijk)-1])\n",
    "            #path_vector = (next_state.getCoordinate() - state.getCoordinate()).squeeze(0)\n",
    "            #reference_vector = env.referenceStreamline_ijk[current_index]-env.referenceStreamline_ijk[current_index-1]\n",
    "            #    #print(path_vector, reference_vector)\n",
    "            #cosine_sim = cos(path_vector, reference_vector)\n",
    "            #dist = torch.sum((env.referenceStreamline_ijk[current_index] - next_state.getCoordinate())**2)\n",
    "            #if dist < 0.1:\n",
    "            #    dist = 0\n",
    "            #else:\n",
    "            #    dist = dist - 0.1\n",
    "            #if dist > 3*0.81:\n",
    "            #    env.stepCounter -= 1\n",
    "            #reward = cosine_sim - dist\n",
    "            #reward = 1- (optimal_reward - reward)\n",
    "            #if reward == optimal_reward:\n",
    "            #    reward = 1\n",
    "            #if action == 100 and env.rewardForTerminalState(next_state) < 0.1:\n",
    "            #    terminal = True\n",
    "\n",
    "            #if episode_step_counter == 200:\n",
    "            #    terminal = True\n",
    "            \n",
    "            #if cosine_sim < 0.9:\n",
    "            #    terminal = True\n",
    "            \n",
    "            eval_episode_reward += reward\n",
    "            state = next_state\n",
    "            #transition = next_transition\n",
    "            if terminal:\n",
    "                terminal = False\n",
    "                if reward == 1.:\n",
    "                    print(reward)\n",
    "                    episode_final += 1\n",
    "                break\n",
    "\n",
    "        eval_rewards.append(eval_episode_reward)\n",
    "\n",
    "    print(\"Evaluation score:\", np.mean(eval_rewards))\n",
    "    print(\"{} of {} episodes ended close to / at the final state.\".format(episode_final, eval_runs))\n",
    "    #if np.mean(eval_rewards) > 500.:\n",
    "    #torch.save(agent.main_dqn.state_dict(), 'checkpoints/defi_{}_reward_{:.2f}.pth'.format(step_counter, np.mean(eval_rewards)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(agent.main_dqn.state_dict(), 'defi_pretrained_95pacc.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(path_vector.shape)\n",
    "print(reference_vector.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = env.reset()\n",
    "\n",
    "q_vals = agent.main_dqn(torch.FloatTensor(state.getValue()).unsqueeze(0).to(device))\n",
    "print(q_vals[0][80])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "defi = DQN(n_actions=n_actions, input_shape=513).to(device)\n",
    "action_scheduler = Action_Scheduler(num_actions=n_actions, max_steps=max_steps, eps_annealing_steps=eps_annealing_steps, eps_final=0.1, eps_final_step=0.02, replay_memory_start_size=start_learning, model=defi)\n",
    "\n",
    "defi.load_state_dict(torch.load('high_gamma/checkpoints/defi_487575_reward_1.00.pth'))\n",
    "defi.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.main_dqn(torch.FloatTensor(state.getValue()).unsqueeze(0).to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [],
   "source": [
    "eval_rewards = []\n",
    "all_distances = []\n",
    "all_states = []\n",
    "#agent.main_dqn.eval()\n",
    "for _ in range(1):\n",
    "    eval_steps = 0\n",
    "    state = env.reset(streamline_index=2)    \n",
    "    #state = env.reset()\n",
    "    #print(state.getCoordinate())\n",
    "    all_states.append(state.getCoordinate())\n",
    "    #transition = init_transition()\n",
    "    #all_states.append(torch.tensor(list(transition)[:3]))\n",
    "    eval_episode_reward = 0\n",
    "    episode_final = 0\n",
    "    #print(env.referenceStreamline_ijk[:6])\n",
    "    \n",
    "    while eval_steps < max_episode_length:\n",
    "        action = torch.argmax(agent.main_dqn(torch.FloatTensor(state.getValue()).unsqueeze(0).to(device)))\n",
    "        #action = action_scheduler.get_action(eval_steps, torch.FloatTensor(state.getValue()).unsqueeze(0).to(device), evaluation=True)\n",
    "        #action = action_scheduler.get_action(step_counter, torch.FloatTensor([np.array(transition)]).to(device), evaluation=True)\n",
    "        #action = torch.argmax(agent(torch.FloatTensor([np.array(transition)]).to(device)))\n",
    "        next_state, reward, terminal = env.step(action)\n",
    "        \n",
    "        \n",
    "        #_, optimal_reward = get_best_action(state, env)\n",
    "        #if reward < -0.05:\n",
    "        #        env.stepCounter -= 1\n",
    "        #    \n",
    "        #reward = torch.tanh(1- (optimal_reward - reward))\n",
    "        #if reward >= 0.76:\n",
    "        #    reward = 1.\n",
    "        #elif reward < 0.1:\n",
    "        #    reward = -1.\n",
    "        #else:\n",
    "        #    reward = 0.\n",
    "        current_index = np.min([env.stepCounter,len(env.referenceStreamline_ijk)-1])\n",
    "        dist = torch.dist(env.referenceStreamline_ijk[current_index], next_state.getCoordinate(), p=2)\n",
    "        dist_past = torch.dist(env.referenceStreamline_ijk[current_index], state.getCoordinate(), p=2)\n",
    "\n",
    "\n",
    "        reward = 0.\n",
    "        #positive_run += 1 \n",
    "        if dist > 2.:\n",
    "            reward = -1.\n",
    "\n",
    "        if dist > 0.25:\n",
    "            env.stepCounter -= 1\n",
    "            #if dist < dist_past:\n",
    "            #    reward = 0.5\n",
    "        else:\n",
    "            reward = 1.\n",
    "\n",
    "        #current_index = np.min([env.stepCounter,len(env.referenceStreamline_ijk)-1])\n",
    "        #path_vector = (next_state.getCoordinate() - state.getCoordinate()).squeeze(0)\n",
    "        #reference_vector = env.referenceStreamline_ijk[current_index]-env.referenceStreamline_ijk[current_index-1]\n",
    "        #    #print(path_vector, reference_vector)\n",
    "        #cosine_sim = cos(path_vector, reference_vector)\n",
    "        #dist = torch.sum((env.referenceStreamline_ijk[current_index] - next_state.getCoordinate())**2) * 10\n",
    "        #reward = cosine_sim - dist\n",
    "        #reward = 1 - (optimal_reward - reward)\n",
    "        #if dist > 3*0.81:\n",
    "        #    env.stepCounter -= 1\n",
    "        #if action == 100 and reward == 1:\n",
    "        #    terminal = False\n",
    "            \n",
    "        #if cosine_sim < 0.7:\n",
    "        #    terminal = True\n",
    "        #next_state = next_state\n",
    "        #next_transition = add_to_transition(next_state, transition)\n",
    "        #reward = 1 + (1+(reward/10))\n",
    "        #if reward > 1:\n",
    "        #    reward = 1\n",
    "        #elif reward > 0.:\n",
    "        #    reward = 0\n",
    "        #else:\n",
    "        #    reward = -1\n",
    "        eval_episode_reward += reward\n",
    "        print(eval_steps, action, next_state.getCoordinate().numpy(), env.referenceStreamline_ijk[np.min([env.stepCounter,len(env.referenceStreamline_ijk)-1])].numpy(), reward)\n",
    "        #print(eval_steps, action, next_state, env.referenceStreamline_ijk[np.min([eval_steps,len(env.referenceStreamline_ijk)-1])].numpy(), reward)\n",
    "        eval_steps += 1\n",
    "        if eval_steps == 1000:\n",
    "            terminal = True\n",
    "        all_distances.append(reward)\n",
    "        all_states.append(next_state.getCoordinate())\n",
    "        #all_states.append(next_state)\n",
    "        \n",
    "        state = next_state\n",
    "        #transition = next_transition\n",
    "        if terminal:\n",
    "            terminal = False\n",
    "            #if reward > 0.9:\n",
    "            #    episode_final += 1\n",
    "            break\n",
    "\n",
    "    eval_rewards.append(eval_episode_reward)\n",
    "\n",
    "print(\"Evaluation score:\", np.min(eval_rewards))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "states, actions, rewards, new_states, terminal_flags = agent.replay_memory.get_minibatch()\n",
    "print(np.array_equal(states[0], new_states[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sphere_dist(nextState):\n",
    "    current_index = np.min([env.stepCounter,len(env.referenceStreamline_ijk)-1])\n",
    "    x_dist = (nextState.getCoordinate()[0] - env.referenceStreamline_ijk[current_index][0]) **2\n",
    "    y_dist = (nextState.getCoordinate()[1] - env.referenceStreamline_ijk[current_index][1]) **2\n",
    "    z_dist = (nextState.getCoordinate()[2] - env.referenceStreamline_ijk[current_index][2]) **2\n",
    "    return x_dist + y_dist + z_dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(agent.gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(n_actions):\n",
    "    state = env.reset()\n",
    "    next_state, reward, done = env.step(i)\n",
    "    s_dist = sphere_dist(next_state)\n",
    "    old_dist = torch.sum((env.referenceStreamline_ijk[env.stepCounter] - next_state.getCoordinate())**2)\n",
    "    if s_dist <= 0.52**2:\n",
    "        print(i, reward, s_dist, old_dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "next_state, reward, done = env.step(75)\n",
    "print(next_state.getCoordinate(), reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(list(transition)[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = env.reset()\n",
    "#referenceLine = env.referenceStreamline_ijk\n",
    "print(state.getCoordinate())\n",
    "#print(referenceLine[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "next_state, reward, done = env.step(74)\n",
    "print(next_state.getCoordinate().numpy(), env.referenceStreamline_ijk[np.min([env.stepCounter, len(referenceLine)])].numpy(), reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimal_steps =  [80, 88, 54, 96, 100, 67, 83, 75, 83, 75, 100, 83, 70, 67, 59, 100, 67, 59, 59, 59, 51, 100, 59, 56, 51, 61, 100, 66, 71, 66, 71, 71, 100, 71, 71, 71, 71, 100, 92, 84, 84, 38, 100, 97, 97, 97, 38, 100, 97, 30, 43, 43, 48, 100, 94, 81, 94, 35, 97, 100, 35, 22, 35, 35, 6, 100, 19, 3, 16, 3, 21, 100, 21, 16, 34, 21, 98, 34, 100, 39, 93, 39, 72, 72, 100, 69, 100]\n",
    "transition = init_transition()\n",
    "referenceLine = env.referenceStreamline_ijk\n",
    "print(len(referenceLine))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#action = action_scheduler.get_action(step_counter, torch.FloatTensor([np.array(transition)]).to(device), evaluation=True)\n",
    "next_state, reward, terminal = env.step(88)\n",
    "next_transition = add_to_transition(next_state, transition)\n",
    "print(action, reward)\n",
    "transition = next_transition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Debugging the reward function\n",
    "referenceLine = env.referenceStreamline_ijk\n",
    "stepCounter = 0\n",
    "maxSteps=200\n",
    "state = env.reset()\n",
    "print(\"State: \", state.getCoordinate().numpy())\n",
    "next_state, _, terminal = env.step(80)\n",
    "print(\"Next State: \", next_state.getCoordinate().numpy())\n",
    "\n",
    "def lineseg_dist(p, a, b):\n",
    "\n",
    "    # normalized tangent vector\n",
    "    d = np.divide(b - a, np.linalg.norm(b - a))\n",
    "\n",
    "    # signed parallel distance components\n",
    "    s = np.dot(a - p, d)\n",
    "    t = np.dot(p - b, d)\n",
    "\n",
    "    # clamped parallel distance\n",
    "    h = np.maximum.reduce([s, t, 0])\n",
    "\n",
    "    # perpendicular distance component\n",
    "    c = np.cross(p - a, d)\n",
    "\n",
    "    return np.hypot(h, np.linalg.norm(c))\n",
    "\n",
    "distance = lineseg_dist(referenceLine[86].numpy(), referenceLine[85].numpy(), referenceLine[86].numpy())\n",
    "print(distance)\n",
    "\n",
    "#print(\"Diff: \", next_state.getCoordinate().numpy()-state.getCoordinate().numpy())\n",
    "#qry_pt = next_state.getCoordinate().view(-1,3)\n",
    "#print(\"Reference next state: \", referenceLine[stepCounter+1])\n",
    "#print(\"Diff to reference state: \", referenceLine[stepCounter+1]-next_state.getCoordinate().numpy())\n",
    "#distance = torch.min(torch.sum((referenceLine[np.min([stepCounter+1, maxSteps-1])] - qry_pt)**2, dim=1))\n",
    "#print(distance)\n",
    "#reward = torch.tanh(-distance+5.3)\n",
    "\n",
    "#if distance == -1:\n",
    "#    reward = 0.5\n",
    "#elif distance < 0.8:\n",
    "#    reward = 1+ (1-distance)\n",
    "#else:\n",
    "#    reward = np.max([1 - distance, -1])\n",
    "#print(reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = np.array([ 75.6, 107.95,  92.22])\n",
    "line = np.array([ 75.78847, 107.96255,  92.28433])\n",
    "\n",
    "print(np.linalg.norm(line - state, 2))\n",
    "\n",
    "sphere_dist = ((state[0] - line[0])**2 + (state[1]-line[1])**2 + (state[2]-line[2])**2)\n",
    "print(sphere_dist)\n",
    "normal_diff = np.sum(state-line)**2\n",
    "print(normal_diff)\n",
    "if sphere_dist < 0.2**2:\n",
    "    print(True)\n",
    "else:\n",
    "    print(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#optimal_steps = [80, 75, 80, 75, 62, 75, 83, 96, 51, 24, 62, 62, 62, 77, 65, 64, 67, 59, 56, 83, 82, 54, 56, 53, 56, 38, 56, 84, 66, 71, 71, 64, 97, 84, 71, 71, 38, 51, 30, 92, 97, 84, 43, 79, 27, 46, 89, 25, 81, 25, 48, 43, 86, 48, 57, 14, 89, 43, 43, 19, 92, 14, 27, 9, 78, 4, 16, 3, 29, 3, 47, 6, 42, 21, 39, 5, 72, 34, 98, 88, 90, 75, 77, 59, 49, 32, 82, 100]\n",
    "eps_reward = 0\n",
    "state = env.reset()\n",
    "for i in optimal_steps:\n",
    "    next_state, reward, terminal = env.step(i)\n",
    "    state = next_state\n",
    "    eps_reward += reward.item()\n",
    "    print(\"Action: \", i, \"Reward: \", reward.item())\n",
    "print(eps_reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(\"Init environment..\")\n",
    "env = RLTe.RLtractEnvironment(device = 'cpu')\n",
    "print(\"..done!\")\n",
    "n_actions = env.action_space.n\n",
    "#print(n_actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "referenceLine = env.referenceStreamline_ijk\n",
    "print(referenceLine.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(referenceLine[0])\n",
    "state = TractographyState(referenceLine[0], env.interpolateDWIatState)\n",
    "print(state.getCoordinate())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "possible_actions = []\n",
    "past_state = env.reset()\n",
    "all_next_states = []\n",
    "for i in range(len(referenceLine)):\n",
    "    best_actions = []\n",
    "    next_states = []\n",
    "    for z in range(n_actions):\n",
    "        env.state = TractographyState(referenceLine[i], env.interpolateDWIatState)\n",
    "        next_state, reward, _ = env.step(z)\n",
    "        env.stepCounter = i\n",
    "        #if reward == -1:\n",
    "        #    reward = 0\n",
    "        #elif reward < 0.2:\n",
    "        if reward > 1.0:\n",
    "            print(\"Step: \", i, \"Action: \", z, \"Distance: \", reward)\n",
    "        #    reward = 1\n",
    "        #elif reward < 1.:\n",
    "        #    reward = 0\n",
    "        #else:\n",
    "        #    reward = -1\n",
    "        #if reward == 1:\n",
    "        #    best_actions.append(z)\n",
    "            #print(i, z, referenceLine[i].numpy(), next_state.getCoordinate().numpy(), reward)\n",
    "    print(i, best_actions)\n",
    "    #print(i, reward)\n",
    "    #if reward > 0.9:\n",
    "    #    best_actions.append(i)\n",
    "    possible_actions.append(best_actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "step_distance = []\n",
    "optimal_steps = []#[100, 80, 75, 80, 75, 100, 62, 75, 83, 75, 83, 100, 83, 62, 67, 51, 67, 100, 59, 59, 59, 100, 59, 56, 51, 56, 66, 100, 66, 71, 71, 79, 58, 100, 71, 71, 84, 71, 100, 92, 84, 92, 97, 100, 97, 38, 97, 43, 38, 100, 43, 43, 89, 48]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "last_state = env.reset()\n",
    "print(len(env.referenceStreamline_ijk))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "steps = 0\n",
    "while len(optimal_steps) < 87:\n",
    "    step_distance = []\n",
    "    for i in range(n_actions):\n",
    "        env.reset()\n",
    "        if len(optimal_steps)>0:\n",
    "            for z in range(len(optimal_steps)):\n",
    "                _,_,_ = env.step(optimal_steps[z])\n",
    "        next_state, _, terminal = env.step(i)\n",
    "        #distance = lineseg_dist(next_state.getCoordinate().numpy(), referenceLine[np.min([len(optimal_steps), 85])].numpy(), referenceLine[np.min([len(optimal_steps)+1, len(referenceLine)-1])].numpy())\n",
    "        #distance = ((next_state.getCoordinate()[0] - env.referenceStreamline_ijk[np.min([env.stepCounter, 58])][0])**2 \\\n",
    "        #              + (next_state.getCoordinate()[1] - env.referenceStreamline_ijk[np.min([env.stepCounter, 58])][1])**2 \\\n",
    "        #              + (next_state.getCoordinate()[2] - env.referenceStreamline_ijk[np.min([env.stepCounter, 58])][2])**2)\n",
    "        current_index = np.min([env.stepCounter, len(env.referenceStreamline_ijk)-1])\n",
    "        qry_pt = next_state.getCoordinate().view(-1,3)\n",
    "        distance = torch.sum((env.referenceStreamline_ijk[current_index] - qry_pt)**2)\n",
    "        \n",
    "        step_distance.append(distance)\n",
    "    optimal_steps.append(np.argmin(step_distance))\n",
    "print(optimal_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Streamline index 0, stepWidth 0.81\n",
    "optimal_steps = [80, 88, 67, 75, 75, 62, 75, 83, 75, 83, 83, 83, 62, 67, 67, 54, 59, 59, 59, 59, 59, 59, 51, 56, 51, 56, 66, 66, 79, 71, 71, 66, 71, 71, 71, 71, 71, 84, 92, 84, 84, 92, 97, 97, 38, 97, 97, 38, 43, 43, 38, 35, 89, 48, 48, 73, 94, 35, 89, 43, 35, 22, 35, 35, 14, 14, 11, 3, 3, 16, 16, 21, 16, 21, 21, 34, 26, 47, 26, 39, 93, 39, 72, 72, 77, 77, 100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Streamline index 4, stepWidth 0.81\n",
    "optimal_steps = [17, 30, 17, 30, 17, 30, 17, 1, 6, 3, 3, 3, 11, 16, 16, 24, 16, 24, 37, 24, 91, 45, 78, 78, 86, 99, 86, 86, 86, 70, 70, 65, 70, 65, 86, 65, 86, 99, 99, 40, 45, 45, 100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Streamline index 4, stepWidth 0.79\n",
    "#optimal_steps =[17, 30, 17, 30, 17, 30, 17, 1, 6, 3, 3, 3, 11, 16, 16, 16, 24, 24, 24, 37, 24, 91, 78, 78, 99, 86, 86, 86, 86, 86, 70, 70, 70, 65, 65, 86, 86, 86, 99, 99, 99, 45, 99]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Streamline index 4, stepWidth 0.78\n",
    "#optimal_steps = [17, 30, 17, 30, 17, 30, 17, 1, 6, 1, 3, 11, 3, 16, 16, 24, 16, 24, 24, 37, 37, 45, 78, 78, 99, 86, 86, 78, 86, 86, 78, 70, 65, 65, 65, 86, 86, 86, 99, 99, 99, 45, 78]\n",
    "print(optimal_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Streamline index 4, stepWidth 0.8\n",
    "#optimal_steps = [17, 30, 17, 30, 17, 30, 17, 1, 6, 3, 3, 3, 11, 16, 16, 16, 24, 24, 37, 24, 37, 78, 45, 78, 86, 86, 86, 86, 86, 78, 70, 65, 70, 65, 86, 65, 86, 86, 99, 32, 99, 45]\n",
    "print(optimal_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Streamline index 2\n",
    "optimal_steps = [3, 3, 100, 6, 3, 11, 3, 6, 100, 11, 6, 11, 3, 100, 19, 3, 11, 6, 16, 100, 3, 3, 3, 2, 11, 7, 18, 100, 15, 7, 2, 2, 100, 0, 10, 0, 2, 0, 100, 0, 0, 3, 0, 11, 100, 3, 3, 3, 3, 100, 3, 3, 1, 11, 100, 3, 3, 3, 16, 11, 100, 16, 29, 16, 42, 100, 21, 29, 21, 42, 21, 100, 34, 21, 13, 26, 100, 23, 18, 23, 31, 100, 44, 31, 31, 44, 44, 100, 44, 31, 36, 98, 15, 100, 23, 23, 44, 15, 44, 100, 15, 28, 15, 20, 36, 100, 20, 28, 20, 20, 100, 28, 28, 36, 49, 28, 100, 36, 49, 49, 90, 100, 49, 95, 95, 49, 46, 49, 38, 36, 25, 100, 28, 28, 33, 36, 41, 100, 20, 28, 12, 20, 20, 100, 7, 15, 7, 20, 10, 25, 100]\n",
    "#print(optimal_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For Streamline with len 44 (index 4)\n",
    "#print(optimal_steps)\n",
    "optimal_steps = [17, 30, 100, 17, 30, 17, 17, 6, 0, 17, 37, 0, 0, 78, 24, 16, 24, 24, 100, 37, 45, 45, 70, 100, 99, 86, 86, 78, 94, 62, 100, 65, 70, 86, 65, 100, 86, 94, 99, 45, 99, 100, 100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with line distance\n",
    "#optimal_steps = [80, 88, 54, 96, 46, 75, 75, 75, 83, 75, 83, 83, 62, 54, 1, 59, 54, 59, 59, 67, 56, 59, 51, 59, 61, 11, 53, 61, 66, 71, 71, 79, 58, 71, 71, 71, 21, 71, 84, 92, 84, 92, 97, 84, 43, 84, 30, 97, 47, 97, 43, 30, 89, 35, 94, 73, 48, 89, 22, 72, 43, 35, 22, 35, 35, 6, 19, 3, 16, 16, 66, 16, 8, 21, 29, 21, 26, 26, 93, 26, 93, 85, 35, 85, 72, 77, 100]\n",
    "optimal_steps = [100, 80, 75, 80, 75, 100, 62, 75, 83, 75, 83, 100, 83, 62, 67, 51, 67, 100, 59, 59, 59, 100, 59, 56, 51, 56, 66, 100, 66, 71, 71, 79, 58, 100, 71, 71, 84, 71, 100, 92, 84, 92, 97, 100, 97, 38, 97, 43, 38, 100, 43, 43, 89, 48, 100, 94, 81, 48, 43, 100, 35, 43, 35, 22, 27, 100, 6, 11, 3, 16, 100, 8, 29, 21, 21, 100, 34, 26, 26, 93, 39, 100, 93, 72, 77, 77, 101]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#optimal_steps = [80, 75, 80, 75, 62, 75, 83, 96, 51, 24, 62, 62, 62, 77, 65, 64, 67, 59, 56, 83, 82, 54, 56, 53, 56, 38, 56, 84, 66, 71, 71, 64, 97, 84, 71, 71, 38, 51, 30, 92, 97, 84, 43, 79, 27, 46, 89, 25, 81, 25, 48, 43, 86, 48, 57, 14, 89, 43, 43, 19, 92, 14, 27, 9, 78, 4, 16, 3, 29, 3, 47, 6, 42, 21, 39, 5, 72, 34, 98, 88, 90, 75, 77, 59, 49, 32, 82]\n",
    "print(optimal_steps) # <-- min of max distance reward streamline 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimal_steps = [80, 75, 80, 75, 62, 75, 83, 96, 51, 24, 62, 62, 62, 77, 65, 64, 67, 59, 56, 83, 82, 54, 56, 53, 56, 38, 56, 84, 66, 71, 71, 64, 97, 84, 71, 71, 38, 51, 30, 92, 97, 84, 43, 79, 27, 46, 89, 25, 81, 25, 48, 43, 86, 48, 57, 14, 89, 43, 43, 19, 92, 14, 27, 9, 78, 4, 16, 3, 29, 3, 47, 6, 42, 21, 39, 5, 72, 34, 98, 88, 90, 75, 77, 59, 49, 32, 82]\n",
    "print(optimal_steps) # <-- min of max distance reward streamline 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimal_steps = [80, 88, 54, 96, 59, 37, 54, 37, 67, 91, 62, 78, 64, 70, 64, 62, 69, 83, 56, 59, 59, 53, 42, 53, 53, 56, 79, 58, 87, 60, 87, 58, 92, 52, 46, 58, 38, 58, 30, 58, 38, 84, 17, 55, 30, 76, 25, 76, 17, 76, 17, 81, 30, 86, 48, 65, 27, 97, 14, 84, 6, 97, 14, 89, 3, 27, 8, 19, 13, 19, 13, 29, 8, 96, 2, 88, 31, 47, 26, 59, 5, 72, 72, 85, 61, 39]\n",
    "print(optimal_steps) # <-- min reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change optimal steps\n",
    "optimal_steps = [17, 30, 100, 17, 30, 17, 17, 6, 0, 6, 0, 0, 0, 78, 24, 16, 24, 24, 100, 37, 45, 45, 70, 100, 99, 86, 86, 78, 94, 62, 100, 65, 70, 86, 65, 100, 86, 94, 99, 45, 99, 100, 100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Object(object):\n",
    "    pass\n",
    "\n",
    "state = env.reset()\n",
    "print(env.state.getCoordinate().numpy(), env.referenceStreamline_ijk[0])\n",
    "step = 1\n",
    "all_distances = []\n",
    "all_states = []\n",
    "len_line = len(env.referenceStreamline_ijk)-1\n",
    "all_states.append(state.getCoordinate())\n",
    "\n",
    "for i in optimal_steps:\n",
    "    next_state, reward, terminal = env.step(i)\n",
    "    #print(step, reward)\n",
    "    #current_index = np.min([env.points_visited+1,len(env.referenceStreamline_ijk)-1])\n",
    "    #print(\"Reference Line at current index: \", env.referenceStreamline_ijk[current_index])\n",
    "    #distance = lineseg_dist(next_state.getCoordinate().numpy(), referenceLine[step-1].numpy(), referenceLine[np.min([step, len(referenceLine)-1])].numpy())\n",
    "    #distance = 2 + (distance/10)\n",
    "    #distance = ((next_state.getCoordinate()[0] - env.referenceStreamline_ijk[np.min([env.stepCounter, len_line])][0])**2 \\\n",
    "    #                  + (next_state.getCoordinate()[1] - env.referenceStreamline_ijk[np.min([env.stepCounter, len_line])][1])**2 \\\n",
    "    #                  + (next_state.getCoordinate()[2] - env.referenceStreamline_ijk[np.min([env.stepCounter, len_line])][2])**2)\n",
    "    current_index = np.min([env.stepCounter, len(env.referenceStreamline_ijk)-1])\n",
    "    qry_pt = next_state.getCoordinate().view(-1,3)\n",
    "    distance = torch.sum((env.referenceStreamline_ijk[current_index] - qry_pt)**2)\n",
    "    \n",
    "    print(step, i, next_state.getCoordinate().numpy(), env.referenceStreamline_ijk[np.min([env.stepCounter,len_line])].numpy(), reward, -distance.item())\n",
    "    all_distances.append(distance)\n",
    "    all_states.append(next_state.getCoordinate())\n",
    "    states.x.append(next_state.getCoordinate()[0])\n",
    "    states.y.append(next_state.getCoordinate()[1])\n",
    "    states.z.append(next_state.getCoordinate()[2])\n",
    "    #if distance < 0.71:\n",
    "    #    reward = 1 - distance\n",
    "    #    #print(reward)\n",
    "    #    if reward < 0.3:\n",
    "    #        reward = 1\n",
    "    step += 1\n",
    "\n",
    "print(np.min(all_distances), np.max(all_distances), np.sum(all_distances))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(env.referenceStreamline_ijk[4])\n",
    "print(env.referenceStreamline_ijk.T[1][4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, x in enumerate(all_states):\n",
    "    try:\n",
    "        print(x, env.referenceStreamline_ijk[i])\n",
    "    except IndexError:\n",
    "        print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#for i in range(len(states)):\n",
    "#    print(states[i], env.referenceStreamline_ijk[i])\n",
    "#    distance = ((states.T[0][i] - env.referenceStreamline_ijk.T[0][i])**2 \\\n",
    "#                      + (states.T[1][i] - env.referenceStreamline_ijk.T[1][i] )**2 \\\n",
    "#                      + (states.T[2][i] - env.referenceStreamline_ijk.T[2][i])**2)\n",
    "#    print(distance)\n",
    "\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = plt.axes(projection='3d')\n",
    "ax.plot3D(env.referenceStreamline_ijk.T[0][:], env.referenceStreamline_ijk.T[1][:], env.referenceStreamline_ijk.T[2][:])\n",
    "ax.plot3D(states.T[0][:], states.T[1][:], states.T[2][:])\n",
    "#print(optimal_steps[:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "states = torch.stack(all_states)\n",
    "print(states.T[0][:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(all_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(referenceLine[86])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.reset()\n",
    "print(env.state.getCoordinate().numpy(), referenceLine[0])\n",
    "step = 0\n",
    "#all_rewards = []\n",
    "eps_reward = 0\n",
    "for i in optimal_steps:\n",
    "    next_state, distance, terminal = env.step(i)\n",
    "    if distance < 0.71:\n",
    "        reward = 1 - distance\n",
    "        #print(reward)\n",
    "        if reward < 0.3:\n",
    "            reward = 1\n",
    "    eps_reward += reward\n",
    "    #all_rewards.append(reward)\n",
    "    step += 1\n",
    "print(eps_reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(n_actions):\n",
    "    \n",
    "    next_state, distance, terminal = env.step(i)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.reset()\n",
    "terminal = False\n",
    "step = 0\n",
    "actions = []\n",
    "past_state = env.state\n",
    "step = 1\n",
    "while terminal != True:\n",
    "    for i in range(n_actions)\n",
    "    action = np.random.randint(n_actions)\n",
    "    next_state, reward, terminal = env.step(action)\n",
    "    if reward < 1:\n",
    "        actions.append(action)\n",
    "        past_state = next_state\n",
    "        print(\"Action: \", action, \"Step: \",step, \"Coordinates: \", next_state.getCoordinate().numpy(), referenceLine[step].numpy())\n",
    "        step += 1\n",
    "    else:\n",
    "        env.state = past_state\n",
    "        env.stepCounter = step\n",
    "    #action = np.random.choice(possible_actions[step])\n",
    "    #next_state, reward, terminal = env.step(action)\n",
    "    #step += 1\n",
    "\n",
    "print(actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(referenceLine[6])\n",
    "print(referenceLine[7])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = TractographyState(torch.Tensor([ 77.8994346, 108.7020324, 90.72022516]), env.interpolateDWIatState)\n",
    "for i in range(n_actions):\n",
    "    env.state = state\n",
    "    env.stepCounter -= 1\n",
    "    next_state, _, terminal = env.step(i)\n",
    "    qry_pt = next_state.getCoordinate().view(-1,3)\n",
    "    distance = torch.min(torch.sum((referenceLine[7] - qry_pt)**2, dim=1))\n",
    "    if distance < 0.3:\n",
    "        print(i, next_state.getCoordinate().numpy(), referenceLine[7].numpy(), distance.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.reset()\n",
    "next_state, reward, terminal = env.step(100)\n",
    "print(next_state.getCoordinate().numpy())\n",
    "print(reward)\n",
    "print(terminal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(possible_actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.state = TractographyState(referenceLine[85], env.interpolateDWIatState)\n",
    "print(env.state.getCoordinate())\n",
    "print(referenceLine[86])\n",
    "print(possible_actions[85])\n",
    "for i in possible_actions[85]:\n",
    "    env.state = TractographyState(referenceLine[85], env.interpolateDWIatState)\n",
    "    env.stepCounter = 84\n",
    "    next_state, reward, _ = env.step(z)\n",
    "    print(next_state.getCoordinate(), reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#env.state = TractographyState(referenceLine[85], env.interpolateDWIatState)\n",
    "for i in range(n_actions):\n",
    "    env.reset()\n",
    "    env.state = TractographyState(referenceLine[85], env.interpolateDWIatState)\n",
    "    next_state, reward, _ = env.step(i)\n",
    "    distance = env.rewardForTerminalState(next_state)\n",
    "    if distance < 0.3:\n",
    "        print(referenceLine[86].numpy(), next_state.getCoordinate().numpy(), distance.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distance = env.rewardForTerminalState(next_state)\n",
    "print(referenceLine[86])\n",
    "print(distance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(n_actions):\n",
    "    env.state = TractographyState(torch.FloatTensor([ 74.64776812, 107.9270337, 93.22325858]), env.interpolateDWIatState)\n",
    "    next_state, reward, _ = env.step(i)\n",
    "    env.stepCounter = 2\n",
    "    if reward < 0.1:\n",
    "        reward = 1\n",
    "    elif reward < 0.5:\n",
    "        reward = 0\n",
    "    else:\n",
    "        reward = -1\n",
    "    if reward == 1:\n",
    "        #best_actions.append(i)\n",
    "        print(\"[{}]\".format(i), referenceLine[2].numpy(), next_state.getCoordinate().numpy(), reward)\n",
    "#print(best_actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = TractographyState(torch.FloatTensor(referenceLine[0]), env.interpolateDWIatState)\n",
    "coordinates = state.getCoordinate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(referenceLine[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(referenceLine[0])\n",
    "print(referenceLine[70])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = TractographyState(referenceLine[69], env.interpolateDWIatState)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shape = env.reset().getValue().reshape(-1).shape[0]\n",
    "print(shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = torch.FloatTensor(state.getValue()).unsqueeze(0).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_vals = agent.main_dqn(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(state.getValue().shape)\n",
    "shape = state.getValue().shape\n",
    "shape = np.prod(np.array(shape))\n",
    "print(shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = TractographyState(referenceLine[70], env.interpolateDWIatState)\n",
    "qry_pt = torch.FloatTensor(state.getCoordinate()).view(-1,3)\n",
    "distance = torch.min(torch.sum( (referenceLine - qry_pt)**2, dim =1 ))\n",
    "qry_pt = torch.FloatTensor(state.getCoordinate()).view(3)\n",
    "distance_terminal = torch.sum( (referenceLine[-1,:] - qry_pt)**2 )\n",
    "\n",
    "#print(distance)\n",
    "#print(distance_terminal)\n",
    "reward = (torch.tanh(-distance+5.3) + 2*torch.tanh(-distance_terminal+5.3))/2\n",
    "print(reward)\n",
    "\n",
    "print(torch.tanh(-distance+5.3))\n",
    "print(torch.tanh(-distance_terminal+5.3))\n",
    "\n",
    "reward += 200/20 * reward.sign()\n",
    "print(reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(torch.tanh(-distance_terminal+5.3)+2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = TractographyState([32., 84., 94.], env.interpolateDWIatState)\n",
    "qry_pt = torch.FloatTensor(state.getCoordinate()).view(-1,3)\n",
    "distance = torch.min(torch.sum( (referenceLine - qry_pt)**2, dim =1 ))\n",
    "print(torch.tanh(-distance+5.3))\n",
    "qry_pt = torch.FloatTensor(state.getCoordinate()).view(3)\n",
    "distance = torch.sum( (referenceLine[-1,:] - qry_pt)**2 )\n",
    "print(-distance)\n",
    "print(torch.tanh(-distance)+2)\n",
    "#print(torch.where(distance < env.maxL2dist_to_terminalState, 1, 0 ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(-1.5 + 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qry_pt = torch.FloatTensor(state.getCoordinate()).view(3)\n",
    "distance = torch.sum( (referenceLine[-1,:] - qry_pt)**2 )\n",
    "print(round(-distance.item(),4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Init agent\")\n",
    "#memory = ReplayMemory(size=replay_memory_size)\n",
    "state = env.reset()\n",
    "agent = Agent(n_actions=n_actions, inp_size=state.getValue().shape, device=device, hidden=256, agent_history_length=agent_history_length, memory_size=replay_memory_size, learning_rate=learning_rate)\n",
    "\n",
    "print(\"Init epsilon-greedy action scheduler\")\n",
    "action_scheduler = Action_Scheduler(num_actions=n_actions, max_steps=max_steps, eps_annealing_steps=100000, replay_memory_start_size=replay_memory_size, model=agent.main_dqn)\n",
    "\n",
    "step_counter = 0\n",
    "    \n",
    "eps_rewards = []\n",
    "\n",
    "print(\"Start training...\")\n",
    "while step_counter < max_steps:\n",
    "    epoch_step = 0\n",
    "\n",
    "######## fill memory begins here\n",
    "    while epoch_step < evaluate_every:  # To Do implement evaluation\n",
    "        state = env.reset()\n",
    "        episode_reward_sum = 0\n",
    "        \n",
    "        #fill replay memory while interacting with env\n",
    "        for episode_counter in range(max_episode_length):\n",
    "            # get action with epsilon-greedy strategy       \n",
    "            action = action_scheduler.get_action(step_counter, torch.FloatTensor(state.getValue()).to(device).unsqueeze(0))\n",
    "                    \n",
    "            next_state, reward, terminal = env.step(action)\n",
    "\n",
    "            if reward >= 1:\n",
    "                reward = 10\n",
    "            elif reward > -0.05:\n",
    "                reward = 1\n",
    "            \n",
    "            if episode_counter == max_episode_length-1:\n",
    "                reward = -100\n",
    "                terminal = True\n",
    "            # increase counter\n",
    "            step_counter += 1\n",
    "            epoch_step += 1\n",
    "\n",
    "            # accumulate reward for current episode\n",
    "            episode_reward_sum += reward\n",
    "\n",
    "\n",
    "            agent.replay_memory.add_experience(action=action,\n",
    "                                state=state.getValue(),\n",
    "                                reward=reward,\n",
    "                                new_state=next_state.getValue(),\n",
    "                                terminal=terminal)\n",
    "\n",
    "\n",
    "            state = next_state\n",
    "\n",
    "        \n",
    "\n",
    "            ####### optimization is happening here\n",
    "            if step_counter > replay_memory_size:\n",
    "                loss = agent.optimize()\n",
    "\n",
    "\n",
    "            ####### target network update\n",
    "            if step_counter > replay_memory_size and step_counter % network_update_every == 0:\n",
    "                agent.target_dqn.load_state_dict(agent.main_dqn.state_dict())\n",
    "            \n",
    "            # if episode ended before maximum step\n",
    "            if terminal:\n",
    "                terminal = False\n",
    "                state = env.reset()\n",
    "                break\n",
    "                \n",
    "        eps_rewards.append(episode_reward_sum)\n",
    "        \n",
    "        if len(eps_rewards) % 10 == 0:\n",
    "            with open(path+'/logs/rewards.dat', 'a') as reward_file:\n",
    "                print(\"[{}] {}, {}\".format(len(eps_rewards), step_counter, np.mean(eps_rewards[-100:])), file=reward_file)\n",
    "            print(\"[{}] {}, {}\".format(len(eps_rewards), step_counter, np.mean(eps_rewards[-100:])) )\n",
    "    torch.save(agent.main_dqn.state_dict(), path+'/checkpoints/fibre_agent_{}_reward_{:.2f}.pth'.format(step_counter, np.mean(eps_rewards[-100:])))\n",
    "########## evaluation starting here\n",
    "    eval_rewards = []\n",
    "    for _ in range(eval_runs):\n",
    "        eval_steps = 0\n",
    "        state = env.reset()\n",
    "        eval_episode_reward = 0\n",
    "        while eval_steps < max_episode_length:\n",
    "            action = action_scheduler.get_action(step_counter, torch.FloatTensor(state.getValue()).to(device).unsqueeze(0), evaluation=True)\n",
    "\n",
    "            next_state, reward, terminal = env.step(action)\n",
    "\n",
    "            eval_steps += 1\n",
    "            eval_episode_reward += reward\n",
    "            state = next_state\n",
    "\n",
    "            if terminal:\n",
    "                terminal = False\n",
    "                break\n",
    "\n",
    "        eval_rewards.append(eval_episode_reward)\n",
    "    \n",
    "    print(\"Evaluation score:\", np.mean(eval_rewards))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!mkdir -p 'checkpoints/'\n",
    "#torch.save(agent.main_dqn.state_dict(), 'checkpoints/fiber_agent_{}_reward_{:.2f}.pth'.format(step_counter, np.mean(rewards[-100:])))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CONDA (atari)",
   "language": "python",
   "name": "atari"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
